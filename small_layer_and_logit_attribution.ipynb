{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "7L4cVY0T0WaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqXMyWPozqvT",
        "outputId": "3576585c-d50a-4176-a381-b582ac7e1134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-i7xuo5l5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-i7xuo5l5\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit d48e72a69833881b32fddcba06455ac0000d8ba2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.14.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.65.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.28.1)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.2.15)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (13.3.3)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.6.0)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (2.11.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.27.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.13.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.5.0)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer-lens==0.0.0) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer-lens==0.0.0) (0.40.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (3.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.19.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.31)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.12)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2022.12.7)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer-lens==0.0.0) (5.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer-lens==0.0.0) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.9/dist-packages (1.39.1)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (5.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.22.4)\n",
            "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.13.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.15.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (8.5.0.96)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.10->circuitsvis) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.10->circuitsvis) (67.6.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "%pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "from circuitsvis.activations import text_neuron_activations\n",
        "from jaxtyping import Float, Int\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")\n",
        "from einops import rearrange\n",
        "class NeuronTextSimplifier:\n",
        "    def __init__(self, model, layer: int, neuron: int) -> None:\n",
        "        self.model = model\n",
        "        self.device = model.cfg.device\n",
        "        self.layer = layer\n",
        "        self.neuron = neuron\n",
        "        self.model.requires_grad_(False)\n",
        "        self.embed_weights = list(list(model.children())[0].parameters())[0]\n",
        "        if(\"pythia\" not in model.cfg.model_name):\n",
        "            transformer_block_loc = 4\n",
        "        else:\n",
        "            transformer_block_loc = 2\n",
        "        transformer_blocks = [mod for mod in list(self.model.children())[transformer_block_loc]]\n",
        "        self.model_no_embed = th.nn.Sequential(*(transformer_blocks[:layer+1]))\n",
        "        self.model_no_embed.requires_grad_(False)\n",
        "        self.set_hooks()\n",
        "\n",
        "    def set_hooks(self):\n",
        "        self._neurons = th.empty(0)\n",
        "        def hook(model, input, output):\n",
        "            self._neurons = output\n",
        "        self.model.blocks[self.layer].mlp.hook_post.register_forward_hook(hook)\n",
        "\n",
        "    def ablate_mlp_neurons(self, tokens, neurons: th.Tensor):\n",
        "        def mlp_ablation_hook(\n",
        "            value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
        "            hook: HookPoint\n",
        "        ) -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
        "            if(neurons.shape[0] == 0):\n",
        "                return value\n",
        "            value[:, :, neurons] = 0\n",
        "            return value\n",
        "        return self.model.run_with_hooks(tokens, fwd_hooks=[(f\"blocks.{self.layer}.mlp.hook_post\", mlp_ablation_hook)])\n",
        "        \n",
        "    def add_noise_to_text(self, text, noise_level=1.0):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        activation_list = []\n",
        "        for t in text:\n",
        "            split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            tokens = self.model.to_tokens(t, prepend_bos=False)\n",
        "            # Add gaussian noise to the input of each word in turn, getting the diff in final neuron's response\n",
        "            embedded_tokens = self.model.embed(tokens)\n",
        "            batch_size, seq_size, embedding_size = embedded_tokens.shape\n",
        "            noise = th.randn(1, embedding_size, device=self.device)*noise_level\n",
        "            original = self.embedded_forward(embedded_tokens)[:,-1,self.neuron]\n",
        "            changed_activations = th.zeros(seq_size, device=self.device)\n",
        "            for i in range(seq_size):\n",
        "                embedded_tokens[:,i,:] += noise\n",
        "                neuron_response = self.embedded_forward(embedded_tokens)\n",
        "                changed_activations[i] = neuron_response[:,-1,self.neuron].item()\n",
        "                embedded_tokens[:,i,:] -= noise\n",
        "            changed_activations -= original\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            activation_list += changed_activations.tolist() + [0.0]\n",
        "        activation_list = th.tensor(activation_list).reshape(-1,1,1)\n",
        "        return text_neuron_activations(tokens=text_list, activations=activation_list)\n",
        "\n",
        "    def visualize_logit_diff(self, text, neurons: th.Tensor, setting=\"true_tokens\", verbose=True):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        logit_list = []\n",
        "        for t in text:\n",
        "            split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            tokens = self.model.to_tokens(t, prepend_bos=False)\n",
        "            original_logits = self.model(tokens).log_softmax(-1)\n",
        "            ablated_logits = self.ablate_mlp_neurons(tokens, neurons).log_softmax(-1)\n",
        "            diff_logits =  ablated_logits - original_logits\n",
        "            if setting == \"true_tokens\":\n",
        "                # Gather the logits for the true tokens\n",
        "                diff = rearrange(diff_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "            elif setting == \"max\":\n",
        "                val, ind = diff_logits.max(2)\n",
        "                diff = rearrange(val, \"b s -> (b s)\")\n",
        "                split_text = self.model.to_str_tokens(ind)\n",
        "                tokens = ind\n",
        "            if(verbose):\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "                orig = rearrange(original_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "                ablated = rearrange(ablated_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "                logit_list += orig.tolist() + [0.0]\n",
        "                logit_list += ablated.tolist() + [0.0]\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            logit_list += diff.tolist() + [0.0]\n",
        "        logit_list = th.tensor(logit_list).reshape(-1,1,1)\n",
        "        if verbose:\n",
        "            print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
        "        return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
        "\n",
        "    def get_neuron_activation(self, tokens):\n",
        "        _, cache = self.model.run_with_cache(tokens.to(self.model.cfg.device))\n",
        "        return cache[f\"blocks.{self.layer}.mlp.hook_post\"][0,:,self.neuron].tolist()\n",
        "\n",
        "    def text_to_activations_print(self, text):\n",
        "        token = self.model.to_tokens(text, prepend_bos=False)\n",
        "        act = self.get_neuron_activation(token)\n",
        "        act = [f\" [{a:.2f}]\" for a in act]\n",
        "        if(token.shape[-1] > 1):\n",
        "            string = self.model.to_str_tokens(token, prepend_bos=False)\n",
        "        else: \n",
        "            string = self.model.to_string(token)\n",
        "        res = [None]*(len(string)+len(act))\n",
        "        res[::2] = string\n",
        "        res[1::2] = act\n",
        "        return \"\".join(res)\n",
        "\n",
        "    def text_to_visualize(self, text):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        act_list = []\n",
        "        for t in text:\n",
        "            if isinstance(t, str): # If the text is a list of tokens\n",
        "                split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "                token = self.model.to_tokens(t, prepend_bos=False)\n",
        "            else:\n",
        "                token = t\n",
        "                split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            act_list+= self.get_neuron_activation(token) + [0.0]\n",
        "        act_list = th.tensor(act_list).reshape(-1,1,1)\n",
        "        return text_neuron_activations(tokens=text_list, activations=act_list)\n",
        "\n",
        "    def get_text_and_activations_iteratively(self, text):\n",
        "        tokens = self.model.to_tokens(text, prepend_bos=False)[0]\n",
        "        original_activation = self.get_neuron_activation(tokens)\n",
        "        # To get around the newline issue, we replace the newline with \\newline and then add a newline at the end\n",
        "        text_list = [x.replace('\\n', '\\\\newline') for x in self.model.to_str_tokens(text, prepend_bos=False)] + [\"\\n\"]\n",
        "        act_list = original_activation + [0.0]\n",
        "        changes = th.zeros(tokens.shape[-1])+100\n",
        "        for j in range(len(tokens)-1):\n",
        "            for i in range(len(tokens)):\n",
        "                changes[i] = self.get_neuron_activation(th.cat((tokens[:i],tokens[i+1:])))[-1]\n",
        "            max_ind = changes.argmax()\n",
        "            changes = th.cat((changes[:max_ind], changes[max_ind+1:]))\n",
        "            tokens = th.cat((tokens[:max_ind],tokens[max_ind+1:]))\n",
        "            if(tokens.shape[-1] > 1):\n",
        "                out_text = self.model.to_str_tokens(tokens, prepend_bos=False)\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in out_text] + [\"\\n\"]\n",
        "            else:\n",
        "                out_text = self.model.to_string(tokens)\n",
        "                text_list += [out_text.replace('\\n', '\\\\newline')] + [\"\\n\"]\n",
        "            act_list += self.get_neuron_activation(tokens) + [0.0]\n",
        "        text_list = text_list\n",
        "        act_list = th.tensor(act_list).reshape(-1,1,1)\n",
        "        return text_list, act_list\n",
        "\n",
        "    def visualize_text_color_iteratively(self, text):\n",
        "        if(isinstance(text, str)):\n",
        "            text_list, act_list = self.get_text_and_activations_iteratively(text)\n",
        "            return text_neuron_activations(tokens=text_list, activations=act_list)\n",
        "        elif(isinstance(text, list)):\n",
        "            text_list_final = []\n",
        "            act_list_final = []\n",
        "            for t in range(len(text)):\n",
        "                text_list, act_list = self.get_text_and_activations_iteratively(text[t])\n",
        "                text_list_final.append(text_list)\n",
        "                act_list_final.append(act_list)\n",
        "            return text_neuron_activations(tokens=text_list_final, activations=act_list_final)\n",
        "\n",
        "    def simplify_iteratively(self, text):\n",
        "        # Iteratively remove text that has smallest decrease in activation\n",
        "        # Print out the change in activation for the largest changes, ie if the change is larger than threshold*original_activation\n",
        "        tokens = self.model.to_tokens(text, prepend_bos=False)[0]\n",
        "        self.text_to_activations_print(self.model.to_string(tokens))\n",
        "        original_activation = self.get_neuron_activation(tokens)[-1]\n",
        "        changes = th.zeros(tokens.shape[-1])+100\n",
        "        for j in range(len(tokens)-1):\n",
        "            for i in range(len(tokens)):\n",
        "                changes[i] = self.get_neuron_activation(th.cat((tokens[:i],tokens[i+1:])))[-1]\n",
        "            max_ind = changes.argmax()\n",
        "            changes = th.cat((changes[:max_ind], changes[max_ind+1:]))\n",
        "            tokens = th.cat((tokens[:max_ind],tokens[max_ind+1:]))\n",
        "            out_text = self.model.to_string(tokens)\n",
        "            print(self.text_to_activations_print(out_text))\n",
        "        return\n",
        "\n",
        "    # Assign neuron and layer\n",
        "    def set_layer_and_neuron(self, layer, neuron):\n",
        "        self.layer = layer\n",
        "        self.neuron = neuron\n",
        "        self.set_hooks()\n",
        "\n",
        "    def embedded_forward(self, embedded_x):\n",
        "        self.model_no_embed(embedded_x)\n",
        "        return self._neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.model(x)       \n",
        "        return self._neurons\n",
        "\n",
        "    def prompt_optimization(\n",
        "            self,\n",
        "            diverse_outputs_num=10, \n",
        "            iteration_cap_until_convergence = 30,\n",
        "            init_text = None,\n",
        "            seq_size = 4,\n",
        "            insert_words_and_pos = None, #List of words and positions to insert [word, pos]\n",
        "            neuron_loss_scalar = 1,\n",
        "            diversity_loss_scalar = 1,\n",
        "        ):\n",
        "        _, _, embed_size = self.model.W_out.shape\n",
        "        vocab_size = self.model.W_E.shape[0]\n",
        "        largest_prompts = [None]*diverse_outputs_num\n",
        "        # Use dim-1 when we're doing a for loop (list comprehension)\n",
        "        # Use dim-2 when we're doing all at once\n",
        "        cos_dim_1 = th.nn.CosineSimilarity(dim=1)\n",
        "        cos_dim_2 = th.nn.CosineSimilarity(dim=2)\n",
        "        total_iterations = 0\n",
        "\n",
        "        if init_text is not None:\n",
        "            init_tokens = self.model.to_tokens(init_text, prepend_bos=False)\n",
        "            seq_size = init_tokens.shape[-1]\n",
        "        diverse_outputs = th.zeros(diverse_outputs_num, seq_size, embed_size).to(self.device)\n",
        "        for d_ind in range(diverse_outputs_num):\n",
        "            print(f\"Starting diverse output {d_ind}\")\n",
        "            if init_text is None:\n",
        "                # Random tokens of sequence length\n",
        "                init_tokens = th.randint(0, vocab_size, (1,seq_size))\n",
        "                init_text = self.model.to_string(init_tokens)\n",
        "            prompt_embeds = th.nn.Parameter(self.model.embed(init_tokens)).detach()\n",
        "            prompt_embeds.requires_grad_(True).to(self.device)\n",
        "\n",
        "            optim = th.optim.AdamW([prompt_embeds], lr=.8, weight_decay=0.01)\n",
        "            largest_activation = 0\n",
        "            largest_prompt = None\n",
        "\n",
        "            iterations_since_last_improvement = 0\n",
        "            while(iterations_since_last_improvement < iteration_cap_until_convergence):\n",
        "            # First, project into the embedding matrix\n",
        "                with th.no_grad():\n",
        "                    projected_index = th.stack([cos_dim_1(self.embed_weights,prompt_embeds[0,i,:]).argmax() for i in range(seq_size)]).unsqueeze(0)\n",
        "                    projected_embeds = self.model.embed(projected_index)\n",
        "\n",
        "                # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
        "                tmp_embeds = prompt_embeds.detach().clone()\n",
        "                tmp_embeds.data = projected_embeds.data\n",
        "                # add some gaussian noise to tmp_embeds\n",
        "                # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.01\n",
        "                tmp_embeds.requires_grad_(True)\n",
        "\n",
        "                if insert_words_and_pos is not None:\n",
        "                    text = insert_words_and_pos[0]\n",
        "                    pos = insert_words_and_pos[1]\n",
        "                    if(pos == -1):\n",
        "                        pos = seq_size\n",
        "                    token = self.model.to_tokens(text, prepend_bos=False)\n",
        "                    token_embeds = self.model.embed(token)\n",
        "                    token_pos = pos\n",
        "                    wrapped_embeds = th.cat([tmp_embeds[0,:token_pos], token_embeds[0], tmp_embeds[0,token_pos:]], dim=0).unsqueeze(0)\n",
        "                    if(total_iterations == 0):\n",
        "                        wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
        "                        projected_index = th.stack([cos_dim_1(self.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
        "                        print(f\"Inserting {text} at pos {pos}: {self.model.to_str_tokens(projected_index, prepend_bos=False)}\")\n",
        "                else:\n",
        "                    wrapped_embeds = tmp_embeds\n",
        "\n",
        "                # Then, calculate neuron_output\n",
        "                neuron_output = self.embedded_forward(wrapped_embeds)[0,:, self.neuron]\n",
        "                if(d_ind > 0):\n",
        "                    diversity_loss = cos_dim_2(tmp_embeds[0], diverse_outputs[:d_ind])\n",
        "                    # return cos, tmp_embeds, diverse_outputs\n",
        "                else:\n",
        "                    diversity_loss = th.zeros(1)\n",
        "\n",
        "                loss = neuron_loss_scalar*-neuron_output[-1] + diversity_loss_scalar*diversity_loss.mean()\n",
        "\n",
        "                # Save the highest activation\n",
        "                if neuron_output[-1] > largest_activation:\n",
        "                    iterations_since_last_improvement = 0\n",
        "                    largest_activation = neuron_output[-1]\n",
        "                    wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
        "                    projected_index = th.stack([cos_dim_1(self.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
        "                    largest_prompt = self.model.to_string(projected_index)\n",
        "                    largest_prompts[d_ind] = largest_prompt\n",
        "                    print(f\"New largest activation: {largest_activation} | {largest_prompt}\")\n",
        "\n",
        "                # Transfer the gradient to the continuous embedding space\n",
        "                prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
        "                \n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                total_iterations += 1\n",
        "                iterations_since_last_improvement += 1\n",
        "                init_text = None\n",
        "            diverse_outputs[d_ind] = tmp_embeds.data[0,...]\n",
        "        return largest_prompts\n"
      ],
      "metadata": {
        "id": "1uwEUgp60NS7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "87KpvUmv0cAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Transformer Lens, and load pythia models\n",
        "from transformer_lens import HookedTransformer\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import numpy as np \n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_NAME_LIST = [\n",
        "    # \"EleutherAI/pythia-70m-deduped\", \n",
        "    # \"EleutherAI/pythia-160m-deduped\", \n",
        "    # \"EleutherAI/pythia-410m-deduped\", \n",
        "    # \"gpt2\", \n",
        "    # \"gpt2-medium\",\n",
        "    # \"solu-1l\",\n",
        "    # \"solu-2l\",\n",
        "    # \"solu-3l\",\n",
        "    # \"solu-4l\",\n",
        "    \"gelu-2l\"\n",
        "]\n",
        "model_name = MODEL_NAME_LIST[0]\n",
        "layer = 1 # Layer 1 is actually the 2nd layer because 0-indexing\n",
        "\n",
        "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
        "\n",
        "# Only get the first twenty tokens of every datapoint\n",
        "Token_amount = 20\n",
        "\n",
        "# Load the training set from pile-10k\n",
        "d = load_dataset(\"NeelNanda/pile-10k\", split=\"train\").map(\n",
        "    lambda x: model.tokenizer(x['text']),\n",
        "    batched=True,\n",
        ").filter(\n",
        "    lambda x: len(x['input_ids']) > Token_amount\n",
        ").map(\n",
        "    lambda x: {'input_ids': x['input_ids'][:Token_amount]}\n",
        ")\n",
        "neurons = model.W_in.shape[-1]\n",
        "datapoints = d.num_rows\n",
        "batch_size = 64\n",
        "\n",
        "neuron_activations = th.zeros((datapoints*Token_amount, neurons))\n",
        "\n",
        "with th.no_grad(), d.formatted_as(\"pt\"):\n",
        "    dl = DataLoader(d[\"input_ids\"], batch_size=batch_size)\n",
        "    for i, batch in enumerate(tqdm(dl)):\n",
        "        _, cache = model.run_with_cache(batch.to(device))\n",
        "        neuron_activations[i*batch_size*Token_amount:(i+1)*batch_size*Token_amount,:] = rearrange(cache[f\"blocks.{layer}.mlp.hook_post\"], \"b s n -> (b s) n\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7_8yfw0fk7",
        "outputId": "5537b48c-6713-49fd-fd19-5b22cac1f5c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gelu-2l into HookedTransformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d72bbad15316b3c8.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-86fde1c873d5aff7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a1da85ccbe1bd2ba.arrow\n",
            "100%|██████████| 156/156 [00:05<00:00, 26.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Top Examples"
      ],
      "metadata": {
        "id": "68Q5t6ik0j9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try looking at several neurons for like 5 minutes maximum. Some will be much more interpretable, and other's won't. "
      ],
      "metadata": {
        "id": "M1UWXJm738cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick our specific neuron\n",
        "neuron = 0\n",
        "# Top k datapoint examples\n",
        "k = 10\n",
        "simplifier = NeuronTextSimplifier(model, layer, neuron)\n",
        "\n",
        "values, indices = neuron_activations[:,neuron].topk(k)\n",
        "\n",
        "max_datapoints = [np.unravel_index(i, (datapoints, Token_amount)) for i in indices]\n",
        "\n",
        "text_list = []\n",
        "full_text = []\n",
        "for md, s_ind in max_datapoints:\n",
        "    md = int(md)\n",
        "    s_ind = int(s_ind)\n",
        "    # Get the full text\n",
        "    full_tok = th.tensor(d[md][\"input_ids\"])\n",
        "    full_text.append(model.tokenizer.decode(full_tok))\n",
        "    \n",
        "    # Get just the text up until the max-activating example\n",
        "    tok = d[md][\"input_ids\"][:s_ind+1]\n",
        "    text = model.tokenizer.decode(tok)\n",
        "    text_list.append(text)"
      ],
      "metadata": {
        "id": "5by6Ncx80kQx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the whole text's activation on that neuron.\n",
        "Blue is positive (also has a hovertip if you put your mouse over) and red is negative, which you can interpret as \"no activation\" as opposed to \"opposite\". \n",
        "\n",
        "NOTE: \"Layer\" & \"Neuron\" are fake UI elements. My code is a hack."
      ],
      "metadata": {
        "id": "OSn4AdL54UJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.text_to_visualize(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "0n85VlHA2FDX",
        "outputId": "f2284f71-7de0-4cdb-e2f2-c01f2b5de9e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f73515a3490>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-63fe4320-42db\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-63fe4320-42db\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \"@@\", \" START\", \" COPYRIGHT\", \" @\", \"@@\", \"\\\\newline\", \"//\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\\\newline\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\\\newline\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" high\", \" profile\", \" will\", \" help\", \" them\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \".\", \"\\n\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \" is\", \" the\", \" act\", \" of\", \" wearing\", \" campaign\", \" parap\", \"her\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" two\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"TE\", \"HR\", \"AN\", \" (\", \"Press\", \" Sh\", \"ia\", \" Agency\", \")\", \" \\u2013\", \" Israel\", \" protested\", \" to\", \" Jordan\", \" on\", \" Sunday\", \" after\", \" the\", \" spokeswoman\", \" for\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" two\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Student\", \" Union\", \":\", \" Can\", \" we\", \" have\", \" a\", \" resolution\", \" on\", \" Christianity\", \"-\", \"\\n\", \"Extra\", \"-\", \"oral\", \" fistula\", \" caused\", \" by\", \" a\", \" dental\", \" implant\", \".\", \"\\\\newline\", \"D\", \"ental\", \" implantation\", \" has\", \" become\", \" an\", \" important\", \" procedure\", \" for\", \"\\n\"], \"activations\": [[[-0.09887218475341797]], [[-0.14802312850952148]], [[-0.14366194605827332]], [[-0.15744367241859436]], [[-0.16533207893371582]], [[-0.13588988780975342]], [[-0.15886184573173523]], [[-0.1307990401983261]], [[0.42160627245903015]], [[0.040902040898799896]], [[-0.1398569792509079]], [[-0.020312048494815826]], [[-0.16934961080551147]], [[-0.09906524419784546]], [[-0.16964568197727203]], [[-0.07459450513124466]], [[0.9865639209747314]], [[0.9481028318405151]], [[0.28540685772895813]], [[2.5151894092559814]], [[0.0]], [[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[0.33888211846351624]], [[0.05219234526157379]], [[0.14035862684249878]], [[0.27355116605758667]], [[1.4552218914031982]], [[0.9012847542762756]], [[0.6713790893554688]], [[0.4472935199737549]], [[0.24280411005020142]], [[0.9782126545906067]], [[1.1879156827926636]], [[1.6260696649551392]], [[1.5775634050369263]], [[0.6751286387443542]], [[1.9010306596755981]], [[1.8627022504806519]], [[1.4051188230514526]], [[0.0]], [[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[0.33888211846351624]], [[0.05219234526157379]], [[0.14035862684249878]], [[0.27355116605758667]], [[1.4552218914031982]], [[0.9012847542762756]], [[0.6713790893554688]], [[0.4472935199737549]], [[0.24280411005020142]], [[0.9782126545906067]], [[1.1879156827926636]], [[1.6260696649551392]], [[1.5775634050369263]], [[0.6751286387443542]], [[1.9010306596755981]], [[1.8627022504806519]], [[1.4051188230514526]], [[0.0]], [[0.3403335213661194]], [[0.5585132241249084]], [[1.304084062576294]], [[0.8060926795005798]], [[0.6882948279380798]], [[1.036104679107666]], [[0.7222857475280762]], [[0.33937525749206543]], [[0.06825684756040573]], [[0.11842165142297745]], [[1.1280287504196167]], [[1.2536267042160034]], [[1.0960564613342285]], [[1.272553563117981]], [[0.9714876413345337]], [[0.7141429781913757]], [[0.47745394706726074]], [[0.40084558725357056]], [[1.7530765533447266]], [[0.4764714241027832]], [[0.0]], [[-0.1560264527797699]], [[-0.16111114621162415]], [[-0.09184077382087708]], [[-0.08249937742948532]], [[-0.06847098469734192]], [[-0.09613284468650818]], [[-0.1678568422794342]], [[-0.1353330761194229]], [[0.036966659128665924]], [[0.13746277987957]], [[0.09602191299200058]], [[0.11003806442022324]], [[1.2584477663040161]], [[1.7462037801742554]], [[1.0256941318511963]], [[0.960261344909668]], [[-0.06743725389242172]], [[0.4457358717918396]], [[0.48426440358161926]], [[0.03217647224664688]], [[0.0]], [[-0.11763390898704529]], [[-0.08076704293489456]], [[0.12169582396745682]], [[0.06359822303056717]], [[0.08625131845474243]], [[-0.1300266832113266]], [[-0.06444206088781357]], [[0.32874688506126404]], [[1.7451539039611816]], [[0.010316566564142704]], [[-0.08823926001787186]], [[0.02970852144062519]], [[0.9366324543952942]], [[0.4551143944263458]], [[-0.16291776299476624]], [[0.20516113936901093]], [[0.2907007336616516]], [[0.42264607548713684]], [[0.4911215007305145]], [[1.6946460008621216]], [[0.0]], [[0.851527988910675]], [[0.4749625027179718]], [[1.0295218229293823]], [[0.525887131690979]], [[0.7040841579437256]], [[-0.06543657183647156]], [[0.11412101984024048]], [[0.7839542627334595]], [[1.4998246431350708]], [[0.7603797912597656]], [[1.7095104455947876]], [[0.30957216024398804]], [[0.14066153764724731]], [[0.13585546612739563]], [[-0.050556786358356476]], [[0.6576266884803772]], [[0.16144147515296936]], [[0.5722848176956177]], [[0.9409554600715637]], [[0.22934332489967346]], [[0.0]], [[-0.11763390898704529]], [[-0.08076704293489456]], [[0.12169582396745682]], [[0.06359822303056717]], [[0.08625131845474243]], [[-0.1300266832113266]], [[-0.06444206088781357]], [[0.32874688506126404]], [[1.7451539039611816]], [[0.010316566564142704]], [[-0.08823926001787186]], [[0.02970852144062519]], [[0.9366324543952942]], [[0.4551143944263458]], [[-0.16291776299476624]], [[0.20516113936901093]], [[0.2907007336616516]], [[0.42264607548713684]], [[0.4911215007305145]], [[1.6946460008621216]], [[0.0]], [[0.45668187737464905]], [[0.2664715647697449]], [[-0.017658734694123268]], [[0.2215522974729538]], [[0.36081454157829285]], [[0.33310502767562866]], [[-0.06034082546830177]], [[-0.1317916065454483]], [[0.6790573000907898]], [[0.8200266361236572]], [[0.5117771625518799]], [[0.30500391125679016]], [[0.7682309746742249]], [[-0.16013240814208984]], [[-0.11051265895366669]], [[-0.169698104262352]], [[0.3448892831802368]], [[-0.06071978062391281]], [[1.334957480430603]], [[1.657362937927246]], [[0.0]], [[0.17135944962501526]], [[0.22131431102752686]], [[0.19895461201667786]], [[0.9599190950393677]], [[0.3754573166370392]], [[0.47558140754699707]], [[-0.092800073325634]], [[0.3200685679912567]], [[1.6346697807312012]], [[0.3339213728904724]], [[0.02879660576581955]], [[-0.16979047656059265]], [[0.10031436383724213]], [[0.921159565448761]], [[0.6405698657035828]], [[1.3121068477630615]], [[0.4110022187232971]], [[1.0429216623306274]], [[1.3884989023208618]], [[0.5261064171791077]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we add gaussian noise to each token and see the effect on the last token? So for 20 token-text, we'll add noise to the first token, run the model, save that activation, then repeat with adding noise to the second token (but not the first). Below you'll see Red, as in adding noise to this token caused a decrease in activation. Some evidence that this token is important. "
      ],
      "metadata": {
        "id": "mC-r5dmX4jL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.add_noise_to_text(text_list, noise_level=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ucuNzgxz2K9u",
        "outputId": "f7d2b5d0-5507-4ba2-9805-52f90fd8ff0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f73288f6700>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-2575cfa7-b872\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-2575cfa7-b872\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \"@@\", \" START\", \" COPYRIGHT\", \" @\", \"@@\", \"\\\\newline\", \"//\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" high\", \" profile\", \" will\", \" help\", \" them\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \" is\", \" the\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"TE\", \"HR\", \"AN\", \" (\", \"Press\", \" Sh\", \"ia\", \" Agency\", \")\", \" \\u2013\", \" Israel\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" two\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Student\", \" Union\", \":\", \" Can\", \" we\", \" have\", \" a\", \" resolution\", \" on\", \" Christianity\", \"-\", \"\\n\", \"Extra\", \"-\", \"oral\", \" fistula\", \" caused\", \" by\", \" a\", \" dental\", \" implant\", \"\\n\"], \"activations\": [[[-0.026813983917236328]], [[-0.40910714864730835]], [[0.03920769691467285]], [[0.2623910903930664]], [[-0.18049943447113037]], [[-0.049201250076293945]], [[0.017575383186340332]], [[0.08227074146270752]], [[-0.07872319221496582]], [[0.023160457611083984]], [[0.04673922061920166]], [[-0.05655848979949951]], [[0.0140913724899292]], [[-0.09589850902557373]], [[0.052016377449035645]], [[-0.45757144689559937]], [[-0.3185994625091553]], [[-0.06798160076141357]], [[-0.3443434238433838]], [[-1.3621805906295776]], [[0.0]], [[-0.07410308718681335]], [[-0.035348325967788696]], [[0.01597854495048523]], [[-0.0472433865070343]], [[-0.053160786628723145]], [[-0.049908339977264404]], [[-0.0544281005859375]], [[-0.054171234369277954]], [[-0.10862687230110168]], [[0.06443947553634644]], [[-0.12020078301429749]], [[0.0016169548034667969]], [[-0.03943803906440735]], [[0.029422610998153687]], [[-0.0288010835647583]], [[-0.0871286392211914]], [[0.017574995756149292]], [[-0.5490225553512573]], [[0.0]], [[0.04999983310699463]], [[0.07408612966537476]], [[0.12370699644088745]], [[0.47024911642074585]], [[0.053856849670410156]], [[0.10151201486587524]], [[0.13256710767745972]], [[0.0664631724357605]], [[0.12412625551223755]], [[0.26244717836380005]], [[0.2135511040687561]], [[0.1309528946876526]], [[0.09164446592330933]], [[0.16982370615005493]], [[0.11798650026321411]], [[0.025202572345733643]], [[-0.24489492177963257]], [[0.007955312728881836]], [[-1.105408787727356]], [[0.0]], [[-0.23831157386302948]], [[-0.11620721966028214]], [[-0.1257966160774231]], [[0.4052644371986389]], [[0.1399705559015274]], [[-0.061739563941955566]], [[-0.05598427355289459]], [[-0.024414584040641785]], [[-0.06627872586250305]], [[-0.045112401247024536]], [[-0.007399275898933411]], [[0.050447508692741394]], [[-0.03473569452762604]], [[-0.09816565364599228]], [[0.019930005073547363]], [[-0.098673515021801]], [[-0.013840004801750183]], [[-0.11143733561038971]], [[-0.3082435727119446]], [[0.0]], [[-0.10115137696266174]], [[-0.035277508199214935]], [[-0.11434077471494675]], [[-0.03068821132183075]], [[-0.0417931005358696]], [[-0.05908457189798355]], [[-0.10128320753574371]], [[-0.06442788988351822]], [[0.08163496851921082]], [[0.03141780197620392]], [[0.06567360460758209]], [[0.039712533354759216]], [[0.0007975101470947266]], [[-0.20510254800319672]], [[0.0]], [[0.04307052493095398]], [[-0.05528338998556137]], [[0.019633173942565918]], [[-0.05804663151502609]], [[0.03746254742145538]], [[-0.016948603093624115]], [[-0.11217502504587173]], [[-0.23689192533493042]], [[-0.2872113287448883]], [[0.0]], [[-0.45299839973449707]], [[-0.1393653154373169]], [[-0.7580570578575134]], [[-1.5497117042541504]], [[-0.47114038467407227]], [[-0.019337892532348633]], [[0.23765432834625244]], [[-0.5004264116287231]], [[-1.2953020334243774]], [[-0.17149138450622559]], [[-1.4056105613708496]], [[0.0]], [[0.04668130725622177]], [[0.019442133605480194]], [[0.06736303120851517]], [[0.060140080749988556]], [[0.06338959187269211]], [[0.0023174360394477844]], [[0.1002260074019432]], [[-0.07590240240097046]], [[0.1321389377117157]], [[0.05027350038290024]], [[-0.06012386828660965]], [[-0.010515853762626648]], [[-0.06126714497804642]], [[-0.10273036360740662]], [[0.015712380409240723]], [[-0.0428791381418705]], [[-0.00023309141397476196]], [[-0.015033528208732605]], [[-0.10469972342252731]], [[-0.2461218535900116]], [[0.0]], [[0.01128716766834259]], [[0.0612117201089859]], [[0.06064151972532272]], [[0.005068058148026466]], [[-0.012626353651285172]], [[0.05358690768480301]], [[0.04524686187505722]], [[0.03719916194677353]], [[-0.02145044133067131]], [[0.03630031645298004]], [[-0.047234877943992615]], [[0.08070532977581024]], [[0.1883130669593811]], [[-0.02005787193775177]], [[0.15421518683433533]], [[-0.031401339918375015]], [[-0.0474662259221077]], [[0.0546569749712944]], [[-0.11749614030122757]], [[-0.13917037844657898]], [[0.0]], [[-0.04196485877037048]], [[0.04276638478040695]], [[-0.02873513102531433]], [[0.07052592188119888]], [[-0.103119395673275]], [[-0.07028770446777344]], [[0.028675951063632965]], [[-0.04904770851135254]], [[-0.2799701392650604]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove each token that has the least effect on the last token's neuron activation. Should see something similar to the noise i.e. the tokens that have the most effect when noised should be the last tokens to be removed below. \n",
        "\n",
        "NOTE: change \"Samples per page\" to a larger number to see more. "
      ],
      "metadata": {
        "id": "ntx2GfSa3U4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.visualize_text_color_iteratively(text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oPYLkh9A2QM5",
        "outputId": "0c491614-47b6-4224-d5bc-d1eee5fdd301"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f73288f66a0>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-8d49c9ea-3ee1\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-8d49c9ea-3ee1\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [[\"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \"@@\", \" START\", \" COPYRIGHT\", \" @\", \"@@\", \"\\\\newline\", \"//\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \"@@\", \" START\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"//\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \"@@\", \" START\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"//\", \" @\", \" START\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" @\", \" START\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" @\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" COPYRIGHT\", \" @\", \"\\\\newline\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \" COPYRIGHT\", \"\\\\newline\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"\\\\newline\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"****************************************************************\", \"******\", \"\\\\newline\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"******\", \"\\\\newline\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"******\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"\\\\newline\", \" Licensed\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"\\\\newline\", \" the\", \" Apache\", \" Software\", \"\\n\", \"/\", \"\\\\newline\", \" Apache\", \" Software\", \"\\n\", \"/\", \" Apache\", \" Software\", \"\\n\", \"/\", \" Software\", \"\\n\", \" Software\", \"\\n\"], [\"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" notice\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" Copyright\", \" 2\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" Copyright\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"RPC\", \"\\n\", \"\\ufeff\", \"\\n\"], [\"\\ufeff\", \"#\", \"region\", \" Copyright\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \"//\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \" license\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \" Authors\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" The\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" notice\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" and\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"1\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"0\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"\\\\newline\", \" Copyright\", \" 2\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" Copyright\", \" 2\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" Copyright\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \"9\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \"\\\\newline\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"#\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \" g\", \"RPC\", \"\\n\", \"\\ufeff\", \"RPC\", \"\\n\", \"\\ufeff\", \"\\n\"], [\"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" high\", \" profile\", \" will\", \" help\", \" them\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" high\", \" profile\", \" will\", \" help\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" profile\", \" will\", \" help\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \".\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" profile\", \" help\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" profile\", \" help\", \" frame\", \" the\", \" emerging\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \" Bush\", \"\\u2019\", \"s\", \" new\", \" profile\", \" help\", \" frame\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \" Bush\", \"s\", \" new\", \" profile\", \" help\", \" frame\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \" Bush\", \"s\", \" new\", \" profile\", \" help\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Mr\", \" Bush\", \" new\", \" profile\", \" help\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Bush\", \" new\", \" profile\", \" help\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Bush\", \" new\", \" profile\", \" the\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Bush\", \" new\", \" profile\", \" Republican\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" hope\", \" Bush\", \" new\", \" profile\", \" generation\", \"\\n\", \"But\", \" Democrats\", \" Bush\", \" new\", \" profile\", \" generation\", \"\\n\", \"But\", \" Bush\", \" new\", \" profile\", \" generation\", \"\\n\", \"But\", \" new\", \" profile\", \" generation\", \"\\n\", \"But\", \" new\", \" generation\", \"\\n\", \"But\", \" generation\", \"\\n\", \"But\", \"\\n\"], [\"Pass\", \"ive\", \" election\", \"e\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"ive\", \" election\", \"e\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"ive\", \"e\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"ive\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \"ive\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"ering\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"\\\\newline\", \"\\\\newline\", \"Pass\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"\\\\newline\", \"Pass\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"\\\\newline\", \" election\", \"ering\", \" is\", \" the\", \"\\n\", \"Pass\", \"\\\\newline\", \" election\", \" is\", \" the\", \"\\n\", \"Pass\", \" election\", \" is\", \" the\", \"\\n\", \"Pass\", \" is\", \" the\", \"\\n\", \"Pass\", \" is\", \"\\n\", \"Pass\", \"\\n\"], [\"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \" Palestine\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \" Staff\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \"The\", \" politically\", \"-\", \"\\n\", \"By\", \" politically\", \"-\", \"\\n\", \" politically\", \"-\", \"\\n\", \" politically\", \"\\n\"], [\"TE\", \"HR\", \"AN\", \" (\", \"Press\", \" Sh\", \"ia\", \" Agency\", \")\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"HR\", \"AN\", \" (\", \"Press\", \" Sh\", \" Agency\", \")\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"HR\", \"AN\", \"Press\", \" Sh\", \" Agency\", \")\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"HR\", \"AN\", \"Press\", \" Sh\", \" Agency\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"HR\", \"AN\", \" Sh\", \" Agency\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"AN\", \" Sh\", \" Agency\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \"AN\", \" Agency\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \" Agency\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \" \\u2013\", \" Israel\", \"\\n\", \"TE\", \" Israel\", \"\\n\", \"TE\", \"\\n\"], [\"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \" politically\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" two\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" two\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"By\", \" Palestine\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" young\", \" American\", \" athletes\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"-\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"s\", \"ymb\", \"olic\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"ymb\", \"olic\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \" Staff\", \"\\\\newline\", \"\\\\newline\", \"The\", \"ymb\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"\\\\newline\", \"\\\\newline\", \"The\", \"ymb\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"\\\\newline\", \"The\", \"ymb\", \" act\", \" of\", \" young\", \" American\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"\\\\newline\", \"The\", \"ymb\", \" act\", \" of\", \" young\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"\\\\newline\", \"The\", \" act\", \" of\", \" young\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"The\", \" act\", \" of\", \" young\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"The\", \" act\", \" of\", \" has\", \" been\", \"\\n\", \"By\", \" Chronicle\", \"The\", \" act\", \" has\", \" been\", \"\\n\", \"By\", \"The\", \" act\", \" has\", \" been\", \"\\n\", \"The\", \" act\", \" has\", \" been\", \"\\n\", \"The\", \" has\", \" been\", \"\\n\", \"The\", \" been\", \"\\n\", \"The\", \"\\n\"], [\"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Student\", \" Union\", \":\", \" Can\", \" we\", \" have\", \" a\", \" resolution\", \" on\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Student\", \" Union\", \":\", \" Can\", \" we\", \" a\", \" resolution\", \" on\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Student\", \" Union\", \":\", \" Can\", \" we\", \" a\", \" resolution\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \":\", \" Can\", \" we\", \" a\", \" resolution\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \":\", \" we\", \" a\", \" resolution\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \" we\", \" a\", \" resolution\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \" we\", \" a\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \" we\", \" a\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"L\", \"SE\", \" Union\", \" we\", \" a\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"SE\", \" Union\", \" we\", \" a\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"\\\\newline\", \"EV\", \"ENTS\", \"\\\\newline\", \"SE\", \" Union\", \" we\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"EV\", \"ENTS\", \"\\\\newline\", \"SE\", \" Union\", \" we\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"EV\", \"ENTS\", \"\\\\newline\", \"SE\", \" we\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"ENTS\", \"\\\\newline\", \"SE\", \" we\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"ENTS\", \"\\\\newline\", \"SE\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"ENTS\", \"SE\", \" Christianity\", \"-\", \"\\n\", \"Categories\", \"ENTS\", \"SE\", \" Christianity\", \"\\n\", \"Categories\", \"SE\", \" Christianity\", \"\\n\", \"Categories\", \" Christianity\", \"\\n\", \"Categories\", \"\\n\"], [\"Extra\", \"-\", \"oral\", \" fistula\", \" caused\", \" by\", \" a\", \" dental\", \" implant\", \"\\n\", \"Extra\", \"-\", \"oral\", \" fistula\", \" by\", \" a\", \" dental\", \" implant\", \"\\n\", \"Extra\", \"-\", \"oral\", \" fistula\", \" by\", \" dental\", \" implant\", \"\\n\", \"Extra\", \"-\", \"oral\", \" fistula\", \" by\", \" implant\", \"\\n\", \"Extra\", \"oral\", \" fistula\", \" by\", \" implant\", \"\\n\", \"Extra\", \" fistula\", \" by\", \" implant\", \"\\n\", \"Extra\", \" by\", \" implant\", \"\\n\", \"Extra\", \" implant\", \"\\n\", \" implant\", \"\\n\"]], \"activations\": [[[[-0.09887218475341797]], [[-0.14802312850952148]], [[-0.14366194605827332]], [[-0.15744367241859436]], [[-0.16533207893371582]], [[-0.13588988780975342]], [[-0.15886184573173523]], [[-0.1307990401983261]], [[0.42160627245903015]], [[0.040902040898799896]], [[-0.1398569792509079]], [[-0.020312048494815826]], [[-0.16934961080551147]], [[-0.09906524419784546]], [[-0.16964568197727203]], [[-0.07459450513124466]], [[0.9865639209747314]], [[0.9481028318405151]], [[0.28540685772895813]], [[2.5151894092559814]], [[0.0]], [[-0.09887218475341797]], [[-0.14802312850952148]], [[-0.14366194605827332]], [[-0.15744367241859436]], [[-0.16533207893371582]], [[-0.13588988780975342]], [[-0.15886184573173523]], [[-0.1307990401983261]], [[0.42160627245903015]], [[0.040902040898799896]], [[0.28307363390922546]], [[-0.11440978944301605]], [[-0.0006103445775806904]], [[-0.15884622931480408]], [[0.03245658054947853]], [[1.1992993354797363]], [[1.1222093105316162]], [[0.34929823875427246]], [[2.648746967315674]], [[0.0]], [[-0.09887218475341797]], [[-0.14802312850952148]], [[-0.14366194605827332]], [[-0.15744367241859436]], [[-0.16533207893371582]], [[-0.13588988780975342]], [[-0.15886184573173523]], [[-0.1307990401983261]], [[0.42160627245903015]], [[0.040902040898799896]], [[0.28307363390922546]], [[0.07818600535392761]], [[-0.16680657863616943]], [[0.10760556161403656]], [[1.3062020540237427]], [[1.2922344207763672]], [[0.4213195741176605]], [[2.7940118312835693]], [[0.0]], [[-0.09887218475341797]], [[-0.14802312850952148]], [[-0.14366194605827332]], [[-0.15744367241859436]], [[-0.16533207893371582]], [[-0.13588988780975342]], [[-0.13443361222743988]], [[0.47805312275886536]], [[0.09768591821193695]], [[0.4035487174987793]], [[0.1171756461262703]], [[-0.15141914784908295]], [[0.16948260366916656]], [[1.4236842393875122]], [[1.4100892543792725]], [[0.46342772245407104]], [[2.836334705352783]], [[0.0]], [[-0.09887225925922394]], [[-0.14802318811416626]], [[-0.14366188645362854]], [[-0.15744361281394958]], [[-0.16211919486522675]], [[-0.1473432332277298]], [[0.5198245644569397]], [[0.052455030381679535]], [[0.4390087127685547]], [[0.14538805186748505]], [[-0.14810094237327576]], [[0.20143909752368927]], [[1.4885807037353516]], [[1.4747002124786377]], [[0.559824526309967]], [[2.8926477432250977]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[-0.16211915016174316]], [[0.3431289494037628]], [[0.3517691493034363]], [[0.6460463404655457]], [[0.1493312269449234]], [[-0.09507087618112564]], [[0.149872824549675]], [[1.533041000366211]], [[1.4646832942962646]], [[0.558466911315918]], [[2.905311107635498]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[0.11437536776065826]], [[0.08955159038305283]], [[0.8842920660972595]], [[0.20113925635814667]], [[-0.07698187977075577]], [[0.1568620353937149]], [[1.5351279973983765]], [[1.462447166442871]], [[0.5126980543136597]], [[2.900486469268799]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[0.11437536776065826]], [[0.08955159038305283]], [[0.8842920660972595]], [[0.20113925635814667]], [[-0.07698187977075577]], [[0.1568620353937149]], [[1.8460116386413574]], [[0.8493747711181641]], [[2.902392864227295]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[0.11437536776065826]], [[0.08955159038305283]], [[0.8842920660972595]], [[0.20113925635814667]], [[-0.07924903184175491]], [[1.738133192062378]], [[0.8926308751106262]], [[2.890784740447998]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[0.11437536776065826]], [[1.069550633430481]], [[0.30456310510635376]], [[0.06812266260385513]], [[1.6489999294281006]], [[0.9312391877174377]], [[2.8357696533203125]], [[0.0]], [[-0.09887218475341797]], [[-0.14802323281764984]], [[-0.14366193115711212]], [[-0.15744361281394958]], [[-0.14207112789154053]], [[-0.15511919558048248]], [[-0.16340608894824982]], [[1.3440932035446167]], [[0.7222484946250916]], [[2.8794479370117188]], [[0.0]], [[-0.09887219220399857]], [[-0.14802326261997223]], [[-0.14366191625595093]], [[-0.15744370222091675]], [[-0.1420711725950241]], [[-0.15011891722679138]], [[1.3075662851333618]], [[0.678467333316803]], [[2.793194532394409]], [[0.0]], [[-0.09887218475341797]], [[-0.14560118317604065]], [[-0.14211010932922363]], [[-0.15939852595329285]], [[-0.1688573956489563]], [[1.031348466873169]], [[0.49842360615730286]], [[2.678389310836792]], [[0.0]], [[-0.09887240827083588]], [[-0.1456013321876526]], [[-0.1421101987361908]], [[-0.169392392039299]], [[0.9791580438613892]], [[0.4478638470172882]], [[2.525804281234741]], [[0.0]], [[-0.09887228906154633]], [[-0.043670084327459335]], [[-0.1338229477405548]], [[0.9508781433105469]], [[0.4532136917114258]], [[2.441781759262085]], [[0.0]], [[-0.09887228906154633]], [[-0.043670084327459335]], [[-0.09200581163167953]], [[-0.08515552431344986]], [[2.3178882598876953]], [[0.0]], [[-0.09887216240167618]], [[-0.04366998001933098]], [[-0.16961735486984253]], [[1.9897809028625488]], [[0.0]], [[-0.09887216240167618]], [[-0.1673433929681778]], [[1.3931137323379517]], [[0.0]], [[-0.09887216240167618]], [[0.16060395538806915]], [[0.0]], [[0.37699124217033386]], [[0.0]]], [[[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[0.33888211846351624]], [[0.05219234526157379]], [[0.14035862684249878]], [[0.27355116605758667]], [[1.4552218914031982]], [[0.9012847542762756]], [[0.6713790893554688]], [[0.4472935199737549]], [[0.24280411005020142]], [[0.9782126545906067]], [[1.1879156827926636]], [[1.6260696649551392]], [[1.5775634050369263]], [[0.6751286387443542]], [[1.9010306596755981]], [[0.0]], [[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[0.33888211846351624]], [[0.05219234526157379]], [[0.14035862684249878]], [[0.27355116605758667]], [[1.4552218914031982]], [[0.9012847542762756]], [[0.44246578216552734]], [[0.4989829957485199]], [[1.1870678663253784]], [[1.253124713897705]], [[1.8781609535217285]], [[1.5521602630615234]], [[0.6670510172843933]], [[2.213580846786499]], [[0.0]], [[-0.05925064906477928]], [[-0.06868380308151245]], [[-0.014122473075985909]], [[0.33888429403305054]], [[0.052192553877830505]], [[0.14035893976688385]], [[0.2735515534877777]], [[1.455222487449646]], [[0.9012853503227234]], [[0.44246619939804077]], [[0.4989839196205139]], [[1.1870702505111694]], [[1.2531251907348633]], [[1.8781620264053345]], [[1.3558697700500488]], [[2.3380823135375977]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.014122864231467247]], [[-0.03901303559541702]], [[-0.026413913816213608]], [[0.1037377193570137]], [[0.5300822854042053]], [[0.3520251214504242]], [[0.3493664264678955]], [[0.5358079075813293]], [[1.4051716327667236]], [[1.3946856260299683]], [[1.774596929550171]], [[1.4468061923980713]], [[2.4041409492492676]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.014122864231467247]], [[-0.03901303559541702]], [[-0.026413913816213608]], [[0.05653071776032448]], [[0.13581706583499908]], [[0.26753708720207214]], [[0.5585264563560486]], [[1.3756972551345825]], [[1.3425557613372803]], [[1.8311363458633423]], [[1.383543610572815]], [[2.4780991077423096]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.1394525170326233]], [[-0.10681989043951035]], [[0.02395705133676529]], [[0.09584306180477142]], [[0.295619934797287]], [[0.5364474058151245]], [[1.3384968042373657]], [[1.3230866193771362]], [[1.846450924873352]], [[1.4227678775787354]], [[2.467895269393921]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.0778692439198494]], [[-0.005312031134963036]], [[0.11676882952451706]], [[0.26755276322364807]], [[0.5871695280075073]], [[1.337204933166504]], [[1.4084601402282715]], [[1.9210952520370483]], [[1.372038722038269]], [[2.5560851097106934]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.07856431603431702]], [[0.08327613770961761]], [[0.21480035781860352]], [[0.5318485498428345]], [[1.4030306339263916]], [[1.4273583889007568]], [[1.9465168714523315]], [[1.5006554126739502]], [[2.543466567993164]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.07856431603431702]], [[0.08327613770961761]], [[0.21480035781860352]], [[0.5318485498428345]], [[1.4030306339263916]], [[1.8762601613998413]], [[1.3977441787719727]], [[2.5884735584259033]], [[0.0]], [[-0.059250809252262115]], [[-0.06868381798267365]], [[-0.0785643458366394]], [[0.08327668905258179]], [[0.21480028331279755]], [[0.5318491458892822]], [[1.5929924249649048]], [[1.351834774017334]], [[2.5324037075042725]], [[0.0]], [[-0.059250764548778534]], [[-0.06868388503789902]], [[-0.07856455445289612]], [[0.14374394714832306]], [[0.41966474056243896]], [[1.5111929178237915]], [[1.138673186302185]], [[2.3982155323028564]], [[0.0]], [[-0.05925111845135689]], [[-0.06868433207273483]], [[-0.07856448739767075]], [[0.14374332129955292]], [[0.8218237161636353]], [[1.1879723072052002]], [[2.135967254638672]], [[0.0]], [[-0.059251051396131516]], [[-0.06868399679660797]], [[-0.07856462150812149]], [[-0.02882525511085987]], [[0.052042849361896515]], [[1.7472727298736572]], [[0.0]], [[-0.059251051396131516]], [[-0.06868399679660797]], [[-0.07856462150812149]], [[-0.15902723371982574]], [[1.3743716478347778]], [[0.0]], [[-0.05925073102116585]], [[-0.06868381798267365]], [[0.04561245068907738]], [[0.8504723310470581]], [[0.0]], [[-0.05925073102116585]], [[0.0007300293655134737]], [[1.1397446393966675]], [[0.0]], [[-0.05925073102116585]], [[0.25221866369247437]], [[0.0]], [[-0.05925123393535614]], [[0.0]]], [[[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[0.33888211846351624]], [[0.05219234526157379]], [[0.14035862684249878]], [[0.27355116605758667]], [[1.4552218914031982]], [[0.9012847542762756]], [[0.6713790893554688]], [[0.4472935199737549]], [[0.24280411005020142]], [[0.9782126545906067]], [[1.1879156827926636]], [[1.6260696649551392]], [[1.5775634050369263]], [[0.6751286387443542]], [[1.9010306596755981]], [[1.8627022504806519]], [[0.0]], [[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.014122985303401947]], [[-0.039013296365737915]], [[-0.026413913816213608]], [[0.10373737663030624]], [[0.5300837159156799]], [[0.3520250916481018]], [[0.25065895915031433]], [[0.3708367645740509]], [[0.3384610116481781]], [[1.2728174924850464]], [[1.225498080253601]], [[1.5024038553237915]], [[2.0372314453125]], [[0.6596093773841858]], [[2.04374361038208]], [[2.1285665035247803]], [[0.0]], [[-0.0592513307929039]], [[-0.06868410855531693]], [[-0.13945254683494568]], [[-0.1068200021982193]], [[0.06223386153578758]], [[0.42648041248321533]], [[0.3360285758972168]], [[0.23045267164707184]], [[0.4208458364009857]], [[0.37264034152030945]], [[1.2658839225769043]], [[1.225578784942627]], [[1.4286690950393677]], [[2.010982036590576]], [[0.7661567330360413]], [[2.1043219566345215]], [[2.23663330078125]], [[0.0]], [[-0.05925064906477928]], [[-0.06868380308151245]], [[-0.1394524872303009]], [[-0.10681982338428497]], [[0.062234461307525635]], [[0.42648079991340637]], [[0.3360285460948944]], [[0.35695698857307434]], [[0.5440427660942078]], [[1.3992527723312378]], [[1.322137713432312]], [[1.7814421653747559]], [[1.8609397411346436]], [[0.7225931286811829]], [[2.250789165496826]], [[2.2806739807128906]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.1394525170326233]], [[-0.10681989043951035]], [[0.02395705133676529]], [[0.09584306180477142]], [[0.295619934797287]], [[0.5364474058151245]], [[1.3384968042373657]], [[1.3230866193771362]], [[1.846450924873352]], [[1.9213355779647827]], [[0.6728714108467102]], [[2.2866013050079346]], [[2.2558116912841797]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.1394525170326233]], [[-0.10681989043951035]], [[0.02395705133676529]], [[0.09584306180477142]], [[0.295619934797287]], [[0.5364474058151245]], [[1.3384968042373657]], [[1.3230866193771362]], [[1.846450924873352]], [[1.9213355779647827]], [[0.6728714108467102]], [[2.2866013050079346]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.1394525170326233]], [[-0.10681989043951035]], [[0.02395705133676529]], [[0.09584306180477142]], [[0.295619934797287]], [[0.5364474058151245]], [[1.3384968042373657]], [[1.3230866193771362]], [[1.846450924873352]], [[1.4227678775787354]], [[2.467895269393921]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.0778692439198494]], [[-0.005312031134963036]], [[0.11676882952451706]], [[0.26755276322364807]], [[0.5871695280075073]], [[1.337204933166504]], [[1.4084601402282715]], [[1.9210952520370483]], [[1.372038722038269]], [[2.5560851097106934]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.07856431603431702]], [[0.08327613770961761]], [[0.21480035781860352]], [[0.5318485498428345]], [[1.4030306339263916]], [[1.4273583889007568]], [[1.9465168714523315]], [[1.5006554126739502]], [[2.543466567993164]], [[0.0]], [[-0.05925099924206734]], [[-0.06868384033441544]], [[-0.07856431603431702]], [[0.08327613770961761]], [[0.21480035781860352]], [[0.5318485498428345]], [[1.4030306339263916]], [[1.8762601613998413]], [[1.3977441787719727]], [[2.5884735584259033]], [[0.0]], [[-0.059250809252262115]], [[-0.06868381798267365]], [[-0.0785643458366394]], [[0.08327668905258179]], [[0.21480028331279755]], [[0.5318491458892822]], [[1.5929924249649048]], [[1.351834774017334]], [[2.5324037075042725]], [[0.0]], [[-0.059250764548778534]], [[-0.06868388503789902]], [[-0.07856455445289612]], [[0.14374394714832306]], [[0.41966474056243896]], [[1.5111929178237915]], [[1.138673186302185]], [[2.3982155323028564]], [[0.0]], [[-0.05925111845135689]], [[-0.06868433207273483]], [[-0.07856448739767075]], [[0.14374332129955292]], [[0.8218237161636353]], [[1.1879723072052002]], [[2.135967254638672]], [[0.0]], [[-0.059251051396131516]], [[-0.06868399679660797]], [[-0.07856462150812149]], [[-0.02882525511085987]], [[0.052042849361896515]], [[1.7472727298736572]], [[0.0]], [[-0.059251051396131516]], [[-0.06868399679660797]], [[-0.07856462150812149]], [[-0.15902723371982574]], [[1.3743716478347778]], [[0.0]], [[-0.05925073102116585]], [[-0.06868381798267365]], [[0.04561245068907738]], [[0.8504723310470581]], [[0.0]], [[-0.05925073102116585]], [[0.0007300293655134737]], [[1.1397446393966675]], [[0.0]], [[-0.05925073102116585]], [[0.25221866369247437]], [[0.0]], [[-0.05925123393535614]], [[0.0]]], [[[0.3403335213661194]], [[0.5585132241249084]], [[1.304084062576294]], [[0.8060926795005798]], [[0.6882948279380798]], [[1.036104679107666]], [[0.7222857475280762]], [[0.33937525749206543]], [[0.06825684756040573]], [[0.11842165142297745]], [[1.1280287504196167]], [[1.2536267042160034]], [[1.0960564613342285]], [[1.272553563117981]], [[0.9714876413345337]], [[0.7141429781913757]], [[0.47745394706726074]], [[0.40084558725357056]], [[1.7530765533447266]], [[0.0]], [[0.3403335213661194]], [[0.5585132241249084]], [[1.304084062576294]], [[0.8060926795005798]], [[0.6882948279380798]], [[1.036104679107666]], [[0.7222857475280762]], [[0.33937525749206543]], [[0.06825684756040573]], [[0.11842165142297745]], [[1.1280287504196167]], [[1.2536267042160034]], [[1.0960564613342285]], [[1.6067129373550415]], [[1.0915380716323853]], [[0.5658013820648193]], [[0.46880239248275757]], [[1.846937894821167]], [[0.0]], [[0.3403335213661194]], [[0.5585132241249084]], [[1.304084062576294]], [[0.8060926795005798]], [[0.6882948279380798]], [[1.036104679107666]], [[0.7222857475280762]], [[0.33937525749206543]], [[0.06825684756040573]], [[1.2985411882400513]], [[1.363282322883606]], [[1.262444257736206]], [[1.7106415033340454]], [[1.1358228921890259]], [[0.5611215829849243]], [[0.4879966080188751]], [[1.9188636541366577]], [[0.0]], [[0.3403336703777313]], [[0.5585135221481323]], [[1.3040838241577148]], [[0.8060922622680664]], [[0.688294529914856]], [[1.0361040830612183]], [[0.7222859263420105]], [[0.3393752872943878]], [[0.06825669854879379]], [[1.2985409498214722]], [[1.6312469244003296]], [[1.7663519382476807]], [[1.1570444107055664]], [[0.6374046802520752]], [[0.5626294612884521]], [[1.9688845872879028]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[0.8060930371284485]], [[0.9125929474830627]], [[0.781277060508728]], [[0.398391455411911]], [[0.14906056225299835]], [[1.36147940158844]], [[1.6919795274734497]], [[1.7886378765106201]], [[1.1521210670471191]], [[0.664553165435791]], [[0.6019172072410583]], [[2.0347821712493896]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[0.8060930371284485]], [[0.9125929474830627]], [[0.781277060508728]], [[0.398391455411911]], [[0.14906056225299835]], [[1.36147940158844]], [[1.6919795274734497]], [[1.7886378765106201]], [[1.1521210670471191]], [[0.5989260673522949]], [[2.0721888542175293]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[0.8060930371284485]], [[0.9125929474830627]], [[0.7497527003288269]], [[0.2763652503490448]], [[1.4043090343475342]], [[1.703722357749939]], [[1.8655954599380493]], [[1.1495931148529053]], [[0.6226780414581299]], [[2.09499192237854]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[0.8060930371284485]], [[0.9125929474830627]], [[0.7497527003288269]], [[0.2763652503490448]], [[1.4043090343475342]], [[1.703722357749939]], [[1.487694263458252]], [[0.6867336630821228]], [[2.118420362472534]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[0.8060930371284485]], [[0.9125929474830627]], [[0.47583478689193726]], [[1.35086989402771]], [[1.685883641242981]], [[1.5012543201446533]], [[0.7326095700263977]], [[2.090142011642456]], [[0.0]], [[0.34033384919166565]], [[0.5585131645202637]], [[1.3040839433670044]], [[1.0634082555770874]], [[0.6330425143241882]], [[1.4885135889053345]], [[1.7823622226715088]], [[1.4507557153701782]], [[0.6824175715446472]], [[2.117170572280884]], [[0.0]], [[0.34033361077308655]], [[0.5585139393806458]], [[1.304084300994873]], [[1.063408613204956]], [[0.6330430507659912]], [[1.4885145425796509]], [[1.4915415048599243]], [[0.6983181238174438]], [[2.082225799560547]], [[0.0]], [[0.3403341472148895]], [[0.5585137605667114]], [[1.3040835857391357]], [[1.0634084939956665]], [[0.6330421566963196]], [[1.4885141849517822]], [[0.956049382686615]], [[2.0441696643829346]], [[0.0]], [[0.3403335213661194]], [[0.5585137605667114]], [[1.3040838241577148]], [[1.0634078979492188]], [[0.6330415606498718]], [[1.4885133504867554]], [[2.023287057876587]], [[0.0]], [[0.34033334255218506]], [[0.5585135221481323]], [[0.9242823719978333]], [[0.4935878813266754]], [[1.3687797784805298]], [[1.932531714439392]], [[0.0]], [[0.34033334255218506]], [[0.774273693561554]], [[0.4175921380519867]], [[1.372702717781067]], [[1.9034998416900635]], [[0.0]], [[0.3403330147266388]], [[0.05043554678559303]], [[1.0940395593643188]], [[1.7535170316696167]], [[0.0]], [[0.3403330147266388]], [[0.05043554678559303]], [[1.4249151945114136]], [[0.0]], [[0.3403330147266388]], [[1.0560117959976196]], [[0.0]], [[0.3403334617614746]], [[0.0]]], [[[-0.1560264527797699]], [[-0.16111110150814056]], [[-0.09184058010578156]], [[-0.08249916881322861]], [[-0.06847120821475983]], [[-0.09613268077373505]], [[-0.167856827378273]], [[-0.13533322513103485]], [[0.03696735203266144]], [[0.13746251165866852]], [[0.09602191299200058]], [[0.11003749072551727]], [[1.2584466934204102]], [[1.7462029457092285]], [[0.0]], [[-0.1560264527797699]], [[-0.16111110150814056]], [[-0.09184058010578156]], [[-0.08249916881322861]], [[-0.06847120821475983]], [[-0.09613268077373505]], [[-0.167856827378273]], [[-0.13533322513103485]], [[0.03696735203266144]], [[0.13746251165866852]], [[0.3692713677883148]], [[1.3545814752578735]], [[1.8454303741455078]], [[0.0]], [[-0.1560264527797699]], [[-0.16111110150814056]], [[-0.12678460776805878]], [[-0.1607287973165512]], [[-0.12828874588012695]], [[-0.16015289723873138]], [[-0.1605810970067978]], [[-0.01771889068186283]], [[0.2718050479888916]], [[0.3602020740509033]], [[1.3877942562103271]], [[1.8777203559875488]], [[0.0]], [[-0.1560264527797699]], [[-0.16111110150814056]], [[-0.1659160852432251]], [[-0.11061477661132812]], [[-0.1595795750617981]], [[-0.15059369802474976]], [[-0.003517772303894162]], [[0.3543653190135956]], [[0.31122180819511414]], [[1.3824959993362427]], [[1.8705580234527588]], [[0.0]], [[-0.1560264527797699]], [[-0.1692531406879425]], [[-0.10822762548923492]], [[-0.15738673508167267]], [[-0.14395035803318024]], [[0.08436641097068787]], [[0.3817084729671478]], [[0.41264528036117554]], [[1.3707736730575562]], [[1.8612253665924072]], [[0.0]], [[-0.1560264676809311]], [[-0.1692531555891037]], [[-0.10822782665491104]], [[-0.15738674998283386]], [[-0.1439504325389862]], [[0.15130648016929626]], [[0.2788502871990204]], [[1.2270697355270386]], [[1.786787748336792]], [[0.0]], [[-0.1560264378786087]], [[-0.1697336733341217]], [[-0.14136217534542084]], [[-0.16357479989528656]], [[0.05892190337181091]], [[0.31622275710105896]], [[1.091905951499939]], [[1.6409097909927368]], [[0.0]], [[-0.15602634847164154]], [[-0.1697336733341217]], [[-0.1672450304031372]], [[-0.01789534091949463]], [[0.28542718291282654]], [[0.8860785961151123]], [[1.4802030324935913]], [[0.0]], [[-0.15602634847164154]], [[-0.1697336584329605]], [[-0.1085822731256485]], [[0.3026030957698822]], [[0.7696044445037842]], [[1.3992741107940674]], [[0.0]], [[-0.15602634847164154]], [[-0.1697336584329605]], [[-0.1085822731256485]], [[0.25058940052986145]], [[1.0142574310302734]], [[0.0]], [[-0.15602640807628632]], [[-0.10989119112491608]], [[0.19176393747329712]], [[0.7032782435417175]], [[0.0]], [[-0.15602640807628632]], [[0.02668408863246441]], [[0.3314628303050995]], [[0.0]], [[-0.15602640807628632]], [[0.02668408863246441]], [[0.0]], [[-0.15602640807628632]], [[0.0]]], [[[-0.1176338940858841]], [[-0.08076705783605576]], [[0.12169620394706726]], [[0.06359846144914627]], [[0.0862516388297081]], [[-0.1300266832113266]], [[-0.06444191187620163]], [[0.3287470042705536]], [[1.7451528310775757]], [[0.0]], [[-0.11763384193181992]], [[-0.08076713979244232]], [[0.005779414437711239]], [[0.11088632792234421]], [[-0.03260257467627525]], [[-0.06231475621461868]], [[0.18532516062259674]], [[1.8485901355743408]], [[0.0]], [[-0.11763381958007812]], [[-0.16620127856731415]], [[-0.06741447746753693]], [[-0.11788951605558395]], [[-0.0907897874712944]], [[0.2419658601284027]], [[1.8692402839660645]], [[0.0]], [[-0.11763399094343185]], [[-0.16620129346847534]], [[-0.06741452217102051]], [[0.00619145343080163]], [[0.3656109869480133]], [[1.8839508295059204]], [[0.0]], [[-0.11763399094343185]], [[-0.1613597273826599]], [[-0.07208754122257233]], [[0.38698920607566833]], [[1.7741639614105225]], [[0.0]], [[-0.11763399094343185]], [[-0.1450403928756714]], [[0.2669563889503479]], [[1.399916648864746]], [[0.0]], [[-0.11763399094343185]], [[0.1879340261220932]], [[1.1145695447921753]], [[0.0]], [[0.892080545425415]], [[0.8307971358299255]], [[0.0]], [[0.8920809030532837]], [[0.0]]], [[[0.8515278697013855]], [[0.4749619662761688]], [[1.0295215845108032]], [[0.5258869528770447]], [[0.7040842175483704]], [[-0.06543657928705215]], [[0.11412082612514496]], [[0.7839546203613281]], [[1.499823808670044]], [[0.7603787779808044]], [[1.7095096111297607]], [[0.0]], [[0.8515278697013855]], [[0.4749619662761688]], [[1.0295215845108032]], [[0.5258869528770447]], [[0.7040842175483704]], [[-0.06543657928705215]], [[0.4115667939186096]], [[1.5212334394454956]], [[0.7633237838745117]], [[1.7486519813537598]], [[0.0]], [[0.8515276312828064]], [[0.47496169805526733]], [[1.0295219421386719]], [[1.2298212051391602]], [[0.2615298330783844]], [[0.8016138672828674]], [[1.332961082458496]], [[0.5982139110565186]], [[1.7722713947296143]], [[0.0]], [[0.8515284061431885]], [[0.4749619960784912]], [[1.0295225381851196]], [[1.229821801185608]], [[0.2615306079387665]], [[0.8016138672828674]], [[1.1008954048156738]], [[2.0028433799743652]], [[0.0]], [[0.8515282869338989]], [[0.47496262192726135]], [[1.0295225381851196]], [[0.3652154207229614]], [[0.8973618149757385]], [[1.0214403867721558]], [[2.0236849784851074]], [[0.0]], [[0.8515287041664124]], [[0.43288880586624146]], [[0.3500705659389496]], [[0.7322748899459839]], [[0.8059041500091553]], [[1.9745019674301147]], [[0.0]], [[0.8515287041664124]], [[0.43288880586624146]], [[0.8168386816978455]], [[0.7348687052726746]], [[1.918581247329712]], [[0.0]], [[0.8515281677246094]], [[0.9566397666931152]], [[0.6569241881370544]], [[1.9438713788986206]], [[0.0]], [[0.8515281677246094]], [[0.18069332838058472]], [[1.9097163677215576]], [[0.0]], [[0.8515281677246094]], [[1.8574707508087158]], [[0.0]], [[0.8515281677246094]], [[0.0]]], [[[-0.11763390898704529]], [[-0.08076704293489456]], [[0.12169582396745682]], [[0.06359822303056717]], [[0.08625131845474243]], [[-0.1300266832113266]], [[-0.06444206088781357]], [[0.32874688506126404]], [[1.7451539039611816]], [[0.010316566564142704]], [[-0.08823926001787186]], [[0.02970852144062519]], [[0.9366324543952942]], [[0.4551143944263458]], [[-0.16291776299476624]], [[0.20516113936901093]], [[0.2907007336616516]], [[0.42264607548713684]], [[0.4911215007305145]], [[1.6946460008621216]], [[0.0]], [[-0.11763390898704529]], [[-0.08076704293489456]], [[0.12169582396745682]], [[0.06359822303056717]], [[0.08625131845474243]], [[-0.1300266832113266]], [[-0.06444206088781357]], [[0.28020456433296204]], [[-0.15406955778598785]], [[-0.16730903089046478]], [[-0.06412862241268158]], [[0.8836976289749146]], [[0.523612916469574]], [[-0.14457733929157257]], [[0.213949054479599]], [[0.33288511633872986]], [[0.5016147494316101]], [[0.5812313556671143]], [[1.9613525867462158]], [[0.0]], [[-0.11763390898704529]], [[-0.08076704293489456]], [[0.12169582396745682]], [[0.06359822303056717]], [[0.08625131845474243]], [[-0.1300266832113266]], [[-0.06444206088781357]], [[0.28020456433296204]], [[-0.15406955778598785]], [[-0.16730903089046478]], [[-0.06412862241268158]], [[0.8836976289749146]], [[0.523612916469574]], [[-0.04733078554272652]], [[0.48442304134368896]], [[0.7321720123291016]], [[0.8199103474617004]], [[2.0163633823394775]], [[0.0]], [[-0.11763390898704529]], [[-0.13136714696884155]], [[0.021243466064333916]], [[0.009173473343253136]], [[-0.11745585501194]], [[-0.05044608190655708]], [[0.30283835530281067]], [[-0.16010920703411102]], [[-0.14474883675575256]], [[-0.06461052596569061]], [[0.9138681292533875]], [[0.6808213591575623]], [[-0.014026861637830734]], [[0.5311341881752014]], [[0.7914441227912903]], [[0.8719714283943176]], [[2.1779487133026123]], [[0.0]], [[-0.11763380467891693]], [[-0.1313668042421341]], [[0.021243561059236526]], [[0.009173719212412834]], [[-0.11745570600032806]], [[-0.050446175038814545]], [[0.3028385639190674]], [[-0.1601092666387558]], [[-0.14474867284297943]], [[-0.06461050361394882]], [[0.9138689041137695]], [[0.6808208227157593]], [[-0.014027424156665802]], [[0.5311334729194641]], [[0.9568579792976379]], [[2.1517012119293213]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.02124328911304474]], [[0.009173719212412834]], [[-0.11745578050613403]], [[-0.05044635012745857]], [[-0.11198671907186508]], [[-0.0664944276213646]], [[-0.0008931432384997606]], [[0.951995313167572]], [[0.6952794790267944]], [[-0.019420744851231575]], [[0.47408103942871094]], [[0.9819941520690918]], [[2.176605463027954]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.02124328911304474]], [[0.009173719212412834]], [[-0.11745578050613403]], [[-0.05044635012745857]], [[0.010262526571750641]], [[-0.01126016117632389]], [[1.031864047050476]], [[0.6787830591201782]], [[-0.05855945497751236]], [[0.5197493433952332]], [[1.0084600448608398]], [[2.1834657192230225]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.02124328911304474]], [[0.009173719212412834]], [[-0.11745578050613403]], [[-0.05044635012745857]], [[0.010262526571750641]], [[0.5962211489677429]], [[0.6890935301780701]], [[-0.07255030423402786]], [[0.525245189666748]], [[1.0262346267700195]], [[2.2820093631744385]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.05975976958870888]], [[-0.11994052678346634]], [[-0.015524245798587799]], [[0.03719828650355339]], [[0.6524919867515564]], [[0.8614593744277954]], [[-0.03269501030445099]], [[0.4934261739253998]], [[1.0366744995117188]], [[2.263254165649414]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.05975976958870888]], [[0.1947493553161621]], [[0.12804558873176575]], [[0.6715455651283264]], [[0.7562267184257507]], [[-0.06768181174993515]], [[0.4333098232746124]], [[0.9317858219146729]], [[2.236863374710083]], [[0.0]], [[-0.11763391643762589]], [[-0.1313670426607132]], [[0.05975976958870888]], [[0.1947493553161621]], [[0.12804558873176575]], [[0.6715455651283264]], [[0.7562267184257507]], [[-0.06768181174993515]], [[0.6982682347297668]], [[2.19124698638916]], [[0.0]], [[-0.1176338940858841]], [[-0.13136696815490723]], [[0.059759803116321564]], [[0.19474969804286957]], [[0.9160845875740051]], [[0.4927205741405487]], [[-0.114845409989357]], [[0.6668732762336731]], [[2.0769641399383545]], [[0.0]], [[-0.11763384193181992]], [[-0.1313668042421341]], [[0.16483564674854279]], [[0.54368656873703]], [[0.3006477653980255]], [[-0.12305411696434021]], [[0.5026618838310242]], [[1.9485740661621094]], [[0.0]], [[-0.11763381958007812]], [[-0.1313667595386505]], [[0.1648358553647995]], [[0.5436869859695435]], [[0.30064857006073]], [[0.2270568311214447]], [[1.742345929145813]], [[0.0]], [[-0.11763399094343185]], [[-0.13136672973632812]], [[0.16483506560325623]], [[0.5436869859695435]], [[0.5393978953361511]], [[1.6911441087722778]], [[0.0]], [[-0.11763399094343185]], [[-0.14504024386405945]], [[0.2851705551147461]], [[0.2830130457878113]], [[1.5258251428604126]], [[0.0]], [[0.00802438985556364]], [[0.152899831533432]], [[-0.03142038732767105]], [[1.4513674974441528]], [[0.0]], [[0.00802438985556364]], [[-0.14552612602710724]], [[0.7192447185516357]], [[0.0]], [[0.00802438985556364]], [[0.06152473762631416]], [[0.0]], [[0.008024756796658039]], [[0.0]]], [[[0.45668187737464905]], [[0.2664715647697449]], [[-0.017658734694123268]], [[0.2215522974729538]], [[0.36081454157829285]], [[0.33310502767562866]], [[-0.06034082546830177]], [[-0.1317916065454483]], [[0.6790573000907898]], [[0.8200266361236572]], [[0.5117771625518799]], [[0.30500391125679016]], [[0.7682309746742249]], [[-0.16013240814208984]], [[-0.11051265895366669]], [[-0.169698104262352]], [[0.3448892831802368]], [[-0.06071978062391281]], [[1.334957480430603]], [[1.657362937927246]], [[0.0]], [[0.45668187737464905]], [[0.2664715647697449]], [[-0.017658734694123268]], [[0.2215522974729538]], [[0.36081454157829285]], [[0.33310502767562866]], [[-0.06034082546830177]], [[-0.1317916065454483]], [[0.6790573000907898]], [[0.8200266361236572]], [[0.5117771625518799]], [[0.30500391125679016]], [[0.7682309746742249]], [[-0.16013240814208984]], [[-0.15733928978443146]], [[0.5585840940475464]], [[-0.01716737076640129]], [[1.4307876825332642]], [[1.7206039428710938]], [[0.0]], [[0.45668187737464905]], [[0.2664715647697449]], [[-0.017658734694123268]], [[0.2215522974729538]], [[0.36081454157829285]], [[0.33310502767562866]], [[-0.06034082546830177]], [[-0.1317916065454483]], [[0.6790573000907898]], [[0.8200266361236572]], [[0.5117771625518799]], [[0.30500391125679016]], [[0.7682309746742249]], [[-0.16013240814208984]], [[-0.15733928978443146]], [[0.5585840940475464]], [[1.3717331886291504]], [[1.8186657428741455]], [[0.0]], [[0.45668187737464905]], [[0.2664715647697449]], [[-0.017658734694123268]], [[0.2215522974729538]], [[0.36081454157829285]], [[0.33310502767562866]], [[-0.06034082546830177]], [[-0.1317916065454483]], [[0.6790573000907898]], [[0.5437700152397156]], [[0.4698266088962555]], [[0.9678182601928711]], [[-0.16976700723171234]], [[-0.11255204677581787]], [[0.7190148830413818]], [[1.4690455198287964]], [[1.8713382482528687]], [[0.0]], [[0.4566814601421356]], [[0.26647037267684937]], [[-0.017658749595284462]], [[0.2215522974729538]], [[0.36081424355506897]], [[0.3331056237220764]], [[-0.06034041568636894]], [[-0.13179171085357666]], [[0.6790575385093689]], [[0.5437697172164917]], [[0.4698265492916107]], [[-0.010739414021372795]], [[0.05570893734693527]], [[0.9220736026763916]], [[1.5868597030639648]], [[1.8901371955871582]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[-0.01765884831547737]], [[0.22155198454856873]], [[0.36081448197364807]], [[0.3331056237220764]], [[-0.06034056097269058]], [[-0.13179166615009308]], [[0.6790571808815002]], [[0.5437702536582947]], [[0.4620625674724579]], [[0.16046558320522308]], [[1.0024712085723877]], [[1.6270793676376343]], [[1.9984391927719116]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[-0.01765884831547737]], [[0.22155198454856873]], [[0.36081448197364807]], [[0.3331056237220764]], [[-0.06034056097269058]], [[-0.13179166615009308]], [[0.6790571808815002]], [[0.5437702536582947]], [[0.4620625674724579]], [[0.16046558320522308]], [[1.6403135061264038]], [[2.0321390628814697]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[-0.01765884831547737]], [[0.22155198454856873]], [[0.36081448197364807]], [[0.3331056237220764]], [[0.08043993264436722]], [[0.985820472240448]], [[0.6907265782356262]], [[0.6159298419952393]], [[0.25717997550964355]], [[1.7196581363677979]], [[2.0802512168884277]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[0.569242000579834]], [[0.6964364051818848]], [[0.5913488268852234]], [[0.26074787974357605]], [[1.1672812700271606]], [[0.7836406230926514]], [[0.7606561183929443]], [[0.3714956045150757]], [[1.7536448240280151]], [[2.1013808250427246]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[0.569242000579834]], [[0.6964364051818848]], [[0.5913488268852234]], [[1.0591081380844116]], [[0.7380295991897583]], [[0.6976522207260132]], [[0.4053035378456116]], [[1.768937349319458]], [[2.0911293029785156]], [[0.0]], [[0.4566815197467804]], [[0.26647070050239563]], [[0.569242000579834]], [[0.6964364051818848]], [[0.5913488268852234]], [[1.0591081380844116]], [[0.7380295991897583]], [[0.6976522207260132]], [[1.8253602981567383]], [[2.1089038848876953]], [[0.0]], [[0.45668166875839233]], [[1.0199435949325562]], [[0.9848009943962097]], [[0.9843166470527649]], [[1.3111213445663452]], [[0.8867592215538025]], [[0.9036139249801636]], [[1.8284157514572144]], [[2.1467154026031494]], [[0.0]], [[0.4566813111305237]], [[1.0199432373046875]], [[0.9848007559776306]], [[0.9843161702156067]], [[1.311122179031372]], [[0.555949330329895]], [[1.8291593790054321]], [[2.150890827178955]], [[0.0]], [[0.4566813111305237]], [[0.742989182472229]], [[0.8545324206352234]], [[1.1543015241622925]], [[0.4649085998535156]], [[1.8458729982376099]], [[2.0737531185150146]], [[0.0]], [[0.45668157935142517]], [[0.7429885268211365]], [[0.8545328974723816]], [[1.1543028354644775]], [[1.7821459770202637]], [[1.9918324947357178]], [[0.0]], [[0.45668157935142517]], [[0.7429885268211365]], [[1.4299129247665405]], [[1.9220322370529175]], [[1.988667368888855]], [[0.0]], [[0.45668166875839233]], [[0.742988646030426]], [[1.4299134016036987]], [[1.9220328330993652]], [[0.0]], [[0.45668166875839233]], [[1.2929211854934692]], [[1.6592854261398315]], [[0.0]], [[0.45668166875839233]], [[1.4497617483139038]], [[0.0]], [[0.45668160915374756]], [[0.0]]], [[[0.1713588982820511]], [[0.221313938498497]], [[0.19895480573177338]], [[0.9599178433418274]], [[0.37545716762542725]], [[0.4755809009075165]], [[-0.09280036389827728]], [[0.3200682997703552]], [[1.634668231010437]], [[0.0]], [[0.1713586002588272]], [[0.2213139832019806]], [[0.19895483553409576]], [[0.9599180817604065]], [[0.3003554046154022]], [[-0.05080677196383476]], [[0.40654700994491577]], [[1.7258567810058594]], [[0.0]], [[0.1713588386774063]], [[0.22131389379501343]], [[0.19895419478416443]], [[0.9599185585975647]], [[0.3003554344177246]], [[0.7119072675704956]], [[1.7501622438430786]], [[0.0]], [[0.1713593602180481]], [[0.2213144749403]], [[0.19895483553409576]], [[0.959918737411499]], [[0.3003555238246918]], [[1.7412240505218506]], [[0.0]], [[0.1713593602180481]], [[0.45124301314353943]], [[0.9014917016029358]], [[0.31248021125793457]], [[1.680700659751892]], [[0.0]], [[0.17135967314243317]], [[0.8063873052597046]], [[0.40363094210624695]], [[1.5856012105941772]], [[0.0]], [[0.17135967314243317]], [[0.12217758595943451]], [[1.3741319179534912]], [[0.0]], [[0.17135967314243317]], [[1.2711951732635498]], [[0.0]], [[0.9912424683570862]], [[0.0]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's add our own text to text a few hypotheses. Add/ remove words. Replace words with similar words or opposites. \n",
        "\n",
        "Note: Some neurons perform multiple different functions, so your hypothesis might be {\"Harry Potter character names\" OR \"Repeated words\" OR \"these three punctuation marks after closing quotation marks\"}. This can be teased apart later when we see cross-neuron comparison (maybe this neuron and another do the Harry Potter characters?). Another source of info is the logit attribution part (see below)"
      ],
      "metadata": {
        "id": "r3zES3YY6CYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\n",
        "    \"1 2 3 4 5 6\",\n",
        "    \" 1 2 3 4 5 6\",\n",
        "    \"bacon & eggs\",\n",
        "    \" bacon & eggs\",\n",
        "]\n",
        "simplifier.text_to_visualize(text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "SRO7Qusg2Qnk",
        "outputId": "6f9adbc4-afac-4661-9934-b4cf7088bb08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f748104a2b0>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-92602313-0875\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-92602313-0875\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"1\", \" 2\", \" 3\", \" 4\", \" 5\", \" 6\", \"\\n\", \" 1\", \" 2\", \" 3\", \" 4\", \" 5\", \" 6\", \"\\n\", \"b\", \"acon\", \" &\", \" eggs\", \"\\n\", \" bacon\", \" &\", \" eggs\", \"\\n\"], \"activations\": [[[-0.16162660717964172]], [[-0.152877077460289]], [[-0.16566932201385498]], [[-0.13533787429332733]], [[-0.13226811587810516]], [[-0.1292981505393982]], [[0.0]], [[0.0537872351706028]], [[-0.012345206923782825]], [[0.052131231874227524]], [[0.05940918251872063]], [[0.020923206582665443]], [[0.001847998471930623]], [[0.0]], [[-0.16825072467327118]], [[-0.13127020001411438]], [[-0.02459920197725296]], [[0.14122015237808228]], [[0.0]], [[0.20644602179527283]], [[-0.09822818636894226]], [[-0.0010563372634351254]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logit Attribution"
      ],
      "metadata": {
        "id": "Mq9J31Kf0kaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the row or column of MLP_out that is size d_model. Look through model.cfg & the shape of the model to find it. Multiply that row by the unembedding matrix"
      ],
      "metadata": {
        "id": "1qyWECz_0sTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model config is useful to look at for model shape info\n",
        "model.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rUDm1Zm1ZgU",
        "outputId": "0cdc9779-697c-48a9-8746-2a0beee29698"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HookedTransformerConfig:\n",
              "{'act_fn': 'gelu',\n",
              " 'attention_dir': 'causal',\n",
              " 'attn_only': False,\n",
              " 'attn_types': None,\n",
              " 'checkpoint_index': None,\n",
              " 'checkpoint_label_type': None,\n",
              " 'checkpoint_value': None,\n",
              " 'd_head': 64,\n",
              " 'd_mlp': 2048,\n",
              " 'd_model': 512,\n",
              " 'd_vocab': 48262,\n",
              " 'd_vocab_out': 48262,\n",
              " 'device': 'cuda',\n",
              " 'eps': 1e-05,\n",
              " 'final_rms': False,\n",
              " 'from_checkpoint': False,\n",
              " 'gated_mlp': False,\n",
              " 'init_mode': 'gpt2',\n",
              " 'init_weights': False,\n",
              " 'initializer_range': 0.035355339059327376,\n",
              " 'model_name': 'GELU_2L512W_C4_Code',\n",
              " 'n_ctx': 1024,\n",
              " 'n_devices': 1,\n",
              " 'n_heads': 8,\n",
              " 'n_layers': 2,\n",
              " 'n_params': 6291456,\n",
              " 'normalization_type': 'LNPre',\n",
              " 'original_architecture': 'neel',\n",
              " 'parallel_attn_mlp': False,\n",
              " 'positional_embedding_type': 'standard',\n",
              " 'rotary_dim': None,\n",
              " 'scale_attn_by_inverse_layer_idx': False,\n",
              " 'seed': None,\n",
              " 'tokenizer_name': 'NeelNanda/gpt-neox-tokenizer-digits',\n",
              " 'use_attn_result': False,\n",
              " 'use_attn_scale': True,\n",
              " 'use_hook_tokens': False,\n",
              " 'use_local_attn': False,\n",
              " 'use_split_qkv_input': False,\n",
              " 'window_size': None}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model has several easy ways to access the weights of the model, such as model.W_in, model.QK. We care about model.W.out, the second part of the MLP\n",
        "# The shape is [layer, d_mlp, d_model]\n",
        "model.W_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrgMnD7S7OqG",
        "outputId": "1573ffc4-caea-431f-c1d7-5bff2a25b359"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2048, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unembeed\n",
        "model.W_U.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCaN2pfZ8RBB",
        "outputId": "b6673b17-e246-4334-9e23-76b4c6fc9de8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 48262])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.einsum('ij,jk->ik', model.W_out[-1], model.W_U)\n",
        "x.shape\n",
        "# x is the direct contribution of each neuron for all logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqenYDl2IOpl",
        "outputId": "22716f72-06fa-4211-f602-04af507324d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048, 48262])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm unsure if we need to multiply W_out by the layer norm or anything. This is just something I need to ask\n",
        "# To a first approximation, we multiply by the row in W_out with all of W_U"
      ],
      "metadata": {
        "id": "yNoDvmWY9B4g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### look at top neurons for a token"
      ],
      "metadata": {
        "id": "aVZdIPFYyn9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in range(10):\n",
        "    print(f\"token {model.to_string(token)}\\n\")\n",
        "    top_k_values, top_k_neurons = th.topk(x.T[token], k=10) # x.T[token] is the direct contribution of each neuron for the token\n",
        "    for neuron, value in zip(top_k_neurons, top_k_values):\n",
        "        print(f'\\tneuron {neuron}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG3RfRnSzx5j",
        "outputId": "01f247eb-515e-4ac0-cd39-1dc12b4eb146"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token <|EOS|>\n",
            "\n",
            "\tneuron 1429: 2.349\n",
            "\tneuron 1890: 1.735\n",
            "\tneuron 1430: 1.558\n",
            "\tneuron 1888: 1.494\n",
            "\tneuron 160: 1.419\n",
            "\tneuron 25: 1.261\n",
            "\tneuron 916: 1.232\n",
            "\tneuron 1379: 1.001\n",
            "\tneuron 208: 0.993\n",
            "\tneuron 436: 0.965\n",
            "\n",
            "\n",
            "token <|BOS|>\n",
            "\n",
            "\tneuron 154: 2.002\n",
            "\tneuron 1938: 0.514\n",
            "\tneuron 1274: 0.489\n",
            "\tneuron 889: 0.441\n",
            "\tneuron 683: 0.426\n",
            "\tneuron 254: 0.418\n",
            "\tneuron 835: 0.410\n",
            "\tneuron 1866: 0.403\n",
            "\tneuron 1780: 0.370\n",
            "\tneuron 1810: 0.354\n",
            "\n",
            "\n",
            "token <|PAD|>\n",
            "\n",
            "\tneuron 154: 1.991\n",
            "\tneuron 1938: 0.508\n",
            "\tneuron 1274: 0.490\n",
            "\tneuron 889: 0.444\n",
            "\tneuron 683: 0.427\n",
            "\tneuron 835: 0.418\n",
            "\tneuron 254: 0.416\n",
            "\tneuron 1866: 0.388\n",
            "\tneuron 1780: 0.369\n",
            "\tneuron 1810: 0.358\n",
            "\n",
            "\n",
            "token !\n",
            "\n",
            "\tneuron 160: 2.333\n",
            "\tneuron 1379: 2.311\n",
            "\tneuron 1146: 1.486\n",
            "\tneuron 755: 1.123\n",
            "\tneuron 1434: 1.118\n",
            "\tneuron 1183: 1.075\n",
            "\tneuron 156: 1.062\n",
            "\tneuron 1811: 1.056\n",
            "\tneuron 1353: 1.034\n",
            "\tneuron 222: 1.014\n",
            "\n",
            "\n",
            "token \"\n",
            "\n",
            "\tneuron 1071: 2.331\n",
            "\tneuron 1379: 2.055\n",
            "\tneuron 1255: 1.398\n",
            "\tneuron 160: 1.295\n",
            "\tneuron 381: 1.167\n",
            "\tneuron 472: 1.145\n",
            "\tneuron 524: 1.090\n",
            "\tneuron 61: 1.080\n",
            "\tneuron 447: 1.035\n",
            "\tneuron 1117: 0.928\n",
            "\n",
            "\n",
            "token #\n",
            "\n",
            "\tneuron 360: 1.292\n",
            "\tneuron 472: 1.275\n",
            "\tneuron 1890: 1.144\n",
            "\tneuron 1344: 0.942\n",
            "\tneuron 249: 0.894\n",
            "\tneuron 1473: 0.823\n",
            "\tneuron 1945: 0.812\n",
            "\tneuron 1383: 0.787\n",
            "\tneuron 1493: 0.783\n",
            "\tneuron 1234: 0.761\n",
            "\n",
            "\n",
            "token $\n",
            "\n",
            "\tneuron 472: 1.399\n",
            "\tneuron 979: 1.052\n",
            "\tneuron 15: 0.988\n",
            "\tneuron 1486: 0.985\n",
            "\tneuron 89: 0.819\n",
            "\tneuron 215: 0.817\n",
            "\tneuron 2026: 0.806\n",
            "\tneuron 1384: 0.802\n",
            "\tneuron 742: 0.800\n",
            "\tneuron 1576: 0.782\n",
            "\n",
            "\n",
            "token %\n",
            "\n",
            "\tneuron 510: 1.557\n",
            "\tneuron 472: 1.345\n",
            "\tneuron 716: 1.196\n",
            "\tneuron 386: 1.154\n",
            "\tneuron 1032: 1.111\n",
            "\tneuron 805: 1.084\n",
            "\tneuron 310: 1.081\n",
            "\tneuron 1894: 1.075\n",
            "\tneuron 541: 0.858\n",
            "\tneuron 443: 0.766\n",
            "\n",
            "\n",
            "token &\n",
            "\n",
            "\tneuron 2006: 1.844\n",
            "\tneuron 1599: 1.570\n",
            "\tneuron 840: 1.115\n",
            "\tneuron 1811: 1.078\n",
            "\tneuron 1486: 0.965\n",
            "\tneuron 1183: 0.918\n",
            "\tneuron 1685: 0.903\n",
            "\tneuron 1170: 0.835\n",
            "\tneuron 110: 0.822\n",
            "\tneuron 1171: 0.818\n",
            "\n",
            "\n",
            "token '\n",
            "\n",
            "\tneuron 1071: 2.412\n",
            "\tneuron 2006: 2.378\n",
            "\tneuron 1530: 1.706\n",
            "\tneuron 1811: 1.383\n",
            "\tneuron 1379: 1.302\n",
            "\tneuron 1255: 1.109\n",
            "\tneuron 447: 0.941\n",
            "\tneuron 785: 0.877\n",
            "\tneuron 360: 0.856\n",
            "\tneuron 1493: 0.841\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### look at top tokens for a neuron"
      ],
      "metadata": {
        "id": "wzrqJh8zy5Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for neuron in range(10):\n",
        "    print(f\"neuron {neuron}\\n\")\n",
        "    top_k_values, top_k_tokens = th.topk(x[neuron], k=10) # x[neuron] is the direct contribution of the neuron to each token\n",
        "    for token, value in zip(top_k_tokens, top_k_values):\n",
        "        print(f'\\ttoken {model.to_string(token)}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD2VnXnJpFen",
        "outputId": "d627bf66-1903-4f31-c916-86ec8e9efeb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neuron 0\n",
            "\n",
            "\ttoken ishing: 1.178\n",
            "\ttoken bes: 1.100\n",
            "\ttoken posed: 1.023\n",
            "\ttoken ères: 0.958\n",
            "\ttoken coma: 0.951\n",
            "\ttoken rays: 0.936\n",
            "\ttoken ished: 0.935\n",
            "\ttoken ons: 0.925\n",
            "\ttoken aned: 0.918\n",
            "\ttoken icity: 0.915\n",
            "\n",
            "\n",
            "neuron 1\n",
            "\n",
            "\ttoken edu: 0.977\n",
            "\ttoken ther: 0.965\n",
            "\ttoken �: 0.937\n",
            "\ttoken iest: 0.883\n",
            "\ttoken  there: 0.862\n",
            "\ttoken �: 0.851\n",
            "\ttoken ход: 0.826\n",
            "\ttoken usta: 0.819\n",
            "\ttoken ч: 0.801\n",
            "\ttoken henyl: 0.797\n",
            "\n",
            "\n",
            "neuron 2\n",
            "\n",
            "\ttoken uses: 0.996\n",
            "\ttoken iana: 0.988\n",
            "\ttoken ians: 0.941\n",
            "\ttoken us: 0.933\n",
            "\ttoken 니다: 0.911\n",
            "\ttoken aga: 0.900\n",
            "\ttoken inafter: 0.896\n",
            "\ttoken izon: 0.895\n",
            "\ttoken mania: 0.895\n",
            "\ttoken ician: 0.886\n",
            "\n",
            "\n",
            "neuron 3\n",
            "\n",
            "\ttoken asus: 1.034\n",
            "\ttoken rolog: 1.033\n",
            "\ttoken ró: 1.025\n",
            "\ttoken Tell: 0.991\n",
            "\ttoken ermine: 0.984\n",
            "\ttoken ime: 0.984\n",
            "\ttoken rile: 0.976\n",
            "\ttoken ras: 0.951\n",
            "\ttoken iana: 0.931\n",
            "\ttoken rano: 0.923\n",
            "\n",
            "\n",
            "neuron 4\n",
            "\n",
            "\ttoken 9: 0.918\n",
            "\ttoken 限: 0.823\n",
            "\ttoken ordin: 0.808\n",
            "\ttoken rah: 0.796\n",
            "\ttoken KES: 0.778\n",
            "\ttoken  mine: 0.777\n",
            "\ttoken bies: 0.760\n",
            "\ttoken  probability: 0.756\n",
            "\ttoken  it: 0.740\n",
            "\ttoken OSE: 0.728\n",
            "\n",
            "\n",
            "neuron 5\n",
            "\n",
            "\ttoken  me: 1.591\n",
            "\ttoken  us: 1.392\n",
            "\ttoken self: 1.129\n",
            "\ttoken  itself: 1.021\n",
            "\ttoken  herself: 1.017\n",
            "\ttoken quier: 1.001\n",
            "\ttoken  myself: 0.985\n",
            "\ttoken ycin: 0.968\n",
            "\ttoken ling: 0.964\n",
            "\ttoken liv: 0.957\n",
            "\n",
            "\n",
            "neuron 6\n",
            "\n",
            "\ttoken  plight: 0.930\n",
            "\ttoken  entire: 0.862\n",
            "\ttoken  whole: 0.839\n",
            "\ttoken  gap: 0.833\n",
            "\ttoken  entirety: 0.819\n",
            "\ttoken  slightest: 0.815\n",
            "\ttoken  deficit: 0.812\n",
            "\ttoken  Soon: 0.798\n",
            "\ttoken  remainder: 0.793\n",
            "\ttoken  chance: 0.786\n",
            "\n",
            "\n",
            "neuron 7\n",
            "\n",
            "\ttoken  ending: 1.010\n",
            "\ttoken  meaning: 0.896\n",
            "\ttoken  multiplying: 0.888\n",
            "\ttoken  subtracting: 0.870\n",
            "\ttoken  end: 0.870\n",
            "\ttoken  '</: 0.865\n",
            "\ttoken  stopping: 0.863\n",
            "\ttoken ppo: 0.857\n",
            "\ttoken  LICENSE: 0.852\n",
            "\ttoken  intervening: 0.823\n",
            "\n",
            "\n",
            "neuron 8\n",
            "\n",
            "\ttoken  on: 1.032\n",
            "\ttoken  thereon: 1.013\n",
            "\ttoken  upon: 0.873\n",
            "\ttoken  at: 0.824\n",
            "\ttoken tering: 0.817\n",
            "\ttoken eenth: 0.791\n",
            "\ttoken  atop: 0.789\n",
            "\ttoken HEL: 0.787\n",
            "\ttoken OWN: 0.759\n",
            "\ttoken aping: 0.746\n",
            "\n",
            "\n",
            "neuron 9\n",
            "\n",
            "\ttoken bo: 1.102\n",
            "\ttoken matically: 0.940\n",
            "\ttoken gered: 0.868\n",
            "\ttoken mit: 0.864\n",
            "\ttoken eded: 0.860\n",
            "\ttoken orously: 0.819\n",
            "\ttoken ma: 0.814\n",
            "\ttoken pered: 0.814\n",
            "\ttoken  kindly: 0.810\n",
            "\ttoken vier: 0.808\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### take a look at the top neurons for a token and then look at their top tokens"
      ],
      "metadata": {
        "id": "cLWHlB89kjhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = 25886\n",
        "print(f\"token {model.to_string(token)}\\n\")\n",
        "top_k_values, top_k_neurons = th.topk(x.T[token], k=10) # x.T[token] is the direct contribution of each neuron for the token\n",
        "for neuron, value in zip(top_k_neurons, top_k_values):\n",
        "    print(f'\\tneuron {neuron}: {value:.3f}\\n')\n",
        "    top_k_values, top_k_tokens = th.topk(x[neuron], k=10) # x[neuron] is the direct contribution of the neuron to each token\n",
        "    for token, value in zip(top_k_tokens, top_k_values):\n",
        "        print(f'\\t\\ttoken {model.to_string(token)}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj2z4yZbg6_b",
        "outputId": "d6210460-6b3e-412d-dad8-497536ec65db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token Monday\n",
            "\n",
            "\tneuron 472: 1.335\n",
            "\n",
            "\t\ttoken disambiguation: 1.834\n",
            "\t\ttoken resistance: 1.765\n",
            "\t\ttoken ©: 1.711\n",
            "\t\ttoken LICENSE: 1.707\n",
            "\t\ttoken Unable: 1.681\n",
            "\t\ttoken Female: 1.671\n",
            "\t\ttoken BMI: 1.670\n",
            "\t\ttoken Washington: 1.666\n",
            "\t\ttoken Looks: 1.665\n",
            "\t\ttoken Prince: 1.658\n",
            "\n",
            "\n",
            "\tneuron 154: 1.242\n",
            "\n",
            "\t\ttoken MOESM: 3.614\n",
            "\t\ttoken  že: 3.296\n",
            "\t\ttoken gebras: 3.265\n",
            "\t\ttoken  että: 3.179\n",
            "\t\ttoken ycin: 3.101\n",
            "\t\ttoken ~).: 3.079\n",
            "\t\ttoken  surjective: 3.058\n",
            "\t\ttoken  Eqs: 3.049\n",
            "\t\ttoken chaft: 3.020\n",
            "\t\ttoken  ktor: 3.017\n",
            "\n",
            "\n",
            "\tneuron 1381: 1.018\n",
            "\n",
            "\t\ttoken  than: 1.583\n",
            "\t\ttoken  past: 1.509\n",
            "\t\ttoken  September: 1.361\n",
            "\t\ttoken  January: 1.358\n",
            "\t\ttoken  August: 1.354\n",
            "\t\ttoken  December: 1.353\n",
            "\t\ttoken  March: 1.353\n",
            "\t\ttoken  April: 1.329\n",
            "\t\ttoken  June: 1.322\n",
            "\t\ttoken  February: 1.321\n",
            "\n",
            "\n",
            "\tneuron 215: 0.953\n",
            "\n",
            "\t\ttoken Blo: 1.520\n",
            "\t\ttoken University: 1.507\n",
            "\t\ttoken Whe: 1.503\n",
            "\t\ttoken Well: 1.479\n",
            "\t\ttoken Related: 1.477\n",
            "\t\ttoken Risk: 1.465\n",
            "\t\ttoken Ell: 1.449\n",
            "\t\ttoken Too: 1.442\n",
            "\t\ttoken Yes: 1.441\n",
            "\t\ttoken Va: 1.440\n",
            "\n",
            "\n",
            "\tneuron 429: 0.927\n",
            "\n",
            "\t\ttoken  celebrations: 1.177\n",
            "\t\ttoken regnancy: 1.167\n",
            "\t\ttoken week: 1.127\n",
            "\t\ttoken  holidays: 1.120\n",
            "\t\ttoken  dinner: 1.114\n",
            "\t\ttoken  periods: 1.097\n",
            "\t\ttoken  talks: 1.096\n",
            "\t\ttoken  weekend: 1.088\n",
            "\t\ttoken  Month: 1.088\n",
            "\t\ttoken  Meeting: 1.078\n",
            "\n",
            "\n",
            "\tneuron 473: 0.913\n",
            "\n",
            "\t\ttoken while: 1.277\n",
            "\t\ttoken than: 1.249\n",
            "\t\ttoken through: 1.207\n",
            "\t\ttoken during: 1.180\n",
            "\t\ttoken unto: 1.156\n",
            "\t\ttoken until: 1.127\n",
            "\t\ttoken from: 1.126\n",
            "\t\ttoken within: 1.119\n",
            "\t\ttoken and: 1.107\n",
            "\t\ttoken ](: 1.092\n",
            "\n",
            "\n",
            "\tneuron 1104: 0.883\n",
            "\n",
            "\t\ttoken ().: 1.389\n",
            "\t\ttoken .: 1.342\n",
            "\t\ttoken ._: 1.245\n",
            "\t\ttoken _.: 1.196\n",
            "\t\ttoken Report: 1.156\n",
            "\t\ttoken Heat: 1.138\n",
            "\t\ttoken Dictionary: 1.130\n",
            "\t\ttoken TestCase: 1.112\n",
            "\t\ttoken Factory: 1.112\n",
            "\t\ttoken Ele: 1.096\n",
            "\n",
            "\n",
            "\tneuron 298: 0.836\n",
            "\n",
            "\t\ttoken  Sept: 1.539\n",
            "\t\ttoken  Feb: 1.506\n",
            "\t\ttoken  April: 1.480\n",
            "\t\ttoken  July: 1.464\n",
            "\t\ttoken  June: 1.427\n",
            "\t\ttoken  May: 1.424\n",
            "\t\ttoken  December: 1.406\n",
            "\t\ttoken  Oct: 1.395\n",
            "\t\ttoken  October: 1.392\n",
            "\t\ttoken  November: 1.389\n",
            "\n",
            "\n",
            "\tneuron 1815: 0.787\n",
            "\n",
            "\t\ttoken zing: 0.905\n",
            "\t\ttoken ferr: 0.871\n",
            "\t\ttoken zo: 0.860\n",
            "\t\ttoken ersion: 0.857\n",
            "\t\ttoken forming: 0.852\n",
            "\t\ttoken ouss: 0.828\n",
            "\t\ttoken chn: 0.815\n",
            "\t\ttoken  import: 0.799\n",
            "\t\ttoken Monday: 0.787\n",
            "\t\ttoken gg: 0.766\n",
            "\n",
            "\n",
            "\tneuron 1746: 0.781\n",
            "\n",
            "\t\ttoken cause: 1.308\n",
            "\t\ttoken tera: 1.276\n",
            "\t\ttoken mpt: 1.247\n",
            "\t\ttoken anmar: 1.242\n",
            "\t\ttoken ppings: 1.205\n",
            "\t\ttoken Slot: 1.201\n",
            "\t\ttoken Star: 1.195\n",
            "\t\ttoken Appeal: 1.192\n",
            "\t\ttoken License: 1.188\n",
            "\t\ttoken lation: 1.156\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at a specific neuron, it’s top tokens and top activating examples, and see if it makes sense\n",
        "\n",
        "Like maybe the token is \" 5\" and the example is \" 1 2 3 4\". We can guess it's encoding the information about the sequence. So you can remove parts of the context to \"bob apple 4\" and see if the neuron is still activating.\n",
        "\n",
        "check \"visualize top examples\" part to get too activating examples for a specific neuron"
      ],
      "metadata": {
        "id": "ETM8OeTGwc_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = 298"
      ],
      "metadata": {
        "id": "uzxvcN_CvZFr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "looks at top tokens for this neuron"
      ],
      "metadata": {
        "id": "wbP-cTJmvbIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_values, top_k_tokens = th.topk(x[neuron], k=30) # x[neuron] is the direct contribution of the neuron to each token\n",
        "for token, value in zip(top_k_tokens, top_k_values):\n",
        "    print(f'\\ttoken {model.to_string(token)}: {value:.3f}')"
      ],
      "metadata": {
        "id": "cNEMHdgdvAiE",
        "outputId": "b3a72e9e-e36a-4d77-9120-8d7f5e9795aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ttoken  Sept: 1.539\n",
            "\ttoken  Feb: 1.506\n",
            "\ttoken  April: 1.480\n",
            "\ttoken  July: 1.464\n",
            "\ttoken  June: 1.427\n",
            "\ttoken  May: 1.424\n",
            "\ttoken  December: 1.406\n",
            "\ttoken  Oct: 1.395\n",
            "\ttoken  October: 1.392\n",
            "\ttoken  November: 1.389\n",
            "\ttoken  Wednesday: 1.355\n",
            "\ttoken  February: 1.344\n",
            "\ttoken  Thursday: 1.344\n",
            "\ttoken  March: 1.342\n",
            "\ttoken  Jan: 1.341\n",
            "\ttoken  September: 1.336\n",
            "\ttoken  January: 1.328\n",
            "\ttoken  Monday: 1.309\n",
            "\ttoken  Friday: 1.292\n",
            "\ttoken  August: 1.270\n",
            "\ttoken  Tuesday: 1.263\n",
            "\ttoken  Nov: 1.260\n",
            "\ttoken  Aug: 1.245\n",
            "\ttoken  Saturday: 1.216\n",
            "\ttoken  on: 1.135\n",
            "\ttoken  Dec: 1.125\n",
            "\ttoken April: 1.123\n",
            "\ttoken  Sunday: 1.117\n",
            "\ttoken Tuesday: 1.102\n",
            "\ttoken  late: 1.072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at top activating examples"
      ],
      "metadata": {
        "id": "ZfcpAEZgvjQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30\n",
        "simplifier = NeuronTextSimplifier(model, layer, neuron)\n",
        "\n",
        "values, indices = neuron_activations[:,neuron].topk(k)\n",
        "\n",
        "max_datapoints = [np.unravel_index(i, (datapoints, Token_amount)) for i in indices]\n",
        "\n",
        "text_list = []\n",
        "full_text = []\n",
        "for md, s_ind in max_datapoints:\n",
        "    md = int(md)\n",
        "    s_ind = int(s_ind)\n",
        "    # Get the full text\n",
        "    full_tok = th.tensor(d[md][\"input_ids\"])\n",
        "    full_text.append(model.tokenizer.decode(full_tok))\n",
        "    \n",
        "    # Get just the text up until the max-activating example\n",
        "    tok = d[md][\"input_ids\"][:s_ind+1]\n",
        "    text = model.tokenizer.decode(tok)\n",
        "    text_list.append(text)"
      ],
      "metadata": {
        "id": "TC-xNgUNrFeh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.text_to_visualize(full_text)"
      ],
      "metadata": {
        "id": "DjFRCTm-rK7m",
        "outputId": "2f52b792-a4b6-407d-a43b-3ec232480958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f7394a80520>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-2c47b30f-a0c0\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-2c47b30f-a0c0\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"L\", \"avi\", \" was\", \" born\", \" in\", \" K\", \"rak\", \"ow\", \",\", \" Poland\", \",\", \" and\", \" was\", \" 2\", \" years\", \" old\", \" when\", \" World\", \" War\", \" II\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"Z\", \"TE\", \" Z\", \"MAX\", \" 2\", \" Smart\", \"phone\", \"\\\\newline\", \"\\\\newline\", \"Z\", \"TE\", \" USA\", \" announced\", \" the\", \" release\", \" of\", \" Z\", \"MAX\", \" 2\", \",\", \"\\n\", \"A\", \" woman\", \" who\", \" helped\", \" arrange\", \" adopt\", \"ions\", \" for\", \" an\", \" Ohio\", \"-\", \"based\", \" agency\", \" pleaded\", \" guilty\", \" last\", \" week\", \" to\", \" brib\", \"ing\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"Z\", \"TE\", \" Z\", \"MAX\", \" 2\", \" Smart\", \"phone\", \"\\\\newline\", \"\\\\newline\", \"Z\", \"TE\", \" USA\", \" announced\", \" the\", \" release\", \" of\", \" Z\", \"MAX\", \" 2\", \",\", \"\\n\", \"Sport\", \" news\", \"\\\\newline\", \"\\\\newline\", \"Daniel\", \" Pol\", \"esh\", \"ch\", \"uk\", \" climbed\", \" to\", \" the\", \" top\", \" of\", \" the\", \" Under\", \" 1\", \"5\", \" world\", \" rankings\", \"\\n\", \"Hy\", \"unda\", \"i\", \" Motor\", \" Co\", \".\", \" officially\", \" launched\", \" Santa\", \" Fe\", \" in\", \" New\", \" York\", \" Auto\", \" Show\", \" 2\", \"0\", \"1\", \"2\", \" some\", \"\\n\", \"AP\", \"\\\\newline\", \"\\\\newline\", \"T\", \"ight\", \" end\", \" J\", \"erm\", \"aine\", \" G\", \"resh\", \"am\", \" visited\", \" with\", \" the\", \" Saints\", \" on\", \" Wednesday\", \" and\", \" it\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"I\", \"2\", \" Limited\", \"\\\\newline\", \"\\\\newline\", \"i\", \"2\", \" Limited\", \" was\", \" the\", \" UK\", \"-\", \"based\", \" arm\", \" of\", \" software\", \" company\", \" i\", \"2\", \" Group\", \"\\n\", \"Se\", \"ctions\", \"\\\\newline\", \"\\\\newline\", \"Bre\", \"aking\", \" Out\", \"\\\\newline\", \"\\\\newline\", \"Bre\", \"aking\", \" Out\", \" was\", \" staged\", \" around\", \" the\", \" Arn\", \"olf\", \"ini\", \" building\", \"\\n\", \"L\", \"avi\", \" was\", \" born\", \" in\", \" K\", \"rak\", \"ow\", \",\", \" Poland\", \",\", \" and\", \" was\", \" 2\", \" years\", \" old\", \" when\", \" World\", \" War\", \" II\", \"\\n\", \"MK\", \" Arena\", \"\\\\newline\", \"\\\\newline\", \"It\", \" first\", \" opened\", \" in\", \" 2\", \"0\", \"1\", \"4\", \" for\", \" the\", \" National\", \" Bad\", \"m\", \"inton\", \" Championships\", \" and\", \"\\n\", \"AU\", \"B\", \"URN\", \",\", \" Alabama\", \" --\", \" Detect\", \"ives\", \" were\", \" still\", \" gathering\", \" evidence\", \" this\", \" morning\", \" as\", \" steady\", \" rain\", \" began\", \" to\", \" fall\", \"\\n\", \"AU\", \"G\", \"UST\", \"A\", \" \\u2013\", \" An\", \" attorney\", \" for\", \" Maine\", \"\\u2019\", \"s\", \" largest\", \" medical\", \" marijuana\", \" nonprofit\", \" group\", \" says\", \" it\", \" will\", \" formally\", \"\\n\", \"The\", \" Colorado\", \" M\", \"amm\", \"oth\", \" secured\", \" its\", \" first\", \" victory\", \" of\", \" the\", \" season\", \" during\", \" an\", \" 8\", \"-\", \"7\", \" overtime\", \" win\", \" while\", \"\\n\", \"AU\", \"G\", \"UST\", \"A\", \" \\u2013\", \" An\", \" attorney\", \" for\", \" Maine\", \"\\u2019\", \"s\", \" largest\", \" medical\", \" marijuana\", \" nonprofit\", \" group\", \" says\", \" it\", \" will\", \" formally\", \"\\n\", \"The\", \" South\", \" Korean\", \" Ministry\", \" of\", \" Trade\", \",\", \" Industry\", \" and\", \" Energy\", \" (\", \"M\", \"OT\", \"IE\", \")\", \" announced\", \" in\", \" a\", \" press\", \" release\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"2\", \"0\", \"1\", \"3\", \" Santa\", \" Rosa\", \" local\", \" elections\", \"\\\\newline\", \"\\\\newline\", \"Local\", \" elections\", \" was\", \" held\", \" in\", \" \", \" Santa\", \" Rosa\", \" City\", \" on\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"I\", \"2\", \" Limited\", \"\\\\newline\", \"\\\\newline\", \"i\", \"2\", \" Limited\", \" was\", \" the\", \" UK\", \"-\", \"based\", \" arm\", \" of\", \" software\", \" company\", \" i\", \"2\", \" Group\", \"\\n\", \"News\", \" broadcasting\", \"\\\\newline\", \"\\\\newline\", \"News\", \" broadcasting\", \" is\", \" the\", \" medium\", \" of\", \" broadcasting\", \" of\", \" various\", \" news\", \" events\", \" and\", \" other\", \" information\", \" via\", \" television\", \"\\n\", \"AW\", \"AR\", \" \\u2013\", \" The\", \" C\", \"atch\", \" Up\", \"\\\\newline\", \"\\\\newline\", \"AW\", \"AR\", \" has\", \" been\", \" setting\", \" recording\", \" bo\", \"oths\", \" on\", \" fire\", \",\", \"\\n\", \"More\", \" like\", \" this\", \"\\\\newline\", \"\\\\newline\", \"N\", \"okia\", \" promises\", \" not\", \" one\", \" but\", \" two\", \" Lum\", \"ia\", \" 8\", \"0\", \"0\", \" power\", \" fix\", \" patches\", \"\\n\", \"AU\", \"B\", \"URN\", \",\", \" Alabama\", \" --\", \" Detect\", \"ives\", \" were\", \" still\", \" gathering\", \" evidence\", \" this\", \" morning\", \" as\", \" steady\", \" rain\", \" began\", \" to\", \" fall\", \"\\n\", \"Construction\", \" of\", \" human\", \" antibody\", \" gene\", \" libraries\", \" and\", \" selection\", \" of\", \" antibodies\", \" by\", \" phage\", \" display\", \".\", \"\\\\newline\", \"Ant\", \"ib\", \"ody\", \" phage\", \" display\", \"\\n\", \"News\", \"\\\\newline\", \"\\\\newline\", \"A\", \"ub\", \"urn\", \" Public\", \" Theater\", \" will\", \" welcome\", \" a\", \" pair\", \" of\", \" high\", \"-\", \"profile\", \" musicians\", \" within\", \" a\", \" week\", \"\\n\"], \"activations\": [[[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.1036468967795372]], [[-0.07128920406103134]], [[-0.15909190475940704]], [[2.1344494819641113]], [[1.0251617431640625]], [[-0.12700800597667694]], [[-0.12101477384567261]], [[0.25520962476730347]], [[0.43228909373283386]], [[1.7365078926086426]], [[0.7873294949531555]], [[0.5941225290298462]], [[0.8991771936416626]], [[-0.006413061637431383]], [[-0.06332230567932129]], [[0.01030634343624115]], [[0.06451930105686188]], [[-0.1610478013753891]], [[-0.13123159110546112]], [[-0.14817416667938232]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.1555556207895279]], [[-0.1491585224866867]], [[-0.1681729108095169]], [[-0.16983261704444885]], [[-0.16730155050754547]], [[-0.1683415174484253]], [[-0.16766859591007233]], [[-0.16975562274456024]], [[-0.11039689183235168]], [[-0.16868118941783905]], [[-0.054844558238983154]], [[0.08055391162633896]], [[1.9655526876449585]], [[2.100045919418335]], [[1.439429521560669]], [[0.7762389183044434]], [[-0.03978106006979942]], [[0.3271726071834564]], [[0.3935985863208771]], [[0.03740367665886879]], [[0.0]], [[-0.12765780091285706]], [[-0.1686808317899704]], [[-0.1110835000872612]], [[-0.14210109412670135]], [[-0.16273358464241028]], [[-0.16675615310668945]], [[-0.156318798661232]], [[-0.1259230077266693]], [[-0.16901575028896332]], [[-0.13622701168060303]], [[-0.08328075706958771]], [[-0.11776181310415268]], [[-0.060306645929813385]], [[0.256445050239563]], [[0.8724626898765564]], [[2.0409770011901855]], [[0.937505304813385]], [[-0.1666838824748993]], [[-0.16344429552555084]], [[-0.12761154770851135]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.1555556207895279]], [[-0.1491585224866867]], [[-0.1681729108095169]], [[-0.16983261704444885]], [[-0.16730155050754547]], [[-0.1683415174484253]], [[-0.16766859591007233]], [[-0.16975562274456024]], [[-0.11039689183235168]], [[-0.16868118941783905]], [[-0.054844558238983154]], [[0.08055391162633896]], [[1.9655526876449585]], [[2.100045919418335]], [[1.439429521560669]], [[0.7762389183044434]], [[-0.03978106006979942]], [[0.3271726071834564]], [[0.3935985863208771]], [[0.03740367665886879]], [[0.0]], [[-0.13674098253250122]], [[-0.15140412747859955]], [[0.5677497982978821]], [[0.1949831247329712]], [[-0.16371628642082214]], [[-0.04014063626527786]], [[-0.013569972477853298]], [[-0.07206707447767258]], [[-0.16457311809062958]], [[0.697787344455719]], [[0.7145314812660217]], [[1.3956611156463623]], [[1.460714340209961]], [[0.4816027879714966]], [[0.6051102876663208]], [[-0.14824548363685608]], [[0.11049169301986694]], [[0.0867038443684578]], [[1.0910402536392212]], [[1.9590404033660889]], [[0.0]], [[0.2486303448677063]], [[0.06204899027943611]], [[0.28776514530181885]], [[0.11046849191188812]], [[0.4553009569644928]], [[0.6201953291893005]], [[1.050276279449463]], [[1.9189642667770386]], [[0.63095623254776]], [[1.1323108673095703]], [[1.364184021949768]], [[0.29160788655281067]], [[0.8392959237098694]], [[0.754051923751831]], [[1.2860966920852661]], [[1.4544665813446045]], [[0.640407919883728]], [[0.7158641815185547]], [[0.47632554173469543]], [[-0.13678406178951263]], [[0.0]], [[0.496475487947464]], [[-0.16027776896953583]], [[-0.1620546132326126]], [[-0.15637394785881042]], [[-0.043963462114334106]], [[-0.15285280346870422]], [[-0.16181987524032593]], [[-0.15944017469882965]], [[-0.16629648208618164]], [[-0.16813696920871735]], [[-0.1251007467508316]], [[-0.16449229419231415]], [[1.8349809646606445]], [[-0.1563403308391571]], [[0.3177807033061981]], [[0.3340085744857788]], [[0.8625267744064331]], [[0.3839212954044342]], [[-0.137363001704216]], [[-0.024419456720352173]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.13427317142486572]], [[-0.09546630084514618]], [[-0.08909795433282852]], [[-0.14944495260715485]], [[-0.14154312014579773]], [[-0.11384527385234833]], [[-0.13332435488700867]], [[-0.16582131385803223]], [[0.12031517922878265]], [[0.908147394657135]], [[1.0938308238983154]], [[0.15821371972560883]], [[0.9049882888793945]], [[0.583045482635498]], [[-0.04396860674023628]], [[0.17170044779777527]], [[1.6281205415725708]], [[-0.16644497215747833]], [[0.19528605043888092]], [[1.8171755075454712]], [[0.0]], [[-0.08732831478118896]], [[-0.04339165985584259]], [[-0.10460158437490463]], [[-0.15761932730674744]], [[-0.16966553032398224]], [[-0.1531195342540741]], [[-0.13841259479522705]], [[-0.16141295433044434]], [[-0.1671864241361618]], [[-0.15873433649539948]], [[-0.14787402749061584]], [[-0.13816963136196136]], [[0.2386457324028015]], [[1.7419958114624023]], [[0.8547083139419556]], [[0.10507871955633163]], [[-0.11220870912075043]], [[-0.06653794646263123]], [[0.0708453506231308]], [[0.6694624423980713]], [[0.0]], [[-0.1036468967795372]], [[-0.07128920406103134]], [[-0.15909190475940704]], [[2.1344494819641113]], [[1.0251617431640625]], [[-0.12700800597667694]], [[-0.12101477384567261]], [[0.25520962476730347]], [[0.43228909373283386]], [[1.7365078926086426]], [[0.7873294949531555]], [[0.5941225290298462]], [[0.8991771936416626]], [[-0.006413061637431383]], [[-0.06332230567932129]], [[0.01030634343624115]], [[0.06451930105686188]], [[-0.1610478013753891]], [[-0.13123159110546112]], [[-0.14817416667938232]], [[0.0]], [[0.036039769649505615]], [[0.3126242756843567]], [[0.32847461104393005]], [[-0.09396911412477493]], [[-0.05595601350069046]], [[0.4222886562347412]], [[1.7346607446670532]], [[0.7805350422859192]], [[0.5714414119720459]], [[0.035349320620298386]], [[0.008282636292278767]], [[0.3361160457134247]], [[-0.10455022007226944]], [[0.19879202544689178]], [[0.45618295669555664]], [[0.09840086847543716]], [[-0.165049746632576]], [[0.1477631777524948]], [[1.3409065008163452]], [[0.23635846376419067]], [[0.0]], [[1.7000808715820312]], [[1.543555498123169]], [[1.0752545595169067]], [[0.644955039024353]], [[1.3121527433395386]], [[1.1394060850143433]], [[0.029488516971468925]], [[-0.1686243861913681]], [[0.12196257710456848]], [[-0.16908130049705505]], [[-0.16464388370513916]], [[0.2200520634651184]], [[0.5299125909805298]], [[0.12354952841997147]], [[0.004155140835791826]], [[0.09431322664022446]], [[-0.06600009649991989]], [[0.1077122911810875]], [[-0.09340319782495499]], [[-0.08746987581253052]], [[0.0]], [[1.7000808715820312]], [[1.6544133424758911]], [[0.8916471600532532]], [[1.120662808418274]], [[1.0475248098373413]], [[1.013394832611084]], [[0.3718426525592804]], [[-0.1690610647201538]], [[-0.12135878950357437]], [[0.31925275921821594]], [[0.3814510107040405]], [[0.37354689836502075]], [[0.023320404812693596]], [[0.14204128086566925]], [[0.6138650178909302]], [[0.6804458498954773]], [[0.3488002419471741]], [[0.09690357744693756]], [[0.15536244213581085]], [[0.09296506643295288]], [[0.0]], [[-0.16997091472148895]], [[-0.07419759035110474]], [[-0.16401948034763336]], [[-0.15920238196849823]], [[-0.1528056561946869]], [[-0.07939962297677994]], [[0.0021266501862555742]], [[0.00869783479720354]], [[0.8247143626213074]], [[-0.16804268956184387]], [[0.12941177189350128]], [[0.1313265562057495]], [[0.08977201581001282]], [[0.3482736051082611]], [[0.14353825151920319]], [[0.21116891503334045]], [[0.8274937272071838]], [[0.9461418390274048]], [[1.665120005607605]], [[-0.16801896691322327]], [[0.0]], [[1.7000808715820312]], [[1.6544133424758911]], [[0.8916471600532532]], [[1.120662808418274]], [[1.0475248098373413]], [[1.013394832611084]], [[0.3718426525592804]], [[-0.1690610647201538]], [[-0.12135878950357437]], [[0.31925275921821594]], [[0.3814510107040405]], [[0.37354689836502075]], [[0.023320404812693596]], [[0.14204128086566925]], [[0.6138650178909302]], [[0.6804458498954773]], [[0.3488002419471741]], [[0.09690357744693756]], [[0.15536244213581085]], [[0.09296506643295288]], [[0.0]], [[-0.16997091472148895]], [[-0.09287376701831818]], [[-0.11192946135997772]], [[0.04768500477075577]], [[-0.16990144550800323]], [[-0.14599524438381195]], [[-0.11054958403110504]], [[-0.13044555485248566]], [[-0.16378526389598846]], [[-0.15358774363994598]], [[-0.06198499724268913]], [[-0.1376069039106369]], [[-0.1510574072599411]], [[-0.16918475925922394]], [[-0.09685089439153671]], [[0.9531236290931702]], [[0.4127410352230072]], [[1.1901782751083374]], [[0.3062337338924408]], [[1.6525774002075195]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.04522882401943207]], [[-0.06282948702573776]], [[-0.045112673193216324]], [[-0.07043159008026123]], [[-0.11043203622102737]], [[-0.15552988648414612]], [[-0.14533725380897522]], [[-0.13464269042015076]], [[-0.15653997659683228]], [[-0.10642427206039429]], [[-0.1304280310869217]], [[0.26486513018608093]], [[1.2665082216262817]], [[1.647660493850708]], [[0.6099666357040405]], [[-0.16995853185653687]], [[-0.10282932221889496]], [[1.072086215019226]], [[0.8965487480163574]], [[1.2344279289245605]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[-0.13427317142486572]], [[-0.09546630084514618]], [[-0.08909795433282852]], [[-0.14944495260715485]], [[-0.14154312014579773]], [[-0.11384527385234833]], [[-0.13332435488700867]], [[-0.16582131385803223]], [[0.12031517922878265]], [[0.908147394657135]], [[1.0938308238983154]], [[0.15821371972560883]], [[0.9049882888793945]], [[0.583045482635498]], [[-0.04396860674023628]], [[0.17170044779777527]], [[1.6281205415725708]], [[-0.16644497215747833]], [[0.19528605043888092]], [[1.8171755075454712]], [[0.0]], [[0.3086838722229004]], [[0.22244276106357574]], [[0.34589722752571106]], [[0.28595802187919617]], [[0.7891134023666382]], [[1.075543999671936]], [[0.4480011463165283]], [[1.6176445484161377]], [[0.04625696316361427]], [[0.013464023359119892]], [[0.5372816920280457]], [[0.3051755726337433]], [[-0.10741566121578217]], [[0.35131776332855225]], [[0.9023914933204651]], [[-0.02697840891778469]], [[0.18497410416603088]], [[0.30508098006248474]], [[-0.06678283959627151]], [[0.026933981105685234]], [[0.0]], [[0.1053779348731041]], [[-0.04041187837719917]], [[0.8376308679580688]], [[0.8660728335380554]], [[0.46074193716049194]], [[0.2016477882862091]], [[-0.10272680968046188]], [[0.04293184354901314]], [[0.06942450255155563]], [[0.4659881889820099]], [[0.20664191246032715]], [[0.5224161148071289]], [[1.610062837600708]], [[-0.16864857077598572]], [[-0.16549736261367798]], [[-0.12458470463752747]], [[-0.1324327290058136]], [[0.08427735418081284]], [[0.26004910469055176]], [[0.25436025857925415]], [[0.0]], [[-0.1363385170698166]], [[0.03001912496984005]], [[-0.02055468037724495]], [[-0.0852862074971199]], [[-0.15591655671596527]], [[0.36690056324005127]], [[-0.16315482556819916]], [[0.670928955078125]], [[-0.12469925731420517]], [[-0.16602091491222382]], [[0.007409911137074232]], [[-0.021964136511087418]], [[0.0046387286856770515]], [[0.2650563418865204]], [[0.887407124042511]], [[0.2099609076976776]], [[0.6480388045310974]], [[0.2666425406932831]], [[1.5482583045959473]], [[0.21119867265224457]], [[0.0]], [[1.7000808715820312]], [[1.543555498123169]], [[1.0752545595169067]], [[0.644955039024353]], [[1.3121527433395386]], [[1.1394060850143433]], [[0.029488516971468925]], [[-0.1686243861913681]], [[0.12196257710456848]], [[-0.16908130049705505]], [[-0.16464388370513916]], [[0.2200520634651184]], [[0.5299125909805298]], [[0.12354952841997147]], [[0.004155140835791826]], [[0.09431322664022446]], [[-0.06600009649991989]], [[0.1077122911810875]], [[-0.09340319782495499]], [[-0.08746987581253052]], [[0.0]], [[0.7339012622833252]], [[0.8110767006874084]], [[0.46844029426574707]], [[1.3623266220092773]], [[2.1339969635009766]], [[1.8326104879379272]], [[0.6793203949928284]], [[1.440386176109314]], [[0.8701293468475342]], [[1.3355391025543213]], [[0.7916522026062012]], [[1.2982245683670044]], [[2.1776621341705322]], [[1.5043213367462158]], [[1.3980555534362793]], [[1.0320923328399658]], [[1.6478447914123535]], [[1.6281461715698242]], [[1.3377617597579956]], [[1.9743318557739258]], [[0.0]], [[0.3086838722229004]], [[0.1056690514087677]], [[0.32890644669532776]], [[1.5028377771377563]], [[0.4089758098125458]], [[0.9578718543052673]], [[0.7898035645484924]], [[0.3257555663585663]], [[0.43912994861602783]], [[0.43021032214164734]], [[0.6547256708145142]], [[0.7826440334320068]], [[0.22765903174877167]], [[0.011160383000969887]], [[0.6368570327758789]], [[0.6333709955215454]], [[0.6642969846725464]], [[0.029132086783647537]], [[0.2697546184062958]], [[0.9951680898666382]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try this for pythia 70M model, last layer. Try on 3 neurons."
      ],
      "metadata": {
        "id": "tEjnBPpUU_YQ"
      }
    }
  ]
}