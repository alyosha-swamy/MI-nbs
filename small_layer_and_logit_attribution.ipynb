{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ],
      "metadata": {
        "id": "7L4cVY0T0WaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqXMyWPozqvT",
        "outputId": "720fdd14-b8fb-42fa-89ab-cba5a851eb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-pujacnw4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-pujacnw4\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 6eca22f0acbe17458204026edbc74535f97c2704\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (2.11.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.6.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.14.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.22.4)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.2.15)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.28.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (13.3.3)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.14)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.13.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.27.1)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer-lens==0.0.0) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10->transformer-lens==0.0.0) (0.40.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.13.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.19.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.31)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.15)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.13.3->jaxtyping>=0.2.11->transformer-lens==0.0.0) (5.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping>=0.2.11->transformer-lens==0.0.0) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.9/dist-packages (1.39.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (5.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.15.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (11.10.3.66)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.10->circuitsvis) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.10->circuitsvis) (0.40.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "%pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "from circuitsvis.activations import text_neuron_activations\n",
        "from jaxtyping import Float, Int\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")\n",
        "from einops import rearrange\n",
        "class NeuronTextSimplifier:\n",
        "    def __init__(self, model, layer: int, neuron: int) -> None:\n",
        "        self.model = model\n",
        "        self.device = model.cfg.device\n",
        "        self.layer = layer\n",
        "        self.neuron = neuron\n",
        "        self.model.requires_grad_(False)\n",
        "        self.embed_weights = list(list(model.children())[0].parameters())[0]\n",
        "        if(\"pythia\" not in model.cfg.model_name):\n",
        "            transformer_block_loc = 4\n",
        "        else:\n",
        "            transformer_block_loc = 2\n",
        "        transformer_blocks = [mod for mod in list(self.model.children())[transformer_block_loc]]\n",
        "        self.model_no_embed = th.nn.Sequential(*(transformer_blocks[:layer+1]))\n",
        "        self.model_no_embed.requires_grad_(False)\n",
        "        self.set_hooks()\n",
        "\n",
        "    def set_hooks(self):\n",
        "        self._neurons = th.empty(0)\n",
        "        def hook(model, input, output):\n",
        "            self._neurons = output\n",
        "        self.model.blocks[self.layer].mlp.hook_post.register_forward_hook(hook)\n",
        "\n",
        "    def ablate_mlp_neurons(self, tokens, neurons: th.Tensor):\n",
        "        def mlp_ablation_hook(\n",
        "            value: Float[th.Tensor, \"batch pos d_mlp\"],\n",
        "            hook: HookPoint\n",
        "        ) -> Float[th.Tensor, \"batch pos d_mlp\"]:\n",
        "            if(neurons.shape[0] == 0):\n",
        "                return value\n",
        "            value[:, :, neurons] = 0\n",
        "            return value\n",
        "        return self.model.run_with_hooks(tokens, fwd_hooks=[(f\"blocks.{self.layer}.mlp.hook_post\", mlp_ablation_hook)])\n",
        "        \n",
        "    def add_noise_to_text(self, text, noise_level=1.0):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        activation_list = []\n",
        "        for t in text:\n",
        "            split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            tokens = self.model.to_tokens(t, prepend_bos=False)\n",
        "            # Add gaussian noise to the input of each word in turn, getting the diff in final neuron's response\n",
        "            embedded_tokens = self.model.embed(tokens)\n",
        "            batch_size, seq_size, embedding_size = embedded_tokens.shape\n",
        "            noise = th.randn(1, embedding_size, device=self.device)*noise_level\n",
        "            original = self.embedded_forward(embedded_tokens)[:,-1,self.neuron]\n",
        "            changed_activations = th.zeros(seq_size, device=self.device)\n",
        "            for i in range(seq_size):\n",
        "                embedded_tokens[:,i,:] += noise\n",
        "                neuron_response = self.embedded_forward(embedded_tokens)\n",
        "                changed_activations[i] = neuron_response[:,-1,self.neuron].item()\n",
        "                embedded_tokens[:,i,:] -= noise\n",
        "            changed_activations -= original\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            activation_list += changed_activations.tolist() + [0.0]\n",
        "        activation_list = th.tensor(activation_list).reshape(-1,1,1)\n",
        "        return text_neuron_activations(tokens=text_list, activations=activation_list)\n",
        "\n",
        "    def visualize_logit_diff(self, text, neurons: th.Tensor, setting=\"true_tokens\", verbose=True):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        logit_list = []\n",
        "        for t in text:\n",
        "            split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            tokens = self.model.to_tokens(t, prepend_bos=False)\n",
        "            original_logits = self.model(tokens).log_softmax(-1)\n",
        "            ablated_logits = self.ablate_mlp_neurons(tokens, neurons).log_softmax(-1)\n",
        "            diff_logits =  ablated_logits - original_logits\n",
        "            if setting == \"true_tokens\":\n",
        "                # Gather the logits for the true tokens\n",
        "                diff = rearrange(diff_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "            elif setting == \"max\":\n",
        "                val, ind = diff_logits.max(2)\n",
        "                diff = rearrange(val, \"b s -> (b s)\")\n",
        "                split_text = self.model.to_str_tokens(ind)\n",
        "                tokens = ind\n",
        "            if(verbose):\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "                orig = rearrange(original_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "                ablated = rearrange(ablated_logits.gather(2,tokens.unsqueeze(2)), \"b s n -> (b s n)\")\n",
        "                logit_list += orig.tolist() + [0.0]\n",
        "                logit_list += ablated.tolist() + [0.0]\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            logit_list += diff.tolist() + [0.0]\n",
        "        logit_list = th.tensor(logit_list).reshape(-1,1,1)\n",
        "        if verbose:\n",
        "            print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
        "        return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
        "\n",
        "    def get_neuron_activation(self, tokens):\n",
        "        _, cache = self.model.run_with_cache(tokens.to(self.model.cfg.device))\n",
        "        return cache[f\"blocks.{self.layer}.mlp.hook_post\"][0,:,self.neuron].tolist()\n",
        "\n",
        "    def text_to_activations_print(self, text):\n",
        "        token = self.model.to_tokens(text, prepend_bos=False)\n",
        "        act = self.get_neuron_activation(token)\n",
        "        act = [f\" [{a:.2f}]\" for a in act]\n",
        "        if(token.shape[-1] > 1):\n",
        "            string = self.model.to_str_tokens(token, prepend_bos=False)\n",
        "        else: \n",
        "            string = self.model.to_string(token)\n",
        "        res = [None]*(len(string)+len(act))\n",
        "        res[::2] = string\n",
        "        res[1::2] = act\n",
        "        return \"\".join(res)\n",
        "\n",
        "    def text_to_visualize(self, text):\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        text_list = []\n",
        "        act_list = []\n",
        "        for t in text:\n",
        "            if isinstance(t, str): # If the text is a list of tokens\n",
        "                split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "                token = self.model.to_tokens(t, prepend_bos=False)\n",
        "            else:\n",
        "                token = t\n",
        "                split_text = self.model.to_str_tokens(t, prepend_bos=False)\n",
        "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
        "            act_list+= self.get_neuron_activation(token) + [0.0]\n",
        "        act_list = th.tensor(act_list).reshape(-1,1,1)\n",
        "        return text_neuron_activations(tokens=text_list, activations=act_list)\n",
        "\n",
        "    def get_text_and_activations_iteratively(self, text):\n",
        "        tokens = self.model.to_tokens(text, prepend_bos=False)[0]\n",
        "        original_activation = self.get_neuron_activation(tokens)\n",
        "        # To get around the newline issue, we replace the newline with \\newline and then add a newline at the end\n",
        "        text_list = [x.replace('\\n', '\\\\newline') for x in self.model.to_str_tokens(text, prepend_bos=False)] + [\"\\n\"]\n",
        "        act_list = original_activation + [0.0]\n",
        "        changes = th.zeros(tokens.shape[-1])+100\n",
        "        for j in range(len(tokens)-1):\n",
        "            for i in range(len(tokens)):\n",
        "                changes[i] = self.get_neuron_activation(th.cat((tokens[:i],tokens[i+1:])))[-1]\n",
        "            max_ind = changes.argmax()\n",
        "            changes = th.cat((changes[:max_ind], changes[max_ind+1:]))\n",
        "            tokens = th.cat((tokens[:max_ind],tokens[max_ind+1:]))\n",
        "            if(tokens.shape[-1] > 1):\n",
        "                out_text = self.model.to_str_tokens(tokens, prepend_bos=False)\n",
        "                text_list += [x.replace('\\n', '\\\\newline') for x in out_text] + [\"\\n\"]\n",
        "            else:\n",
        "                out_text = self.model.to_string(tokens)\n",
        "                text_list += [out_text.replace('\\n', '\\\\newline')] + [\"\\n\"]\n",
        "            act_list += self.get_neuron_activation(tokens) + [0.0]\n",
        "        text_list = text_list\n",
        "        act_list = th.tensor(act_list).reshape(-1,1,1)\n",
        "        return text_list, act_list\n",
        "\n",
        "    def visualize_text_color_iteratively(self, text):\n",
        "        if(isinstance(text, str)):\n",
        "            text_list, act_list = self.get_text_and_activations_iteratively(text)\n",
        "            return text_neuron_activations(tokens=text_list, activations=act_list)\n",
        "        elif(isinstance(text, list)):\n",
        "            text_list_final = []\n",
        "            act_list_final = []\n",
        "            for t in range(len(text)):\n",
        "                text_list, act_list = self.get_text_and_activations_iteratively(text[t])\n",
        "                text_list_final.append(text_list)\n",
        "                act_list_final.append(act_list)\n",
        "            return text_neuron_activations(tokens=text_list_final, activations=act_list_final)\n",
        "\n",
        "    def simplify_iteratively(self, text):\n",
        "        # Iteratively remove text that has smallest decrease in activation\n",
        "        # Print out the change in activation for the largest changes, ie if the change is larger than threshold*original_activation\n",
        "        tokens = self.model.to_tokens(text, prepend_bos=False)[0]\n",
        "        self.text_to_activations_print(self.model.to_string(tokens))\n",
        "        original_activation = self.get_neuron_activation(tokens)[-1]\n",
        "        changes = th.zeros(tokens.shape[-1])+100\n",
        "        for j in range(len(tokens)-1):\n",
        "            for i in range(len(tokens)):\n",
        "                changes[i] = self.get_neuron_activation(th.cat((tokens[:i],tokens[i+1:])))[-1]\n",
        "            max_ind = changes.argmax()\n",
        "            changes = th.cat((changes[:max_ind], changes[max_ind+1:]))\n",
        "            tokens = th.cat((tokens[:max_ind],tokens[max_ind+1:]))\n",
        "            out_text = self.model.to_string(tokens)\n",
        "            print(self.text_to_activations_print(out_text))\n",
        "        return\n",
        "\n",
        "    # Assign neuron and layer\n",
        "    def set_layer_and_neuron(self, layer, neuron):\n",
        "        self.layer = layer\n",
        "        self.neuron = neuron\n",
        "        self.set_hooks()\n",
        "\n",
        "    def embedded_forward(self, embedded_x):\n",
        "        self.model_no_embed(embedded_x)\n",
        "        return self._neurons\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.model(x)       \n",
        "        return self._neurons\n",
        "\n",
        "    def prompt_optimization(\n",
        "            self,\n",
        "            diverse_outputs_num=10, \n",
        "            iteration_cap_until_convergence = 30,\n",
        "            init_text = None,\n",
        "            seq_size = 4,\n",
        "            insert_words_and_pos = None, #List of words and positions to insert [word, pos]\n",
        "            neuron_loss_scalar = 1,\n",
        "            diversity_loss_scalar = 1,\n",
        "        ):\n",
        "        _, _, embed_size = self.model.W_out.shape\n",
        "        vocab_size = self.model.W_E.shape[0]\n",
        "        largest_prompts = [None]*diverse_outputs_num\n",
        "        # Use dim-1 when we're doing a for loop (list comprehension)\n",
        "        # Use dim-2 when we're doing all at once\n",
        "        cos_dim_1 = th.nn.CosineSimilarity(dim=1)\n",
        "        cos_dim_2 = th.nn.CosineSimilarity(dim=2)\n",
        "        total_iterations = 0\n",
        "\n",
        "        if init_text is not None:\n",
        "            init_tokens = self.model.to_tokens(init_text, prepend_bos=False)\n",
        "            seq_size = init_tokens.shape[-1]\n",
        "        diverse_outputs = th.zeros(diverse_outputs_num, seq_size, embed_size).to(self.device)\n",
        "        for d_ind in range(diverse_outputs_num):\n",
        "            print(f\"Starting diverse output {d_ind}\")\n",
        "            if init_text is None:\n",
        "                # Random tokens of sequence length\n",
        "                init_tokens = th.randint(0, vocab_size, (1,seq_size))\n",
        "                init_text = self.model.to_string(init_tokens)\n",
        "            prompt_embeds = th.nn.Parameter(self.model.embed(init_tokens)).detach()\n",
        "            prompt_embeds.requires_grad_(True).to(self.device)\n",
        "\n",
        "            optim = th.optim.AdamW([prompt_embeds], lr=.8, weight_decay=0.01)\n",
        "            largest_activation = 0\n",
        "            largest_prompt = None\n",
        "\n",
        "            iterations_since_last_improvement = 0\n",
        "            while(iterations_since_last_improvement < iteration_cap_until_convergence):\n",
        "            # First, project into the embedding matrix\n",
        "                with th.no_grad():\n",
        "                    projected_index = th.stack([cos_dim_1(self.embed_weights,prompt_embeds[0,i,:]).argmax() for i in range(seq_size)]).unsqueeze(0)\n",
        "                    projected_embeds = self.model.embed(projected_index)\n",
        "\n",
        "                # Create a temp embedding that is detached from the graph, but has the same data as the projected embedding\n",
        "                tmp_embeds = prompt_embeds.detach().clone()\n",
        "                tmp_embeds.data = projected_embeds.data\n",
        "                # add some gaussian noise to tmp_embeds\n",
        "                # tmp_embeds.data += th.randn_like(tmp_embeds.data)*0.01\n",
        "                tmp_embeds.requires_grad_(True)\n",
        "\n",
        "                if insert_words_and_pos is not None:\n",
        "                    text = insert_words_and_pos[0]\n",
        "                    pos = insert_words_and_pos[1]\n",
        "                    if(pos == -1):\n",
        "                        pos = seq_size\n",
        "                    token = self.model.to_tokens(text, prepend_bos=False)\n",
        "                    token_embeds = self.model.embed(token)\n",
        "                    token_pos = pos\n",
        "                    wrapped_embeds = th.cat([tmp_embeds[0,:token_pos], token_embeds[0], tmp_embeds[0,token_pos:]], dim=0).unsqueeze(0)\n",
        "                    if(total_iterations == 0):\n",
        "                        wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
        "                        projected_index = th.stack([cos_dim_1(self.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
        "                        print(f\"Inserting {text} at pos {pos}: {self.model.to_str_tokens(projected_index, prepend_bos=False)}\")\n",
        "                else:\n",
        "                    wrapped_embeds = tmp_embeds\n",
        "\n",
        "                # Then, calculate neuron_output\n",
        "                neuron_output = self.embedded_forward(wrapped_embeds)[0,:, self.neuron]\n",
        "                if(d_ind > 0):\n",
        "                    diversity_loss = cos_dim_2(tmp_embeds[0], diverse_outputs[:d_ind])\n",
        "                    # return cos, tmp_embeds, diverse_outputs\n",
        "                else:\n",
        "                    diversity_loss = th.zeros(1)\n",
        "\n",
        "                loss = neuron_loss_scalar*-neuron_output[-1] + diversity_loss_scalar*diversity_loss.mean()\n",
        "\n",
        "                # Save the highest activation\n",
        "                if neuron_output[-1] > largest_activation:\n",
        "                    iterations_since_last_improvement = 0\n",
        "                    largest_activation = neuron_output[-1]\n",
        "                    wrapped_embeds_seq_len = wrapped_embeds.shape[1]\n",
        "                    projected_index = th.stack([cos_dim_1(self.embed_weights,wrapped_embeds[0,i,:]).argmax() for i in range(wrapped_embeds_seq_len)]).unsqueeze(0)\n",
        "                    largest_prompt = self.model.to_string(projected_index)\n",
        "                    largest_prompts[d_ind] = largest_prompt\n",
        "                    print(f\"New largest activation: {largest_activation} | {largest_prompt}\")\n",
        "\n",
        "                # Transfer the gradient to the continuous embedding space\n",
        "                prompt_embeds.grad, = th.autograd.grad(loss, [tmp_embeds])\n",
        "                \n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                total_iterations += 1\n",
        "                iterations_since_last_improvement += 1\n",
        "                init_text = None\n",
        "            diverse_outputs[d_ind] = tmp_embeds.data[0,...]\n",
        "        return largest_prompts\n"
      ],
      "metadata": {
        "id": "1uwEUgp60NS7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "87KpvUmv0cAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Transformer Lens, and load pythia models\n",
        "from transformer_lens import HookedTransformer\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import numpy as np \n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "device = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_NAME_LIST = [\n",
        "     \"EleutherAI/pythia-70m-deduped\", \n",
        "    # \"EleutherAI/pythia-160m-deduped\", \n",
        "    # \"EleutherAI/pythia-410m-deduped\", \n",
        "    # \"gpt2\", \n",
        "    # \"gpt2-medium\",\n",
        "    # \"solu-1l\",\n",
        "    # \"solu-2l\",\n",
        "    # \"solu-3l\",\n",
        "    # \"solu-4l\",\n",
        "    # \"gelu-2l\"\n",
        "]\n",
        "model_name = MODEL_NAME_LIST[0]\n",
        "layer = 1 # Layer 1 is actually the 2nd layer because 0-indexing\n",
        "\n",
        "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
        "\n",
        "# Only get the first twenty tokens of every datapoint\n",
        "Token_amount = 20\n",
        "\n",
        "# Load the training set from pile-10k\n",
        "d = load_dataset(\"NeelNanda/pile-10k\", split=\"train\").map(\n",
        "    lambda x: model.tokenizer(x['text']),\n",
        "    batched=True,\n",
        ").filter(\n",
        "    lambda x: len(x['input_ids']) > Token_amount\n",
        ").map(\n",
        "    lambda x: {'input_ids': x['input_ids'][:Token_amount]}\n",
        ")\n",
        "neurons = model.W_in.shape[-1]\n",
        "datapoints = d.num_rows\n",
        "batch_size = 64\n",
        "\n",
        "neuron_activations = th.zeros((datapoints*Token_amount, neurons))\n",
        "\n",
        "with th.no_grad(), d.formatted_as(\"pt\"):\n",
        "    dl = DataLoader(d[\"input_ids\"], batch_size=batch_size)\n",
        "    for i, batch in enumerate(tqdm(dl)):\n",
        "        _, cache = model.run_with_cache(batch.to(device))\n",
        "        neuron_activations[i*batch_size*Token_amount:(i+1)*batch_size*Token_amount,:] = rearrange(cache[f\"blocks.{layer}.mlp.hook_post\"], \"b s n -> (b s) n\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7_8yfw0fk7",
        "outputId": "d322bfb7-661e-4954-c3ba-37eaad418d6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a791fbe331dd39b3.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d7970ffdef26df61.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9421d564428143f5.arrow\n",
            "100%|██████████| 156/156 [00:10<00:00, 14.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Top Examples"
      ],
      "metadata": {
        "id": "68Q5t6ik0j9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try looking at several neurons for like 5 minutes maximum. Some will be much more interpretable, and other's won't. "
      ],
      "metadata": {
        "id": "M1UWXJm738cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick our specific neuron\n",
        "neuron = 0\n",
        "# Top k datapoint examples\n",
        "k = 10\n",
        "simplifier = NeuronTextSimplifier(model, layer, neuron)\n",
        "\n",
        "values, indices = neuron_activations[:,neuron].topk(k)\n",
        "\n",
        "max_datapoints = [np.unravel_index(i, (datapoints, Token_amount)) for i in indices]\n",
        "\n",
        "text_list = []\n",
        "full_text = []\n",
        "for md, s_ind in max_datapoints:\n",
        "    md = int(md)\n",
        "    s_ind = int(s_ind)\n",
        "    # Get the full text\n",
        "    full_tok = th.tensor(d[md][\"input_ids\"])\n",
        "    full_text.append(model.tokenizer.decode(full_tok))\n",
        "    \n",
        "    # Get just the text up until the max-activating example\n",
        "    tok = d[md][\"input_ids\"][:s_ind+1]\n",
        "    text = model.tokenizer.decode(tok)\n",
        "    text_list.append(text)"
      ],
      "metadata": {
        "id": "5by6Ncx80kQx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the whole text's activation on that neuron.\n",
        "Blue is positive (also has a hovertip if you put your mouse over) and red is negative, which you can interpret as \"no activation\" as opposed to \"opposite\". \n",
        "\n",
        "NOTE: \"Layer\" & \"Neuron\" are fake UI elements. My code is a hack."
      ],
      "metadata": {
        "id": "OSn4AdL54UJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.text_to_visualize(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "0n85VlHA2FDX",
        "outputId": "3a9a4fe1-2165-4af5-cf6b-dffe88e0b9d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f06d505d910>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-e6d419b1-a6f2\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-e6d419b1-a6f2\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"Br\", \"ace\", \" yourselves\", \",\", \" starting\", \" March\", \" 11\", \",\", \" 2014\", \",\", \" a\", \" new\", \" line\", \" of\", \" Game\", \" of\", \" Th\", \"rones\", \"-\", \"inspired\", \"\\n\", \"And\", \" Now\", \",\", \" a\", \" Week\", \" of\", \" Full\", \"-\", \"On\", \" Terror\", \"ism\", \" Against\", \" Republicans\", \"\\\\newline\", \"\\\\newline\", \"Nice\", \" work\", \" @\", \"Jeff\", \"Fl\", \"\\n\", \"Br\", \"ace\", \" yourselves\", \",\", \" starting\", \" March\", \" 11\", \",\", \" 2014\", \",\", \" a\", \" new\", \" line\", \" of\", \" Game\", \" of\", \" Th\", \"rones\", \"-\", \"inspired\", \"\\n\", \"Of\", \" manifest\", \"os\", \" and\", \" man\", \"ias\", \"\\\\newline\", \"\\\\newline\", \"To\", \" say\", \" that\", \" the\", \" market\", \" is\", \" happy\", \" with\", \" the\", \" Congress\", \" victory\", \" would\", \"\\n\", \"Share\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \"Welcome\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \" Cl\", \"othes\", \" Wh\", \"oles\", \"ale\", \" &\", \" Br\", \"anded\", \" clothes\", \"\\n\", \"A\", \"fin\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \"able\", \" dans\", \" ses\", \" super\", \"m\", \"arch\", \"\\u00e9s\", \" et\", \" ses\", \" pharmac\", \"\\n\", \"UN\", \" secretary\", \" general\", \" Ban\", \" Ki\", \"-\", \"m\", \"oon\", \",\", \" second\", \" left\", \",\", \" has\", \" given\", \" world\", \" leaders\", \" a\", \" video\", \" p\", \"ep\", \"\\n\", \"The\", \" unequal\", \" familiarity\", \" of\", \" f\", \"andom\", \"\\\\newline\", \"\\\\newline\", \"The\", \" unequal\", \" familiarity\", \" of\", \" f\", \"andom\", \"\\\\newline\", \"\\\\newline\", \"Because\", \" I\", \"\\u2019\", \"m\", \"\\n\", \"I\", \" like\", \" to\", \" know\", \" what\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" like\", \" me\", \",\", \" tick\", \".\", \"\\n\", \"Forg\", \"otten\", \" password\", \"\\\\newline\", \"\\\\newline\", \"N\", \"iche\", \" Jobs\", \" Ltd\", \" Privacy\", \" Policy\", \"\\\\newline\", \"\\\\newline\", \"N\", \"urs\", \"es\", \".\", \"co\", \".\", \"uk\", \"\\n\"], \"activations\": [[[-0.16233280301094055]], [[-0.12337060272693634]], [[1.1842732429504395]], [[2.929468870162964]], [[2.2609941959381104]], [[0.3793198764324188]], [[0.8624223470687866]], [[-0.16165810823440552]], [[-0.08705119788646698]], [[-0.10430252552032471]], [[-0.16990990936756134]], [[-0.16990603506565094]], [[-0.1567808985710144]], [[-0.12912382185459137]], [[-0.056227702647447586]], [[-0.06089732423424721]], [[-0.1278403401374817]], [[-0.1395808309316635]], [[0.4392298460006714]], [[-0.15106719732284546]], [[0.0]], [[1.5518381595611572]], [[2.452413320541382]], [[1.4233020544052124]], [[0.06434060633182526]], [[-0.16959741711616516]], [[-0.12283220142126083]], [[-0.14508560299873352]], [[0.14038044214248657]], [[0.09674780815839767]], [[-0.16944053769111633]], [[0.1401810199022293]], [[-0.1532488316297531]], [[0.24274633824825287]], [[-0.16640892624855042]], [[-0.12956584990024567]], [[-0.06616201251745224]], [[-0.15062764286994934]], [[-0.15850502252578735]], [[-0.15209251642227173]], [[-0.10997645556926727]], [[0.0]], [[-0.16233280301094055]], [[-0.12337060272693634]], [[1.1842732429504395]], [[2.929468870162964]], [[2.2609941959381104]], [[0.3793198764324188]], [[0.8624223470687866]], [[-0.16165810823440552]], [[-0.08705119788646698]], [[-0.10430252552032471]], [[-0.16990990936756134]], [[-0.16990603506565094]], [[-0.1567808985710144]], [[-0.12912382185459137]], [[-0.056227702647447586]], [[-0.06089732423424721]], [[-0.1278403401374817]], [[-0.1395808309316635]], [[0.4392298460006714]], [[-0.15106719732284546]], [[0.0]], [[1.6299506425857544]], [[2.193253517150879]], [[-0.1566627025604248]], [[-0.07287975400686264]], [[-0.1464991569519043]], [[-0.1698356568813324]], [[-0.08852940052747726]], [[-0.1684747040271759]], [[0.4005814492702484]], [[-0.13665607571601868]], [[-0.0978829637169838]], [[-0.15922607481479645]], [[0.20096461474895477]], [[-0.0001996715145651251]], [[0.1527082920074463]], [[-0.15351608395576477]], [[-0.16679230332374573]], [[0.2937583029270172]], [[-0.03112093359231949]], [[-0.1556510031223297]], [[0.0]], [[-0.0003606821410357952]], [[-0.004122380167245865]], [[-0.0334215871989727]], [[-0.012094234116375446]], [[-0.003269439097493887]], [[-0.04555831849575043]], [[-0.07653303444385529]], [[-0.14163917303085327]], [[-0.16537782549858093]], [[-0.14779488742351532]], [[-0.08342846482992172]], [[-0.14369086921215057]], [[-0.16996894776821136]], [[-0.1630605161190033]], [[2.137584686279297]], [[0.06929241865873337]], [[-0.08993428945541382]], [[-0.1606818288564682]], [[-0.16787713766098022]], [[-0.154249370098114]], [[0.0]], [[-0.015363485552370548]], [[-0.045568279922008514]], [[-0.07119583338499069]], [[-0.1461605429649353]], [[-0.08162079751491547]], [[-0.1113923043012619]], [[-0.08321648091077805]], [[0.451577365398407]], [[0.5110671520233154]], [[-0.03468487784266472]], [[-0.16068390011787415]], [[0.0374823659658432]], [[-0.06346933543682098]], [[2.108785390853882]], [[-0.06676432490348816]], [[-0.09187091886997223]], [[-0.1144753247499466]], [[-0.1649945229291916]], [[-0.1674274504184723]], [[0.8554632067680359]], [[0.0]], [[0.4702589511871338]], [[2.0315451622009277]], [[0.19131100177764893]], [[-0.1649911105632782]], [[-0.16121578216552734]], [[-0.08576462417840958]], [[-0.1425430178642273]], [[-0.02851598896086216]], [[-0.1469239741563797]], [[-0.13171537220478058]], [[-0.10768617689609528]], [[-0.10116655379533768]], [[-0.12179999053478241]], [[-0.169138565659523]], [[-0.15751734375953674]], [[-0.05860321968793869]], [[-0.13740958273410797]], [[-0.10637214034795761]], [[-0.14285241067409515]], [[-0.13455279171466827]], [[0.0]], [[-0.16630075871944427]], [[0.21967552602291107]], [[1.8660134077072144]], [[-0.07568572461605072]], [[0.3477989733219147]], [[0.11624563485383987]], [[-0.16993805766105652]], [[-0.1398916244506836]], [[-0.16748081147670746]], [[0.021574733778834343]], [[1.2036018371582031]], [[-0.11742203682661057]], [[0.1500241756439209]], [[-0.0010146111017093062]], [[-0.1547463983297348]], [[-0.15573419630527496]], [[-0.1561906933784485]], [[-0.16988754272460938]], [[-0.1654568612575531]], [[-0.04876553267240524]], [[0.0]], [[-0.16990581154823303]], [[-0.0779690369963646]], [[-0.16932381689548492]], [[-0.16616521775722504]], [[-0.1320544183254242]], [[-0.1370353251695633]], [[-0.16773857176303864]], [[-0.15643741190433502]], [[-0.12096794694662094]], [[0.11902134865522385]], [[-0.13383635878562927]], [[0.4429062008857727]], [[-0.011489985510706902]], [[0.4968622922897339]], [[-0.014616276137530804]], [[0.4643462002277374]], [[-0.1699434071779251]], [[0.09605882316827774]], [[1.8450031280517578]], [[-0.16351190209388733]], [[0.0]], [[0.3088851571083069]], [[1.0221596956253052]], [[1.7783520221710205]], [[-0.12571993470191956]], [[-0.14952807128429413]], [[-0.15830573439598083]], [[0.30040451884269714]], [[-0.10300726443529129]], [[-0.11857741326093674]], [[-0.1430286020040512]], [[-0.023201221600174904]], [[-0.058937106281518936]], [[-0.03451482579112053]], [[-0.045193690806627274]], [[-0.13730141520500183]], [[-0.03797638788819313]], [[-0.13964641094207764]], [[-0.0938945934176445]], [[-0.08456248044967651]], [[-0.11933530122041702]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we add gaussian noise to each token and see the effect on the last token? So for 20 token-text, we'll add noise to the first token, run the model, save that activation, then repeat with adding noise to the second token (but not the first). Below you'll see Red, as in adding noise to this token caused a decrease in activation. Some evidence that this token is important. "
      ],
      "metadata": {
        "id": "mC-r5dmX4jL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.add_noise_to_text(text_list, noise_level=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ucuNzgxz2K9u",
        "outputId": "e8e0e11d-14ed-40c1-d5c0-6109a952e910"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f05b0b32f40>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-3aae69ad-fb84\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-3aae69ad-fb84\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"Br\", \"ace\", \" yourselves\", \",\", \"\\n\", \"And\", \" Now\", \"\\n\", \"Br\", \"ace\", \" yourselves\", \",\", \" starting\", \"\\n\", \"Of\", \" manifest\", \"\\n\", \"Share\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \"Welcome\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \" Cl\", \"othes\", \" Wh\", \"oles\", \"\\n\", \"A\", \"fin\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \"able\", \" dans\", \" ses\", \" super\", \"\\n\", \"UN\", \" secretary\", \"\\n\", \"The\", \" unequal\", \" familiarity\", \"\\n\", \"I\", \" like\", \" to\", \" know\", \" what\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" like\", \" me\", \",\", \" tick\", \"\\n\", \"Forg\", \"otten\", \" password\", \"\\n\"], \"activations\": [[[0.25026726722717285]], [[-0.25838279724121094]], [[-2.7566661834716797]], [[-2.197512149810791]], [[0.0]], [[-2.6006646156311035]], [[-1.8911104202270508]], [[0.0]], [[-0.5349278450012207]], [[-1.1944414377212524]], [[-2.4247820377349854]], [[-1.8585070371627808]], [[-1.6065236330032349]], [[0.0]], [[-2.345430850982666]], [[-2.161984920501709]], [[0.0]], [[0.008830070495605469]], [[-0.04559659957885742]], [[-0.05781364440917969]], [[-0.08249831199645996]], [[-0.08684039115905762]], [[0.09520506858825684]], [[-0.08516955375671387]], [[-0.09366750717163086]], [[-0.1563122272491455]], [[-0.1081395149230957]], [[-0.0738821029663086]], [[-0.029742002487182617]], [[-0.1939336061477661]], [[-1.9725749492645264]], [[-2.1868319511413574]], [[0.0]], [[0.003376007080078125]], [[0.004738330841064453]], [[-0.0819559097290039]], [[-0.00931096076965332]], [[-0.0065135955810546875]], [[-0.0028116703033447266]], [[-0.07188987731933594]], [[0.0077054500579833984]], [[0.005081892013549805]], [[-0.057375431060791016]], [[0.0170590877532959]], [[-0.06443667411804199]], [[-0.35597121715545654]], [[-1.613923192024231]], [[0.0]], [[-1.8221954107284546]], [[-2.0719082355499268]], [[0.0]], [[-0.602519154548645]], [[-1.4110018014907837]], [[-1.124307632446289]], [[0.0]], [[-0.050343871116638184]], [[0.06674385070800781]], [[-0.029968976974487305]], [[0.03818035125732422]], [[0.06252801418304443]], [[0.020757436752319336]], [[0.06580841541290283]], [[-0.0489886999130249]], [[-0.39104318618774414]], [[0.016492366790771484]], [[-0.04310894012451172]], [[0.017145514488220215]], [[0.007500767707824707]], [[-0.006356954574584961]], [[-0.24196064472198486]], [[0.14312314987182617]], [[-0.12499535083770752]], [[-0.05242109298706055]], [[-2.013218879699707]], [[0.0]], [[-1.9042537212371826]], [[-0.735503077507019]], [[-1.9464770555496216]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove each token that has the least effect on the last token's neuron activation. Should see something similar to the noise i.e. the tokens that have the most effect when noised should be the last tokens to be removed below. \n",
        "\n",
        "NOTE: change \"Samples per page\" to a larger number to see more. "
      ],
      "metadata": {
        "id": "ntx2GfSa3U4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.visualize_text_color_iteratively(text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "oPYLkh9A2QM5",
        "outputId": "2f8cead1-4bf1-4a85-ad38-c9ed2e61d006"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f0621fa47f0>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-fddc931b-3172\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-fddc931b-3172\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [[\"Br\", \"ace\", \" yourselves\", \",\", \"\\n\", \"ace\", \" yourselves\", \",\", \"\\n\", \" yourselves\", \",\", \"\\n\", \" yourselves\", \"\\n\"], [\"And\", \" Now\", \"\\n\", \"And\", \"\\n\"], [\"Br\", \"ace\", \" yourselves\", \",\", \" starting\", \"\\n\", \"Br\", \"ace\", \" yourselves\", \",\", \"\\n\", \"ace\", \" yourselves\", \",\", \"\\n\", \" yourselves\", \",\", \"\\n\", \" yourselves\", \"\\n\"], [\"Of\", \" manifest\", \"\\n\", \"Of\", \"\\n\"], [\"Share\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \"Welcome\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \" Cl\", \"othes\", \" Wh\", \"oles\", \"\\n\", \"Share\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \" Cl\", \"othes\", \" Wh\", \"oles\", \"\\n\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \" Cl\", \"othes\", \" Wh\", \"oles\", \"\\n\", \" on\", \":\", \"\\\\newline\", \"\\\\newline\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \"othes\", \" Wh\", \"oles\", \"\\n\", \":\", \"\\\\newline\", \"\\\\newline\", \" to\", \" the\", \" world\", \" of\", \" Stock\", \"othes\", \" Wh\", \"oles\", \"\\n\", \":\", \"\\\\newline\", \"\\\\newline\", \" the\", \" world\", \" of\", \" Stock\", \"othes\", \" Wh\", \"oles\", \"\\n\", \":\", \"\\\\newline\", \"\\\\newline\", \" world\", \" of\", \" Stock\", \"othes\", \" Wh\", \"oles\", \"\\n\", \":\", \"\\\\newline\", \"\\\\newline\", \" world\", \" of\", \"othes\", \" Wh\", \"oles\", \"\\n\", \"\\\\newline\", \"\\\\newline\", \" world\", \" of\", \"othes\", \" Wh\", \"oles\", \"\\n\", \"\\\\newline\", \" world\", \" of\", \"othes\", \" Wh\", \"oles\", \"\\n\", \" world\", \" of\", \"othes\", \" Wh\", \"oles\", \"\\n\", \" world\", \" of\", \" Wh\", \"oles\", \"\\n\", \" of\", \" Wh\", \"oles\", \"\\n\", \" Wh\", \"oles\", \"\\n\", \" Wh\", \"\\n\"], [\"A\", \"fin\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \"able\", \" dans\", \" ses\", \" super\", \"\\n\", \"A\", \"fin\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \" dans\", \" ses\", \" super\", \"\\n\", \"fin\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \" dans\", \" ses\", \" super\", \"\\n\", \" de\", \" maint\", \"en\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \" dans\", \" ses\", \" super\", \"\\n\", \" de\", \" maint\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \" dans\", \" ses\", \" super\", \"\\n\", \" de\", \" maint\", \"ir\", \" un\", \" invent\", \"aire\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" maint\", \" un\", \" invent\", \"aire\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" un\", \" invent\", \"aire\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" invent\", \"aire\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" invent\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" conven\", \" ses\", \" super\", \"\\n\", \" de\", \" ses\", \" super\", \"\\n\", \" ses\", \" super\", \"\\n\", \" ses\", \"\\n\"], [\"UN\", \" secretary\", \"\\n\", \"UN\", \"\\n\"], [\"The\", \" unequal\", \" familiarity\", \"\\n\", \" unequal\", \" familiarity\", \"\\n\", \" unequal\", \"\\n\"], [\"I\", \" like\", \" to\", \" know\", \" what\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" like\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" like\", \" to\", \" know\", \" what\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" like\", \" to\", \" know\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \" know\", \" makes\", \" people\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \" know\", \" makes\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \" makes\", \",\", \" flesh\", \"-\", \"and\", \"-\", \"blood\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \" makes\", \",\", \" flesh\", \"-\", \"and\", \"-\", \" human\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \" makes\", \",\", \" flesh\", \"-\", \"and\", \"-\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \",\", \" flesh\", \"-\", \"and\", \"-\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \",\", \" flesh\", \"and\", \"-\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" to\", \",\", \" flesh\", \"and\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \",\", \" flesh\", \"and\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \",\", \" flesh\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \"I\", \" flesh\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \" flesh\", \" beings\", \" me\", \",\", \" tick\", \"\\n\", \" flesh\", \" me\", \",\", \" tick\", \"\\n\", \" flesh\", \",\", \" tick\", \"\\n\", \" flesh\", \" tick\", \"\\n\", \" flesh\", \"\\n\"], [\"Forg\", \"otten\", \" password\", \"\\n\", \"Forg\", \" password\", \"\\n\", \"Forg\", \"\\n\"]], \"activations\": [[[[-0.16233274340629578]], [[-0.1233709454536438]], [[1.1842728853225708]], [[2.9294662475585938]], [[0.0]], [[1.2791318893432617]], [[2.330402374267578]], [[3.3390157222747803]], [[0.0]], [[2.6733713150024414]], [[3.2234649658203125]], [[0.0]], [[2.6733713150024414]], [[0.0]]], [[[1.5518381595611572]], [[2.4524123668670654]], [[0.0]], [[1.5518383979797363]], [[0.0]]], [[[-0.16233274340629578]], [[-0.12337099015712738]], [[1.1842739582061768]], [[2.92946720123291]], [[2.2609970569610596]], [[0.0]], [[-0.16233274340629578]], [[-0.1233709454536438]], [[1.1842728853225708]], [[2.9294662475585938]], [[0.0]], [[1.2791318893432617]], [[2.330402374267578]], [[3.3390157222747803]], [[0.0]], [[2.6733713150024414]], [[3.2234649658203125]], [[0.0]], [[2.6733713150024414]], [[0.0]]], [[[1.629951000213623]], [[2.193253517150879]], [[0.0]], [[1.6299514770507812]], [[0.0]]], [[[-0.0003606821992434561]], [[-0.004122382029891014]], [[-0.03342147916555405]], [[-0.012094234116375446]], [[-0.003269437002018094]], [[-0.04555835947394371]], [[-0.07653306424617767]], [[-0.14163920283317566]], [[-0.16537787020206451]], [[-0.1477949321269989]], [[-0.08342865109443665]], [[-0.1436910331249237]], [[-0.16996894776821136]], [[-0.16306056082248688]], [[2.1375861167907715]], [[0.0]], [[-0.0003606821992434561]], [[-0.004122382029891014]], [[-0.03342147916555405]], [[-0.012094234116375446]], [[-0.003269437002018094]], [[-0.05522023141384125]], [[-0.126412034034729]], [[-0.14239631593227386]], [[-0.14952452480793]], [[-0.12460923194885254]], [[-0.13989843428134918]], [[-0.16945505142211914]], [[-0.15988071262836456]], [[2.317925214767456]], [[0.0]], [[-0.16137473285198212]], [[-0.1649734228849411]], [[-0.12801720201969147]], [[-0.09530571103096008]], [[-0.151319220662117]], [[-0.16817617416381836]], [[-0.04747187718749046]], [[-0.1632092148065567]], [[0.14351965487003326]], [[-0.16387563943862915]], [[-0.16826453804969788]], [[-0.16611012816429138]], [[2.3951730728149414]], [[0.0]], [[-0.16137473285198212]], [[-0.1649734228849411]], [[-0.12801720201969147]], [[-0.09530571103096008]], [[-0.151319220662117]], [[-0.16817617416381836]], [[-0.04747187718749046]], [[-0.1632092148065567]], [[0.14351965487003326]], [[-0.16543453931808472]], [[-0.16868466138839722]], [[2.4580202102661133]], [[0.0]], [[-0.12069845199584961]], [[-0.13046717643737793]], [[-0.10149893909692764]], [[-0.1405748724937439]], [[-0.1694902777671814]], [[-0.053339116275310516]], [[-0.15871046483516693]], [[0.12651436030864716]], [[-0.16488158702850342]], [[-0.16886527836322784]], [[2.518474817276001]], [[0.0]], [[-0.12069845199584961]], [[-0.13046717643737793]], [[-0.10149893909692764]], [[-0.10775451362133026]], [[-0.1316606104373932]], [[-0.13509470224380493]], [[0.1139707937836647]], [[-0.15767501294612885]], [[-0.1694415956735611]], [[2.599635601043701]], [[0.0]], [[-0.12069854885339737]], [[-0.13046714663505554]], [[-0.10149891674518585]], [[-0.12744738161563873]], [[-0.09491971135139465]], [[0.14852122962474823]], [[-0.15015248954296112]], [[-0.16766279935836792]], [[2.648235321044922]], [[0.0]], [[-0.12069854885339737]], [[-0.13046713173389435]], [[-0.10149888694286346]], [[-0.12744738161563873]], [[-0.09491968899965286]], [[-0.14286285638809204]], [[-0.028583455830812454]], [[2.722116708755493]], [[0.0]], [[-0.1423616260290146]], [[-0.14236165583133698]], [[-0.13668496906757355]], [[-0.015069713816046715]], [[-0.09918270260095596]], [[-0.006751224398612976]], [[2.806553602218628]], [[0.0]], [[-0.1423616260290146]], [[-0.10638155788183212]], [[0.019110387191176414]], [[-0.0783630907535553]], [[0.014508801512420177]], [[2.871889591217041]], [[0.0]], [[0.26426321268081665]], [[0.00946945883333683]], [[-0.044492460787296295]], [[0.01805460825562477]], [[2.885016679763794]], [[0.0]], [[0.2642629146575928]], [[0.009469305165112019]], [[0.048531290143728256]], [[2.8408870697021484]], [[0.0]], [[-0.10713230818510056]], [[-0.0725395455956459]], [[2.6961584091186523]], [[0.0]], [[0.7023375630378723]], [[1.7352758646011353]], [[0.0]], [[0.7023375630378723]], [[0.0]]], [[[-0.015363485552370548]], [[-0.04556838050484657]], [[-0.0711958035826683]], [[-0.14616048336029053]], [[-0.0816207230091095]], [[-0.11139240860939026]], [[-0.08321640640497208]], [[0.45157715678215027]], [[0.5110673904418945]], [[-0.03468490019440651]], [[-0.16068392992019653]], [[0.0374823659658432]], [[-0.0634692907333374]], [[2.10878586769104]], [[0.0]], [[-0.015363485552370548]], [[-0.04556838050484657]], [[-0.0711958035826683]], [[-0.14616048336029053]], [[-0.0816207230091095]], [[-0.11139240860939026]], [[-0.08321640640497208]], [[0.45157715678215027]], [[0.5110673904418945]], [[-0.03468490019440651]], [[0.043352462351322174]], [[-0.07856099307537079]], [[2.128185749053955]], [[0.0]], [[-0.11736034601926804]], [[-0.10635892301797867]], [[-0.11991207301616669]], [[-0.0880986750125885]], [[-0.11696680635213852]], [[-0.09328441321849823]], [[0.5086030960083008]], [[0.5694448947906494]], [[-0.016625335440039635]], [[0.0709703117609024]], [[-0.06648986786603928]], [[2.154796600341797]], [[0.0]], [[-0.15767835080623627]], [[-0.10222552716732025]], [[-0.08979512751102448]], [[-0.11237811297178268]], [[-0.10817544907331467]], [[0.7596867084503174]], [[0.6202079057693481]], [[0.10788590461015701]], [[0.08242439478635788]], [[-0.05296063795685768]], [[2.178589344024658]], [[0.0]], [[-0.15767835080623627]], [[-0.10222552716732025]], [[-0.16948238015174866]], [[-0.11993536353111267]], [[0.7938063740730286]], [[0.6723936200141907]], [[0.1380293369293213]], [[0.1486981362104416]], [[0.0009661862859502435]], [[2.203857660293579]], [[0.0]], [[-0.15767835080623627]], [[-0.1022254228591919]], [[-0.16948238015174866]], [[-0.11993536353111267]], [[0.7938065528869629]], [[0.6723937392234802]], [[0.13802941143512726]], [[0.19765207171440125]], [[2.2481560707092285]], [[0.0]], [[-0.15767835080623627]], [[-0.10222545266151428]], [[-0.1171264499425888]], [[0.8103176355361938]], [[0.6895219683647156]], [[0.16437183320522308]], [[0.19489677250385284]], [[2.3046839237213135]], [[0.0]], [[-0.15767835080623627]], [[-0.10938214510679245]], [[0.6822832822799683]], [[0.6484053730964661]], [[0.17117270827293396]], [[0.2005511224269867]], [[2.337982416152954]], [[0.0]], [[-0.15767835080623627]], [[0.7956335544586182]], [[0.29518163204193115]], [[0.1427011340856552]], [[0.14025256037712097]], [[2.372896432876587]], [[0.0]], [[-0.15767835080623627]], [[0.7956335544586182]], [[0.2992715537548065]], [[0.25868353247642517]], [[2.472592353820801]], [[0.0]], [[-0.15767833590507507]], [[-0.0920916497707367]], [[0.15681476891040802]], [[2.5294511318206787]], [[0.0]], [[-0.15767833590507507]], [[-0.05104731023311615]], [[2.4373557567596436]], [[0.0]], [[0.5707554817199707]], [[2.282710313796997]], [[0.0]], [[0.5707551836967468]], [[0.0]]], [[[0.4702589511871338]], [[2.031546115875244]], [[0.0]], [[0.47025859355926514]], [[0.0]]], [[[-0.16630077362060547]], [[0.2196761518716812]], [[1.8660138845443726]], [[0.0]], [[1.3569107055664062]], [[1.872368574142456]], [[0.0]], [[1.3569109439849854]], [[0.0]]], [[[-0.16990581154823303]], [[-0.0779690369963646]], [[-0.16932381689548492]], [[-0.16616521775722504]], [[-0.1320544183254242]], [[-0.1370353251695633]], [[-0.16773857176303864]], [[-0.15643741190433502]], [[-0.12096794694662094]], [[0.11902134865522385]], [[-0.13383635878562927]], [[0.4429062008857727]], [[-0.011489985510706902]], [[0.4968622922897339]], [[-0.014616276137530804]], [[0.4643462002277374]], [[-0.1699434071779251]], [[0.09605882316827774]], [[1.8450031280517578]], [[0.0]], [[-0.16990581154823303]], [[-0.0779690369963646]], [[-0.16932381689548492]], [[-0.16616521775722504]], [[-0.1320544183254242]], [[-0.1370353251695633]], [[-0.16773857176303864]], [[-0.15643741190433502]], [[-0.12096794694662094]], [[0.11902134865522385]], [[-0.13383635878562927]], [[0.4429062008857727]], [[-0.011489985510706902]], [[0.4968622922897339]], [[-0.014616276137530804]], [[-0.15815697610378265]], [[0.1836717426776886]], [[1.9827463626861572]], [[0.0]], [[-0.16990581154823303]], [[-0.0779690369963646]], [[-0.16932381689548492]], [[-0.16616521775722504]], [[-0.13311022520065308]], [[-0.1603800654411316]], [[-0.16780461370944977]], [[-0.138859361410141]], [[0.12533758580684662]], [[-0.13584376871585846]], [[0.4618333578109741]], [[-0.008148093707859516]], [[0.5333868861198425]], [[0.010244938544929028]], [[-0.15399588644504547]], [[0.21467220783233643]], [[2.0882010459899902]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236736536026]], [[-0.1423964649438858]], [[-0.13980481028556824]], [[-0.1336478292942047]], [[-0.16428467631340027]], [[-0.15426820516586304]], [[0.20298172533512115]], [[-0.1519526243209839]], [[0.5392541289329529]], [[0.03143183887004852]], [[0.5977840423583984]], [[0.055037952959537506]], [[-0.1341443955898285]], [[0.3649503290653229]], [[2.207960844039917]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[-0.14239642024040222]], [[-0.139804869890213]], [[-0.15908338129520416]], [[-0.16323086619377136]], [[0.20584474503993988]], [[-0.15594936907291412]], [[0.5603374242782593]], [[0.0659894272685051]], [[0.7771466970443726]], [[0.14895936846733093]], [[-0.10595687478780746]], [[0.5254269242286682]], [[2.330838441848755]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[-0.1575634628534317]], [[-0.0984400063753128]], [[-0.16982406377792358]], [[0.2268693596124649]], [[-0.16549617052078247]], [[0.5827506184577942]], [[0.08670647442340851]], [[0.7919712066650391]], [[0.19324277341365814]], [[-0.03096812777221203]], [[0.861373245716095]], [[2.440063714981079]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[-0.1575634628534317]], [[-0.0984400063753128]], [[-0.16982406377792358]], [[0.2268693596124649]], [[-0.16549617052078247]], [[0.5827506184577942]], [[1.339733600616455]], [[0.21636569499969482]], [[-0.005394178908318281]], [[0.7410441040992737]], [[2.545614004135132]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[-0.1575634628534317]], [[-0.0984400063753128]], [[-0.16982406377792358]], [[0.2268693596124649]], [[-0.16549617052078247]], [[0.5827506184577942]], [[0.08321990072727203]], [[-0.014591301791369915]], [[0.7261129021644592]], [[2.614713191986084]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[0.24512560665607452]], [[-0.1468462347984314]], [[0.23714040219783783]], [[-0.16574010252952576]], [[0.5874174237251282]], [[0.14120353758335114]], [[0.056606587022542953]], [[0.8936561942100525]], [[2.675149917602539]], [[0.0]], [[-0.16990581154823303]], [[-0.1670236587524414]], [[0.24512560665607452]], [[-0.1468462347984314]], [[-0.15506355464458466]], [[0.8057345747947693]], [[0.30035844445228577]], [[0.06786870211362839]], [[0.991864025592804]], [[2.776001453399658]], [[0.0]], [[-0.16990579664707184]], [[-0.1670236438512802]], [[0.24512550234794617]], [[-0.14684626460075378]], [[-0.15506355464458466]], [[0.41980665922164917]], [[0.0773807093501091]], [[0.9675185084342957]], [[2.903576135635376]], [[0.0]], [[-0.16990579664707184]], [[0.29576581716537476]], [[0.029684480279684067]], [[-0.15600469708442688]], [[0.4933212697505951]], [[0.20911580324172974]], [[1.0411416292190552]], [[2.995875597000122]], [[0.0]], [[-0.16990579664707184]], [[0.295766144990921]], [[0.029684770852327347]], [[0.3977184593677521]], [[0.16296082735061646]], [[0.7528174519538879]], [[3.028421640396118]], [[0.0]], [[-0.16990581154823303]], [[0.000725541147403419]], [[0.0994349867105484]], [[-0.020452389493584633]], [[1.7450923919677734]], [[3.0446090698242188]], [[0.0]], [[0.0230909064412117]], [[0.38193294405937195]], [[0.7001835107803345]], [[1.8977909088134766]], [[3.072834014892578]], [[0.0]], [[0.02309100329875946]], [[0.919848620891571]], [[1.8297384977340698]], [[2.9829163551330566]], [[0.0]], [[0.02309100329875946]], [[1.2259966135025024]], [[2.7918992042541504]], [[0.0]], [[0.02309100329875946]], [[1.7257428169250488]], [[0.0]], [[0.023091355338692665]], [[0.0]]], [[[0.3088855743408203]], [[1.0221601724624634]], [[1.7783520221710205]], [[0.0]], [[0.3088855743408203]], [[1.8770780563354492]], [[0.0]], [[0.30888476967811584]], [[0.0]]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's add our own text to text a few hypotheses. Add/ remove words. Replace words with similar words or opposites. \n",
        "\n",
        "Note: Some neurons perform multiple different functions, so your hypothesis might be {\"Harry Potter character names\" OR \"Repeated words\" OR \"these three punctuation marks after closing quotation marks\"}. This can be teased apart later when we see cross-neuron comparison (maybe this neuron and another do the Harry Potter characters?). Another source of info is the logit attribution part (see below)"
      ],
      "metadata": {
        "id": "r3zES3YY6CYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\n",
        "    \"1 2 3 4 5 6\",\n",
        "    \" 1 2 3 4 5 6\",\n",
        "    \"bacon & eggs\",\n",
        "    \" bacon & eggs\",\n",
        "]\n",
        "simplifier.text_to_visualize(text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "SRO7Qusg2Qnk",
        "outputId": "5cd95365-c8ae-45be-f691-82c112419f7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f05b0b5ee80>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-9c4956db-7162\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-9c4956db-7162\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"1\", \" 2\", \" 3\", \" 4\", \" 5\", \" 6\", \"\\n\", \" 1\", \" 2\", \" 3\", \" 4\", \" 5\", \" 6\", \"\\n\", \"b\", \"acon\", \" &\", \" eggs\", \"\\n\", \" bacon\", \" &\", \" eggs\", \"\\n\"], \"activations\": [[[-0.02827712893486023]], [[-0.09569506347179413]], [[-0.07182531803846359]], [[-0.08350041508674622]], [[-0.07855567336082458]], [[-0.09168858081102371]], [[0.0]], [[-0.07483179867267609]], [[-0.06570913642644882]], [[-0.04923362657427788]], [[-0.06580939888954163]], [[-0.06406431645154953]], [[-0.083592988550663]], [[0.0]], [[-0.025846807286143303]], [[-0.02707800641655922]], [[-0.08264916390180588]], [[-0.11014945805072784]], [[0.0]], [[-0.15760552883148193]], [[-0.06395194679498672]], [[-0.12413080036640167]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logit Attribution"
      ],
      "metadata": {
        "id": "Mq9J31Kf0kaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the row or column of MLP_out that is size d_model. Look through model.cfg & the shape of the model to find it. Multiply that row by the unembedding matrix"
      ],
      "metadata": {
        "id": "1qyWECz_0sTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model config is useful to look at for model shape info\n",
        "model.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rUDm1Zm1ZgU",
        "outputId": "28bdfaaf-2beb-4bb9-f827-a771114800e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HookedTransformerConfig:\n",
              "{'act_fn': 'gelu',\n",
              " 'attention_dir': 'causal',\n",
              " 'attn_only': False,\n",
              " 'attn_types': None,\n",
              " 'checkpoint_index': None,\n",
              " 'checkpoint_label_type': None,\n",
              " 'checkpoint_value': None,\n",
              " 'd_head': 64,\n",
              " 'd_mlp': 2048,\n",
              " 'd_model': 512,\n",
              " 'd_vocab': 50304,\n",
              " 'd_vocab_out': 50304,\n",
              " 'device': 'cuda',\n",
              " 'eps': 1e-05,\n",
              " 'final_rms': False,\n",
              " 'from_checkpoint': False,\n",
              " 'gated_mlp': False,\n",
              " 'init_mode': 'gpt2',\n",
              " 'init_weights': False,\n",
              " 'initializer_range': 0.035355339059327376,\n",
              " 'model_name': 'pythia-70m-deduped',\n",
              " 'n_ctx': 2048,\n",
              " 'n_devices': 1,\n",
              " 'n_heads': 8,\n",
              " 'n_layers': 6,\n",
              " 'n_params': 18874368,\n",
              " 'normalization_type': 'LNPre',\n",
              " 'original_architecture': 'GPTNeoXForCausalLM',\n",
              " 'parallel_attn_mlp': True,\n",
              " 'positional_embedding_type': 'rotary',\n",
              " 'rotary_dim': 16,\n",
              " 'scale_attn_by_inverse_layer_idx': False,\n",
              " 'seed': None,\n",
              " 'tokenizer_name': 'EleutherAI/pythia-70m-deduped',\n",
              " 'use_attn_result': False,\n",
              " 'use_attn_scale': True,\n",
              " 'use_hook_tokens': False,\n",
              " 'use_local_attn': False,\n",
              " 'use_split_qkv_input': False,\n",
              " 'window_size': None}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model has several easy ways to access the weights of the model, such as model.W_in, model.QK. We care about model.W.out, the second part of the MLP\n",
        "# The shape is [layer, d_mlp, d_model]\n",
        "model.W_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrgMnD7S7OqG",
        "outputId": "1e4b225a-0e81-4f73-8e2b-2edbcf5b60d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 2048, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unembeed\n",
        "model.W_U.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCaN2pfZ8RBB",
        "outputId": "9f8f0b6c-904d-4577-928d-506fbe5b73ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 50304])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = th.einsum('ij,jk->ik', model.W_out[-1], model.W_U)\n",
        "x.shape\n",
        "# x is the direct contribution of each neuron for all logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqenYDl2IOpl",
        "outputId": "3f51135c-9478-43c2-afc9-e0472c7a9991"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048, 50304])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I'm unsure if we need to multiply W_out by the layer norm or anything. This is just something I need to ask\n",
        "# To a first approximation, we multiply by the row in W_out with all of W_U"
      ],
      "metadata": {
        "id": "yNoDvmWY9B4g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### look at top neurons for a token"
      ],
      "metadata": {
        "id": "aVZdIPFYyn9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in range(10):\n",
        "    print(f\"token {model.to_string(token)}\\n\")\n",
        "    top_k_values, top_k_neurons = th.topk(x.T[token], k=10) # x.T[token] is the direct contribution of each neuron for the token\n",
        "    for neuron, value in zip(top_k_neurons, top_k_values):\n",
        "        print(f'\\tneuron {neuron}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG3RfRnSzx5j",
        "outputId": "a71a99f2-4c7a-43ae-ac97-f82d3ebd8927"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token <|endoftext|>\n",
            "\n",
            "\tneuron 1780: 93.394\n",
            "\tneuron 2003: 52.888\n",
            "\tneuron 312: 51.912\n",
            "\tneuron 1058: 44.889\n",
            "\tneuron 838: 39.027\n",
            "\tneuron 311: 37.896\n",
            "\tneuron 800: 37.724\n",
            "\tneuron 213: 35.560\n",
            "\tneuron 986: 33.824\n",
            "\tneuron 1921: 31.526\n",
            "\n",
            "\n",
            "token <|padding|>\n",
            "\n",
            "\tneuron 1780: 102.777\n",
            "\tneuron 2003: 58.307\n",
            "\tneuron 312: 56.966\n",
            "\tneuron 1058: 49.789\n",
            "\tneuron 838: 43.071\n",
            "\tneuron 311: 42.387\n",
            "\tneuron 800: 41.332\n",
            "\tneuron 213: 39.136\n",
            "\tneuron 986: 37.553\n",
            "\tneuron 1921: 34.836\n",
            "\n",
            "\n",
            "token !\n",
            "\n",
            "\tneuron 2034: 2.407\n",
            "\tneuron 667: 1.732\n",
            "\tneuron 392: 1.614\n",
            "\tneuron 518: 1.392\n",
            "\tneuron 379: 1.368\n",
            "\tneuron 388: 1.126\n",
            "\tneuron 745: 1.064\n",
            "\tneuron 1537: 1.031\n",
            "\tneuron 422: 1.011\n",
            "\tneuron 1359: 0.996\n",
            "\n",
            "\n",
            "token \"\n",
            "\n",
            "\tneuron 208: 1.840\n",
            "\tneuron 1003: 1.641\n",
            "\tneuron 1850: 1.579\n",
            "\tneuron 887: 1.484\n",
            "\tneuron 519: 1.353\n",
            "\tneuron 2034: 1.303\n",
            "\tneuron 1811: 1.209\n",
            "\tneuron 1746: 1.036\n",
            "\tneuron 1652: 1.006\n",
            "\tneuron 1675: 0.989\n",
            "\n",
            "\n",
            "token #\n",
            "\n",
            "\tneuron 492: 1.309\n",
            "\tneuron 1934: 1.280\n",
            "\tneuron 502: 1.268\n",
            "\tneuron 740: 1.181\n",
            "\tneuron 1974: 1.118\n",
            "\tneuron 519: 1.105\n",
            "\tneuron 150: 1.041\n",
            "\tneuron 411: 1.006\n",
            "\tneuron 61: 0.930\n",
            "\tneuron 1943: 0.912\n",
            "\n",
            "\n",
            "token $\n",
            "\n",
            "\tneuron 422: 1.173\n",
            "\tneuron 1811: 1.151\n",
            "\tneuron 780: 1.036\n",
            "\tneuron 211: 0.987\n",
            "\tneuron 1589: 0.929\n",
            "\tneuron 844: 0.882\n",
            "\tneuron 268: 0.856\n",
            "\tneuron 1014: 0.831\n",
            "\tneuron 1823: 0.824\n",
            "\tneuron 519: 0.821\n",
            "\n",
            "\n",
            "token %\n",
            "\n",
            "\tneuron 902: 1.085\n",
            "\tneuron 1495: 1.064\n",
            "\tneuron 1329: 1.025\n",
            "\tneuron 691: 1.011\n",
            "\tneuron 1987: 0.969\n",
            "\tneuron 680: 0.968\n",
            "\tneuron 1134: 0.967\n",
            "\tneuron 544: 0.890\n",
            "\tneuron 1718: 0.860\n",
            "\tneuron 502: 0.846\n",
            "\n",
            "\n",
            "token &\n",
            "\n",
            "\tneuron 438: 1.652\n",
            "\tneuron 1115: 1.302\n",
            "\tneuron 1139: 1.204\n",
            "\tneuron 502: 1.190\n",
            "\tneuron 619: 1.106\n",
            "\tneuron 1974: 1.017\n",
            "\tneuron 120: 0.986\n",
            "\tneuron 1598: 0.969\n",
            "\tneuron 887: 0.928\n",
            "\tneuron 1832: 0.898\n",
            "\n",
            "\n",
            "token '\n",
            "\n",
            "\tneuron 366: 5.550\n",
            "\tneuron 515: 4.005\n",
            "\tneuron 1598: 2.895\n",
            "\tneuron 887: 2.423\n",
            "\tneuron 1850: 1.757\n",
            "\tneuron 1652: 1.675\n",
            "\tneuron 929: 1.583\n",
            "\tneuron 1003: 1.504\n",
            "\tneuron 208: 1.436\n",
            "\tneuron 1746: 0.965\n",
            "\n",
            "\n",
            "token (\n",
            "\n",
            "\tneuron 1299: 2.010\n",
            "\tneuron 1942: 1.931\n",
            "\tneuron 2034: 1.398\n",
            "\tneuron 1902: 1.244\n",
            "\tneuron 994: 0.972\n",
            "\tneuron 309: 0.952\n",
            "\tneuron 1221: 0.935\n",
            "\tneuron 1179: 0.935\n",
            "\tneuron 519: 0.933\n",
            "\tneuron 353: 0.914\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### look at top tokens for a neuron"
      ],
      "metadata": {
        "id": "wzrqJh8zy5Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for neuron in range(10):\n",
        "    print(f\"neuron {neuron}\\n\")\n",
        "    top_k_values, top_k_tokens = th.topk(x[neuron], k=10) # x[neuron] is the direct contribution of the neuron to each token\n",
        "    for token, value in zip(top_k_tokens, top_k_values):\n",
        "        print(f'\\ttoken {model.to_string(token)}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD2VnXnJpFen",
        "outputId": "6d2c2294-1101-4da7-85ad-4d89c57bd20b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neuron 0\n",
            "\n",
            "\ttoken \"}](#: 1.752\n",
            "\ttoken arts: 1.544\n",
            "\ttoken law: 1.515\n",
            "\ttoken ose: 1.508\n",
            "\ttoken  sometime: 1.491\n",
            "\ttoken ethanol: 1.471\n",
            "\ttoken _.\": 1.435\n",
            "\ttoken _,: 1.388\n",
            "\ttoken substack: 1.362\n",
            "\ttoken  those: 1.360\n",
            "\n",
            "\n",
            "neuron 1\n",
            "\n",
            "\ttoken ,: 2.561\n",
            "\ttoken ,...: 2.483\n",
            "\ttoken ��: 2.434\n",
            "\ttoken �: 2.376\n",
            "\ttoken �: 2.337\n",
            "\ttoken ,\\: 2.302\n",
            "\ttoken ��: 2.289\n",
            "\ttoken �: 2.239\n",
            "\ttoken ,*: 2.189\n",
            "\ttoken �: 2.124\n",
            "\n",
            "\n",
            "neuron 2\n",
            "\n",
            "\ttoken \n",
            "\n",
            "               : 7.947\n",
            "\ttoken                          : 7.947\n",
            "\ttoken               : 7.947\n",
            "\ttoken \n",
            "                           : 7.947\n",
            "\ttoken : 7.947\n",
            "\ttoken  \n",
            "     : 7.947\n",
            "\ttoken \n",
            "\n",
            "                    : 7.947\n",
            "\ttoken                                        : 7.947\n",
            "\ttoken \n",
            "\n",
            "                                : 7.947\n",
            "\ttoken \n",
            "                                : 7.947\n",
            "\n",
            "\n",
            "neuron 3\n",
            "\n",
            "\ttoken  signatures: 1.286\n",
            "\ttoken uit: 1.261\n",
            "\ttoken inqu: 1.178\n",
            "\ttoken ley: 1.169\n",
            "\ttoken  reads: 1.145\n",
            "\ttoken enna: 1.142\n",
            "\ttoken  thereof: 1.141\n",
            "\ttoken Affirmed: 1.129\n",
            "\ttoken itals: 1.127\n",
            "\ttoken inet: 1.125\n",
            "\n",
            "\n",
            "neuron 4\n",
            "\n",
            "\ttoken  rate: 1.050\n",
            "\ttoken  tissues: 0.990\n",
            "\ttoken  perception: 0.985\n",
            "\ttoken  cycling: 0.961\n",
            "\ttoken  organ: 0.943\n",
            "\ttoken  sensations: 0.933\n",
            "\ttoken  sensation: 0.911\n",
            "\ttoken  tolerance: 0.908\n",
            "\ttoken  sensed: 0.906\n",
            "\ttoken  tissue: 0.903\n",
            "\n",
            "\n",
            "neuron 5\n",
            "\n",
            "\ttoken )):: 0.691\n",
            "\ttoken ))).: 0.649\n",
            "\ttoken  it: 0.628\n",
            "\ttoken )\\]: 0.628\n",
            "\ttoken  */: 0.625\n",
            "\ttoken )\\].: 0.623\n",
            "\ttoken )].: 0.622\n",
            "\ttoken 't: 0.618\n",
            "\ttoken  well: 0.599\n",
            "\ttoken \":[: 0.598\n",
            "\n",
            "\n",
            "neuron 6\n",
            "\n",
            "\ttoken  -,: 1.186\n",
            "\ttoken  nails: 1.094\n",
            "\ttoken  endl: 1.083\n",
            "\ttoken  mechanics: 1.072\n",
            "\ttoken  taut: 1.069\n",
            "\ttoken  typ: 1.066\n",
            "\ttoken  ...,: 1.017\n",
            "\ttoken  erect: 1.000\n",
            "\ttoken  processors: 0.977\n",
            "\ttoken  epit: 0.974\n",
            "\n",
            "\n",
            "neuron 7\n",
            "\n",
            "\ttoken urs: 1.170\n",
            "\ttoken curs: 1.159\n",
            "\ttoken TRY: 1.141\n",
            "\ttoken WORD: 1.062\n",
            "\ttoken iset: 1.060\n",
            "\ttoken  decre: 1.059\n",
            "\ttoken  (;: 1.055\n",
            "\ttoken unct: 1.034\n",
            "\ttoken join: 1.030\n",
            "\ttoken rase: 1.027\n",
            "\n",
            "\n",
            "neuron 8\n",
            "\n",
            "\ttoken ��: 5.436\n",
            "\ttoken                                      : 4.924\n",
            "\ttoken                                   : 4.924\n",
            "\ttoken                                           : 4.924\n",
            "\ttoken                                                                                                                                                                                                                                                                 : 4.924\n",
            "\ttoken \r\n",
            "\r\n",
            "           : 4.924\n",
            "\ttoken           : 4.924\n",
            "\ttoken                                                                                : 4.924\n",
            "\ttoken \n",
            "         : 4.924\n",
            "\ttoken                                                               : 4.924\n",
            "\n",
            "\n",
            "neuron 9\n",
            "\n",
            "\ttoken mere: 1.541\n",
            "\ttoken hip: 1.360\n",
            "\ttoken vim: 1.322\n",
            "\ttoken rey: 1.284\n",
            "\ttoken vist: 1.277\n",
            "\ttoken hest: 1.270\n",
            "\ttoken ****************************************************************************: 1.253\n",
            "\ttoken shot: 1.235\n",
            "\ttoken nas: 1.235\n",
            "\ttoken  analyzer: 1.204\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### take a look at the top neurons for a token and then look at their top tokens"
      ],
      "metadata": {
        "id": "cLWHlB89kjhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = 26788\n",
        "print(f\"token {model.to_string(token)}\\n\")\n",
        "top_k_values, top_k_neurons = th.topk(x.T[token], k=30) # x.T[token] is the direct contribution of each neuron for the token\n",
        "for neuron, value in zip(top_k_neurons, top_k_values):\n",
        "    print(f'\\tneuron {neuron}: {value:.3f}\\n')\n",
        "    top_k_values, top_k_tokens = th.topk(x[neuron], k=10) # x[neuron] is the direct contribution of the neuron to each token\n",
        "    for token, value in zip(top_k_tokens, top_k_values):\n",
        "        print(f'\\t\\ttoken {model.to_string(token)}: {value:.3f}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj2z4yZbg6_b",
        "outputId": "f2d826ac-a069-4075-81f3-a925b06d8bf0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token Monday\n",
            "\n",
            "\tneuron 428: 0.963\n",
            "\n",
            "\t\ttoken : 5.952\n",
            "\t\ttoken \n",
            "\n",
            "                   : 5.952\n",
            "\t\ttoken  \n",
            "   : 5.952\n",
            "\t\ttoken \n",
            "  : 5.952\n",
            "\t\ttoken \n",
            "\n",
            "               : 5.952\n",
            "\t\ttoken                    : 5.952\n",
            "\t\ttoken          : 5.952\n",
            "\t\ttoken                        : 5.952\n",
            "\t\ttoken                                                            : 5.952\n",
            "\t\ttoken                                                                        : 5.952\n",
            "\n",
            "\n",
            "\tneuron 519: 0.948\n",
            "\n",
            "\t\ttoken  ]{}: 2.215\n",
            "\t\ttoken \\): 2.123\n",
            "\t\ttoken \"]): 1.950\n",
            "\t\ttoken ]{.: 1.944\n",
            "\t\ttoken Figure: 1.919\n",
            "\t\ttoken )](#: 1.907\n",
            "\t\ttoken  ](: 1.843\n",
            "\t\ttoken _**: 1.809\n",
            "\t\ttoken \")): 1.802\n",
            "\t\ttoken \")]: 1.797\n",
            "\n",
            "\n",
            "\tneuron 1301: 0.906\n",
            "\n",
            "\t\ttoken  three: 1.455\n",
            "\t\ttoken  two: 1.454\n",
            "\t\ttoken  3: 1.267\n",
            "\t\ttoken  2: 1.264\n",
            "\t\ttoken  five: 1.240\n",
            "\t\ttoken  four: 1.234\n",
            "\t\ttoken  seven: 1.194\n",
            "\t\ttoken  4: 1.178\n",
            "\t\ttoken  six: 1.131\n",
            "\t\ttoken  nine: 1.125\n",
            "\n",
            "\n",
            "\tneuron 1913: 0.850\n",
            "\n",
            "\t\ttoken  Mae: 1.304\n",
            "\t\ttoken boys: 1.290\n",
            "\t\ttoken  Fre: 1.278\n",
            "\t\ttoken  tired: 1.271\n",
            "\t\ttoken  Aber: 1.257\n",
            "\t\ttoken  Cho: 1.232\n",
            "\t\ttoken bsite: 1.219\n",
            "\t\ttoken  sleep: 1.217\n",
            "\t\ttoken ATH: 1.188\n",
            "\t\ttoken meal: 1.179\n",
            "\n",
            "\n",
            "\tneuron 1018: 0.849\n",
            "\n",
            "\t\ttoken                     : 8.830\n",
            "\t\ttoken \n",
            "          : 8.830\n",
            "\t\ttoken \n",
            "   : 8.830\n",
            "\t\ttoken    \n",
            ": 8.830\n",
            "\t\ttoken \r\n",
            "       : 8.830\n",
            "\t\ttoken                            : 8.830\n",
            "\t\ttoken \n",
            "\n",
            "                           : 8.830\n",
            "\t\ttoken : 8.830\n",
            "\t\ttoken                                                                                                                                                                                                                                 : 8.830\n",
            "\t\ttoken                                 : 8.830\n",
            "\n",
            "\n",
            "\tneuron 1900: 0.822\n",
            "\n",
            "\t\ttoken oons: 1.322\n",
            "\t\ttoken ine: 1.293\n",
            "\t\ttoken book: 1.267\n",
            "\t\ttoken itate: 1.219\n",
            "\t\ttoken  scale: 1.157\n",
            "\t\ttoken  scales: 1.151\n",
            "\t\ttoken oon: 1.142\n",
            "\t\ttoken kim: 1.138\n",
            "\t\ttoken  Schedule: 1.128\n",
            "\t\ttoken ingo: 1.126\n",
            "\n",
            "\n",
            "\tneuron 1869: 0.816\n",
            "\n",
            "\t\ttoken yll: 1.428\n",
            "\t\ttoken True: 1.290\n",
            "\t\ttoken ington: 1.131\n",
            "\t\ttoken bred: 1.094\n",
            "\t\ttoken York: 1.061\n",
            "\t\ttoken heit: 1.058\n",
            "\t\ttoken ujah: 1.055\n",
            "\t\ttoken horse: 1.041\n",
            "\t\ttoken uments: 1.039\n",
            "\t\ttoken Williams: 1.034\n",
            "\n",
            "\n",
            "\tneuron 121: 0.795\n",
            "\n",
            "\t\ttoken ��: 3.192\n",
            "\t\ttoken �: 3.085\n",
            "\t\ttoken �: 3.036\n",
            "\t\ttoken   \n",
            " : 2.837\n",
            "\t\ttoken : 2.832\n",
            "\t\ttoken \n",
            "        \n",
            "       : 2.832\n",
            "\t\ttoken : 2.832\n",
            "\t\ttoken \n",
            "\n",
            "      : 2.832\n",
            "\t\ttoken \n",
            "                              : 2.832\n",
            "\t\ttoken \n",
            "\n",
            "            : 2.832\n",
            "\n",
            "\n",
            "\tneuron 1394: 0.774\n",
            "\n",
            "\t\ttoken arial: 1.234\n",
            "\t\ttoken atics: 1.136\n",
            "\t\ttoken coding: 1.085\n",
            "\t\ttoken  script: 1.085\n",
            "\t\ttoken  Rangers: 1.079\n",
            "\t\ttoken  Jets: 1.068\n",
            "\t\ttoken ivid: 1.066\n",
            "\t\ttoken  Springs: 1.032\n",
            "\t\ttoken  hopes: 1.018\n",
            "\t\ttoken PING: 1.009\n",
            "\n",
            "\n",
            "\tneuron 887: 0.750\n",
            "\n",
            "\t\ttoken '\": 2.566\n",
            "\t\ttoken '\\: 2.454\n",
            "\t\ttoken ': 2.423\n",
            "\t\ttoken \\\": 2.351\n",
            "\t\ttoken '_: 2.234\n",
            "\t\ttoken ``: 2.152\n",
            "\t\ttoken \"': 2.105\n",
            "\t\ttoken '': 2.053\n",
            "\t\ttoken “: 2.011\n",
            "\t\ttoken \\\":: 1.957\n",
            "\n",
            "\n",
            "\tneuron 1226: 0.745\n",
            "\n",
            "\t\ttoken  one: 4.706\n",
            "\t\ttoken  ones: 3.253\n",
            "\t\ttoken  One: 2.969\n",
            "\t\ttoken one: 2.718\n",
            "\t\ttoken  ONE: 2.432\n",
            "\t\ttoken One: 2.045\n",
            "\t\ttoken ONE: 1.985\n",
            "\t\ttoken ones: 1.786\n",
            "\t\ttoken two: 1.744\n",
            "\t\ttoken  two: 1.696\n",
            "\n",
            "\n",
            "\tneuron 492: 0.730\n",
            "\n",
            "\t\ttoken #: 1.309\n",
            "\t\ttoken           : 1.291\n",
            "\t\ttoken ophone: 1.224\n",
            "\t\ttoken               : 1.217\n",
            "\t\ttoken                   : 1.195\n",
            "\t\ttoken >\",: 1.187\n",
            "\t\ttoken          : 1.180\n",
            "\t\ttoken              : 1.171\n",
            "\t\ttoken iem: 1.139\n",
            "\t\ttoken  embodiment: 1.135\n",
            "\n",
            "\n",
            "\tneuron 19: 0.729\n",
            "\n",
            "\t\ttoken �: 2.310\n",
            "\t\ttoken �: 2.069\n",
            "\t\ttoken �: 2.036\n",
            "\t\ttoken �: 2.012\n",
            "\t\ttoken �: 1.980\n",
            "\t\ttoken �: 1.924\n",
            "\t\ttoken �: 1.902\n",
            "\t\ttoken �: 1.890\n",
            "\t\ttoken �: 1.882\n",
            "\t\ttoken �: 1.875\n",
            "\n",
            "\n",
            "\tneuron 1728: 0.724\n",
            "\n",
            "\t\ttoken wed: 1.310\n",
            "\t\ttoken \\#: 1.266\n",
            "\t\ttoken edly: 1.225\n",
            "\t\ttoken oting: 1.207\n",
            "\t\ttoken hus: 1.205\n",
            "\t\ttoken  offering: 1.204\n",
            "\t\ttoken  threats: 1.203\n",
            "\t\ttoken  plate: 1.179\n",
            "\t\ttoken po: 1.178\n",
            "\t\ttoken  promise: 1.153\n",
            "\n",
            "\n",
            "\tneuron 1222: 0.710\n",
            "\n",
            "\t\ttoken ��: 5.030\n",
            "\t\ttoken                                                                                : 5.015\n",
            "\t\ttoken \n",
            "\n",
            "                   : 5.015\n",
            "\t\ttoken                                                                                                                                                                                                                                                                 : 5.015\n",
            "\t\ttoken                          : 5.015\n",
            "\t\ttoken \n",
            "                       : 5.015\n",
            "\t\ttoken \n",
            "                              : 5.015\n",
            "\t\ttoken \n",
            "       : 5.015\n",
            "\t\ttoken \n",
            "                                                 : 5.015\n",
            "\t\ttoken \n",
            "                                   : 5.015\n",
            "\n",
            "\n",
            "\tneuron 122: 0.707\n",
            "\n",
            "\t\ttoken \n",
            ": 4.811\n",
            "\t\ttoken \n",
            "\f: 4.138\n",
            "\t\ttoken \n",
            "\n",
            ": 3.921\n",
            "\t\ttoken \n",
            "\n",
            "\n",
            "\n",
            ": 3.901\n",
            "\t\ttoken \n",
            "\t\n",
            ": 3.844\n",
            "\t\ttoken \n",
            "\n",
            "\n",
            ": 3.809\n",
            "\t\ttoken \n",
            ": 3.661\n",
            "\t\ttoken \n",
            "\t: 3.566\n",
            ": 3.533\n",
            "\t\ttoken  \n",
            ": 3.527\n",
            "\n",
            "\n",
            "\tneuron 353: 0.702\n",
            "\n",
            "\t\ttoken “: 1.315\n",
            "\t\ttoken hab: 1.228\n",
            "\t\ttoken in: 1.202\n",
            "\t\ttoken finding: 1.182\n",
            "\t\ttoken «: 1.171\n",
            "\t\ttoken “[: 1.167\n",
            "\t\ttoken license: 1.162\n",
            "\t\ttoken en: 1.161\n",
            "\t\ttoken brew: 1.131\n",
            "\t\ttoken sl: 1.131\n",
            "\n",
            "\n",
            "\tneuron 1285: 0.700\n",
            "\n",
            "\t\ttoken �: 1.688\n",
            "\t\ttoken �: 1.630\n",
            "\t\ttoken �: 1.498\n",
            "\t\ttoken apine: 1.482\n",
            "\t\ttoken �: 1.467\n",
            "\t\ttoken �: 1.435\n",
            "\t\ttoken  neutrality: 1.400\n",
            "\t\ttoken �: 1.393\n",
            "\t\ttoken ��: 1.380\n",
            "\t\ttoken  relief: 1.376\n",
            "\n",
            "\n",
            "\tneuron 738: 0.698\n",
            "\n",
            "\t\ttoken                                                                                                                                                                                                                                 : 4.069\n",
            "\t\ttoken \n",
            "\n",
            "             : 4.068\n",
            "\t\ttoken \n",
            "        : 4.068\n",
            "\t\ttoken : 4.068\n",
            "\t\ttoken                                                             : 4.068\n",
            "\t\ttoken \n",
            "\n",
            "               : 4.068\n",
            "\t\ttoken �: 4.068\n",
            "\t\ttoken                                                       : 4.068\n",
            "\t\ttoken \n",
            "\n",
            "     : 4.068\n",
            "\t\ttoken \n",
            "                                                 : 4.068\n",
            "\n",
            "\n",
            "\tneuron 1896: 0.697\n",
            "\n",
            "\t\ttoken ties: 1.408\n",
            "\t\ttoken ty: 1.378\n",
            "\t\ttoken ite: 1.367\n",
            "\t\ttoken ieri: 1.333\n",
            "\t\ttoken sha: 1.233\n",
            "\t\ttoken  cater: 1.206\n",
            "\t\ttoken  centres: 1.202\n",
            "\t\ttoken  Tehran: 1.181\n",
            "\t\ttoken ta: 1.180\n",
            "\t\ttoken  centers: 1.175\n",
            "\n",
            "\n",
            "\tneuron 184: 0.685\n",
            "\n",
            "\t\ttoken medsc: 2.298\n",
            "\t\ttoken ��: 2.193\n",
            "\t\ttoken PtrFromString: 2.166\n",
            "\t\ttoken NFTA: 2.117\n",
            "\t\ttoken �: 1.980\n",
            "\t\ttoken ��: 1.919\n",
            "\t\ttoken ��: 1.913\n",
            "\t\ttoken ��: 1.908\n",
            "\t\ttoken �: 1.885\n",
            "\t\ttoken �: 1.843\n",
            "\n",
            "\n",
            "\tneuron 1957: 0.679\n",
            "\n",
            "\t\ttoken m: 1.415\n",
            "\t\ttoken min: 1.393\n",
            "\t\ttoken mage: 1.361\n",
            "\t\ttoken amsbsy: 1.348\n",
            "\t\ttoken rpm: 1.344\n",
            "\t\ttoken ops: 1.300\n",
            "\t\ttoken documentclass: 1.289\n",
            "\t\ttoken course: 1.278\n",
            "\t\ttoken ms: 1.227\n",
            "\t\ttoken channel: 1.209\n",
            "\n",
            "\n",
            "\tneuron 1004: 0.669\n",
            "\n",
            "\t\ttoken History: 0.919\n",
            "\t\ttoken Education: 0.917\n",
            "\t\ttoken External: 0.914\n",
            "\t\ttoken Q: 0.888\n",
            "\t\ttoken Background: 0.883\n",
            "\t\ttoken Polit: 0.878\n",
            "\t\ttoken As: 0.875\n",
            "\t\ttoken Geography: 0.864\n",
            "\t\ttoken ADVERTISEMENT: 0.864\n",
            "\t\ttoken At: 0.862\n",
            "\n",
            "\n",
            "\tneuron 1412: 0.668\n",
            "\n",
            "\t\ttoken �: 1.302\n",
            "\t\ttoken  sheet: 1.197\n",
            "\t\ttoken �: 1.158\n",
            "\t\ttoken  suppose: 1.144\n",
            "\t\ttoken lar: 1.115\n",
            "\t\ttoken scribe: 1.103\n",
            "\t\ttoken �: 1.085\n",
            "\t\ttoken quito: 1.083\n",
            "\t\ttoken ktiv: 1.066\n",
            "\t\ttoken �: 1.051\n",
            "\n",
            "\n",
            "\tneuron 136: 0.668\n",
            "\n",
            "\t\ttoken \n",
            "\n",
            "                           : 6.117\n",
            "\t\ttoken                                    : 6.117\n",
            "\t\ttoken                                                          : 6.117\n",
            "\t\ttoken \n",
            "                       : 6.117\n",
            "\t\ttoken \n",
            "\n",
            "            : 6.117\n",
            "\t\ttoken                   : 6.117\n",
            "\t\ttoken \n",
            "               : 6.117\n",
            "\t\ttoken                                                           : 6.117\n",
            "\t\ttoken                                                                       : 6.117\n",
            "\t\ttoken                                              : 6.117\n",
            "\n",
            "\n",
            "\tneuron 1041: 0.661\n",
            "\n",
            "\t\ttoken pal: 1.466\n",
            "\t\ttoken sein: 1.378\n",
            "\t\ttoken station: 1.295\n",
            "\t\ttoken escence: 1.249\n",
            "\t\ttoken  trial: 1.217\n",
            "\t\ttoken ť: 1.145\n",
            "\t\ttoken trial: 1.144\n",
            "\t\ttoken vd: 1.139\n",
            "\t\ttoken rine: 1.120\n",
            "\t\ttoken assic: 1.114\n",
            "\n",
            "\n",
            "\tneuron 273: 0.645\n",
            "\n",
            "\t\ttoken  by: 1.180\n",
            "\t\ttoken burg: 1.161\n",
            "\t\ttoken ======: 1.041\n",
            "\t\ttoken <?: 1.035\n",
            "\t\ttoken by: 1.016\n",
            "\t\ttoken AndroidRuntime: 1.015\n",
            "\t\ttoken Д: 1.014\n",
            "\t\ttoken  from: 1.008\n",
            "\t\ttoken  thanks: 0.992\n",
            "\t\ttoken from: 0.977\n",
            "\n",
            "\n",
            "\tneuron 570: 0.639\n",
            "\n",
            "\t\ttoken cession: 1.316\n",
            "\t\ttoken mighty: 1.270\n",
            "\t\ttoken ificial: 1.206\n",
            "\t\ttoken hift: 1.200\n",
            "\t\ttoken culus: 1.177\n",
            "\t\ttoken erate: 1.169\n",
            "\t\ttoken iors: 1.163\n",
            "\t\ttoken  delivered: 1.145\n",
            "\t\ttoken avirus: 1.132\n",
            "\t\ttoken fare: 1.121\n",
            "\n",
            "\n",
            "\tneuron 531: 0.637\n",
            "\n",
            "\t\ttoken  in: 4.068\n",
            "\t\ttoken  In: 2.449\n",
            "\t\ttoken  therein: 2.396\n",
            "\t\ttoken In: 2.240\n",
            "\t\ttoken �: 1.953\n",
            "\t\ttoken �: 1.927\n",
            "\t\ttoken �: 1.884\n",
            "\t\ttoken �: 1.835\n",
            "\t\ttoken �: 1.783\n",
            "\t\ttoken  prominently: 1.750\n",
            "\n",
            "\n",
            "\tneuron 1479: 0.635\n",
            "\n",
            "\t\ttoken 13: 1.392\n",
            "\t\ttoken 11: 1.373\n",
            "\t\ttoken 14: 1.319\n",
            "\t\ttoken 17: 1.281\n",
            "\t\ttoken 12: 1.259\n",
            "\t\ttoken 18: 1.252\n",
            "\t\ttoken 15: 1.244\n",
            "\t\ttoken 10: 1.235\n",
            "\t\ttoken 16: 1.230\n",
            "\t\ttoken suppl: 1.199\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at a specific neuron, it’s top tokens and top activating examples, and see if it makes sense\n",
        "\n",
        "Like maybe the token is \" 5\" and the example is \" 1 2 3 4\". We can guess it's encoding the information about the sequence. So you can remove parts of the context to \"bob apple 4\" and see if the neuron is still activating.\n",
        "\n",
        "check \"visualize top examples\" part to get too activating examples for a specific neuron"
      ],
      "metadata": {
        "id": "ETM8OeTGwc_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron = 1479"
      ],
      "metadata": {
        "id": "uzxvcN_CvZFr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "looks at top tokens for this neuron"
      ],
      "metadata": {
        "id": "wbP-cTJmvbIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_values, top_k_tokens = th.topk(x[neuron], k=30) # x[neuron] is the direct contribution of the neuron to each token\n",
        "for token, value in zip(top_k_tokens, top_k_values):\n",
        "    print(f'\\ttoken {model.to_string(token)}: {value:.3f}')"
      ],
      "metadata": {
        "id": "cNEMHdgdvAiE",
        "outputId": "a5c9b79a-8ca7-40a3-cfec-9106f6651ecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ttoken 13: 1.392\n",
            "\ttoken 11: 1.373\n",
            "\ttoken 14: 1.319\n",
            "\ttoken 17: 1.281\n",
            "\ttoken 12: 1.259\n",
            "\ttoken 18: 1.252\n",
            "\ttoken 15: 1.244\n",
            "\ttoken 10: 1.235\n",
            "\ttoken 16: 1.230\n",
            "\ttoken suppl: 1.199\n",
            "\ttoken 22: 1.197\n",
            "\ttoken 19: 1.137\n",
            "\ttoken olk: 1.123\n",
            "\ttoken 26: 1.116\n",
            "\ttoken 20: 1.114\n",
            "\ttoken \f: 1.105\n",
            "\ttoken  forth: 1.102\n",
            "\ttoken )$-: 1.100\n",
            "\ttoken 23: 1.083\n",
            "\ttoken iqu: 1.083\n",
            "\ttoken 21: 1.071\n",
            "\ttoken 28: 1.066\n",
            "\ttoken 29: 1.057\n",
            "\ttoken  Studies: 1.054\n",
            "\ttoken issue: 1.047\n",
            "\ttoken  Archives: 1.039\n",
            "\ttoken  studies: 1.038\n",
            "\ttoken  experimentation: 1.038\n",
            "\ttoken 6: 1.029\n",
            "\ttoken 7: 1.021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at top activating examples"
      ],
      "metadata": {
        "id": "ZfcpAEZgvjQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 30\n",
        "simplifier = NeuronTextSimplifier(model, layer, neuron)\n",
        "\n",
        "values, indices = neuron_activations[:,neuron].topk(k)\n",
        "\n",
        "max_datapoints = [np.unravel_index(i, (datapoints, Token_amount)) for i in indices]\n",
        "\n",
        "text_list = []\n",
        "full_text = []\n",
        "for md, s_ind in max_datapoints:\n",
        "    md = int(md)\n",
        "    s_ind = int(s_ind)\n",
        "    # Get the full text\n",
        "    full_tok = th.tensor(d[md][\"input_ids\"])\n",
        "    full_text.append(model.tokenizer.decode(full_tok))\n",
        "    \n",
        "    # Get just the text up until the max-activating example\n",
        "    tok = d[md][\"input_ids\"][:s_ind+1]\n",
        "    text = model.tokenizer.decode(tok)\n",
        "    text_list.append(text)"
      ],
      "metadata": {
        "id": "TC-xNgUNrFeh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplifier.text_to_visualize(full_text)"
      ],
      "metadata": {
        "id": "DjFRCTm-rK7m",
        "outputId": "83b5e55e-1ac8-4673-cff2-86c0f76b4f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x7f05b0b7b160>"
            ],
            "text/html": [
              "<div id=\"circuits-vis-80d02f58-13c7\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-80d02f58-13c7\",\n",
              "      TextNeuronActivations,\n",
              "      {\"tokens\": [\"It\", \"'s\", \" weakening\", \" as\", \" it\", \" moves\", \" north\", \"ward\", \" and\", \" away\", \" from\", \" the\", \" U\", \".\", \"S\", \".\", \" Gulf\", \" Coast\", \".\", \"\\\\newline\", \"\\n\", \"H\", \"at\", \"am\", \"abad\", \",\", \" Mark\", \"azi\", \"\\\\newline\", \"\\\\newline\", \"H\", \"at\", \"am\", \"abad\", \" (,\", \" also\", \" Roman\", \"ized\", \" as\", \" \\ufffd\", \"\\ufffd\\ufffd\", \"\\n\", \"A\", \" driving\", \" mechanism\", \" designed\", \" to\", \" drive\", \" in\", \" an\", \" upward\", \"-\", \"down\", \"ward\", \" direction\", \" a\", \" suction\", \" nozzle\", \" for\", \" suction\", \"-\", \"holding\", \"\\n\", \"Hi\", \"rom\", \"ichi\", \"\\\\newline\", \"\\\\newline\", \"Hi\", \"rom\", \"ichi\", \" (\", \"written\", \":\", \" \", \"\\ufffd\", \"\\ufffd\", \"\\ufffd\", \"\\u901a\", \",\", \" \", \"\\ufffd\", \"\\ufffd\", \"\\n\", \"Description\", \"\\\\newline\", \"\\\\newline\", \"F\", \"lesh\", \"light\", \" is\", \" proud\", \" to\", \" announce\", \" Dor\", \"cel\", \" Girl\", \",\", \" Valent\", \"ina\", \" N\", \"app\", \"i\", \" with\", \"\\n\", \"     \", \"Case\", \":\", \" 16\", \"-\", \"205\", \"46\", \"   \", \"Document\", \":\", \" \", \"005\", \"14\", \"30\", \"66\", \"29\", \"    \", \"Page\", \":\", \" 1\", \"\\n\", \"     \", \"Case\", \":\", \" 11\", \"-\", \"30\", \"209\", \"     \", \"Document\", \":\", \" \", \"005\", \"118\", \"90\", \"671\", \"         \", \"Page\", \":\", \" 1\", \"     \", \"\\n\", \"?\", \"\\\\newline\", \"-----\", \" Original\", \" Message\", \" -----\", \"  \", \"\\\\newline\", \"From\", \":\", \" E\", \"wing\", \",\", \" John\", \" \", \"\\\\newline\", \"To\", \":\", \" Lynn\", \" Br\", \"\\n\", \"     \", \"Case\", \":\", \" 11\", \"-\", \"407\", \"34\", \"     \", \"Document\", \":\", \" \", \"005\", \"116\", \"99\", \"683\", \"         \", \"Page\", \":\", \" 1\", \"     \", \"\\n\", \" \", \"\\\\newline\", \"#\", \" C\", \"urse\", \" of\", \" the\", \" Wolf\", \" Girl\", \"\\\\newline\", \"\\\\newline\", \"##\", \" Martin\", \" Mill\", \"ar\", \"\\\\newline\", \"\\\\newline\", \"Red\", \" Lemon\", \"ade\", \"\\n\", \" \", \"\\\\newline\", \"Ess\", \"ential\", \" Essays\", \"\\\\newline\", \"\\\\newline\", \"C\", \"ULT\", \"URE\", \",\", \" POL\", \"IT\", \"ICS\", \",\", \" AND\", \" THE\", \" ART\", \" OF\", \" PO\", \"\\n\", \" \", \"\\\\newline\", \"##\", \" Contents\", \"\\\\newline\", \"\\\\newline\", \"Title\", \" Page\", \"\\\\newline\", \"\\\\newline\", \"D\", \"ed\", \"ication\", \" and\", \" A\", \"cknowled\", \"gements\", \"\\\\newline\", \"\\\\newline\", \"Chapter\", \"\\n\", \" \", \"\\\\newline\", \"A\", \" BIG\", \" SK\", \"Y\", \" CH\", \"RI\", \"STM\", \"AS\", \"\\\\newline\", \"\\\\newline\", \"W\", \"ILL\", \"IAM\", \" W\", \".\", \" JOHN\", \"ST\", \"ONE\", \"\\n\", \" \", \"\\\\newline\", \"M\", \"aur\", \"icio\", \" G\", \".\", \" C\", \".\", \" Res\", \"ende\", \" and\", \" Cel\", \"so\", \" C\", \".\", \" R\", \"ibe\", \"iro\", \"\\\\newline\", \"\\n\", \" \", \"\\\\newline\", \"#\", \"\\\\newline\", \"\\\\newline\", \"###\", \" Copyright\", \"\\\\newline\", \"\\\\newline\", \"4\", \"th\", \" Estate\", \"\\\\newline\", \"\\\\newline\", \"An\", \" imprint\", \" of\", \" Harper\", \"Collins\", \" _\", \"\\n\", \"All\", \" About\", \" STEM\", \" NEWS\", \"\\\\newline\", \"\\\\newline\", \"All\", \" About\", \" STEM\", \" are\", \" proud\", \" to\", \" be\", \" the\", \" returning\", \" sponsor\", \" of\", \" the\", \" STEM\", \" Project\", \"\\n\", \"     \", \"Case\", \":\", \" 11\", \"-\", \"413\", \"28\", \"     \", \"Document\", \":\", \" \", \"005\", \"12\", \"042\", \"106\", \"         \", \"Page\", \":\", \" 1\", \"     \", \"\\n\", \"A\", \" human\", \" face\", \" lies\", \" inert\", \" on\", \" a\", \" surgical\", \" tray\", \" as\", \" if\", \" staring\", \" up\", \" at\", \" the\", \" team\", \" of\", \" doctors\", \" hovering\", \" over\", \"\\n\", \"There\", \" are\", \" errors\", \" in\", \" the\", \" Funding\", \" section\", \".\", \" The\", \" correct\", \" funding\", \" information\", \" is\", \" as\", \" follows\", \":\", \" This\", \" study\", \" was\", \" supported\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Sky\", \"bot\", \" Python\", \" IRC\", \"bot\", \"\\\\newline\", \"\\\\newline\", \"I\", \" use\", \" the\", \" sky\", \"bot\", \" \", \"irc\", \" bot\", \",\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Example\", \" of\", \" logical\", \" order\", \" and\", \" total\", \" order\", \" in\", \" distributed\", \" system\", \"\\\\newline\", \"\\\\newline\", \"Total\", \" order\", \":\", \"  \", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Getting\", \" Dist\", \"inct\", \" Count\", \" from\", \" two\", \" tables\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" two\", \" tables\", \" for\", \" example\", \" \", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Where\", \" is\", \" the\", \" maximum\", \" packet\", \" size\", \" for\", \" TCP\", \" identified\", \" as\", \" 65\", \",\", \"535\", \"?\", \"\\\\newline\\\\newline\", \"\\n\", \"As\", \" a\", \" follow\", \"-\", \"up\", \" to\", \" our\", \" discussion\", \" at\", \" the\", \" June\", \" 29\", \" HR\", \" PR\", \"C\", \" meeting\", \",\", \" attached\", \" is\", \" \", \"\\n\", \"Step\", \"wise\", \" transition\", \" of\", \" a\", \" topological\", \" defect\", \" from\", \" the\", \" sm\", \"ect\", \"ic\", \" film\", \" to\", \" the\", \" boundary\", \" of\", \" a\", \" dip\", \"olar\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"\\u041f\", \"\\u043b\", \"\\u0430\\u0432\", \"\\u0430\\u044e\", \"\\u0449\", \"\\u0438\\u0439\", \" \\u0431\", \"\\u043b\", \"\\u043e\\u043a\", \" \\u0441\", \" position\", \":\", \"fixed\", \"\\\\newline\", \"\\\\newline\", \"\\u041d\", \"\\n\", \"AB\", \"OUT\", \" US\", \"\\\\newline\", \"\\\\newline\", \"The\", \" doctr\", \"inal\", \" standards\", \" of\", \" the\", \" Methodist\", \" Church\", \" of\", \" the\", \" Bah\", \"amas\", \" are\", \" as\", \" follows\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Syn\", \"chron\", \"ized\", \" statement\", \",\", \" unclear\", \" java\", \" doc\", \" example\", \"\\\\newline\", \"\\\\newline\", \"Currently\", \" I\", \" am\", \"  \", \"t\", \"\\n\", \"Hi\", \"rom\", \"ichi\", \"\\\\newline\", \"\\\\newline\", \"Hi\", \"rom\", \"ichi\", \" (\", \"written\", \":\", \" \", \"\\ufffd\", \"\\ufffd\", \"\\ufffd\", \"\\u901a\", \",\", \" \", \"\\ufffd\", \"\\ufffd\", \"\\n\", \"W\", \"aving\", \" her\", \" hands\", \" from\", \" side\", \" to\", \" side\", \" as\", \" if\", \" conducting\", \" an\", \" orchestra\", \",\", \" Monica\", \" Gal\", \"van\", \" stood\", \" on\", \" the\", \"\\n\"], \"activations\": [[[-0.16341885924339294]], [[-0.16957944631576538]], [[-0.1686323881149292]], [[-0.1517106592655182]], [[-0.11243949830532074]], [[0.06784135848283768]], [[-0.05328650772571564]], [[2.0894596576690674]], [[-0.16543149948120117]], [[-0.16633301973342896]], [[-0.14091114699840546]], [[-0.13814261555671692]], [[1.0220282077789307]], [[-0.10329888761043549]], [[-0.10254930704832077]], [[-0.16747801005840302]], [[-0.15915034711360931]], [[-0.1672435700893402]], [[-0.0823543593287468]], [[-0.1490708589553833]], [[0.0]], [[-0.1265428364276886]], [[-0.02949637733399868]], [[-0.16909217834472656]], [[-0.15988920629024506]], [[-0.09343048930168152]], [[-0.13400837779045105]], [[-0.1576678603887558]], [[-0.09773819893598557]], [[-0.16714873909950256]], [[-0.14375603199005127]], [[-0.0148547338321805]], [[-0.16788895428180695]], [[-0.16965770721435547]], [[-0.12535344064235687]], [[-0.15803676843643188]], [[-0.03980928659439087]], [[0.4607405960559845]], [[-0.015483048744499683]], [[1.0023579597473145]], [[0.03911431133747101]], [[0.0]], [[-0.1548370122909546]], [[-0.1499651074409485]], [[-0.03963259235024452]], [[-0.04791305586695671]], [[-0.05563918501138687]], [[-0.16879254579544067]], [[-0.04301242157816887]], [[-0.16993172466754913]], [[0.18286174535751343]], [[-0.034420937299728394]], [[-0.11513011157512665]], [[1.8226691484451294]], [[-0.058316007256507874]], [[-0.014374757185578346]], [[0.1508307307958603]], [[0.005837936419993639]], [[-0.1235181987285614]], [[0.2977809011936188]], [[-0.1680084764957428]], [[-0.15939103066921234]], [[0.0]], [[0.3763119578361511]], [[0.20412014424800873]], [[-0.1345561444759369]], [[-0.08208293467760086]], [[-0.16809320449829102]], [[0.05278623476624489]], [[0.3194836676120758]], [[-0.14501173794269562]], [[-0.025548934936523438]], [[0.15315014123916626]], [[0.5106325745582581]], [[1.718478798866272]], [[-0.1677255630493164]], [[-0.16997119784355164]], [[-0.04901627078652382]], [[-0.14003723859786987]], [[-0.023360569030046463]], [[1.5245360136032104]], [[-0.1086616963148117]], [[-0.13644523918628693]], [[0.0]], [[-0.10694766789674759]], [[-0.16951057314872742]], [[-0.13214190304279327]], [[-0.08122041076421738]], [[-0.16539552807807922]], [[-0.15344543755054474]], [[0.09297122061252594]], [[1.7183531522750854]], [[-0.14995397627353668]], [[0.1269667148590088]], [[-0.16479414701461792]], [[-0.11324860155582428]], [[-0.16926884651184082]], [[-0.14897261559963226]], [[-0.15897290408611298]], [[-0.14623413980007172]], [[-0.15490877628326416]], [[-0.024407438933849335]], [[-0.1547759622335434]], [[-0.09599224478006363]], [[0.0]], [[0.23165984451770782]], [[-0.11949272453784943]], [[-0.1586228758096695]], [[0.029852448031306267]], [[-0.1689199060201645]], [[-0.1606970727443695]], [[-0.16720430552959442]], [[0.2137579470872879]], [[-0.1379471719264984]], [[-0.1698945313692093]], [[1.7023584842681885]], [[-0.1173194944858551]], [[-0.1693415343761444]], [[-0.16910655796527863]], [[-0.16535736620426178]], [[-0.16157206892967224]], [[0.07857196778059006]], [[-0.16966186463832855]], [[-0.1515350192785263]], [[-0.04967121407389641]], [[0.0]], [[0.23165984451770782]], [[-0.11949272453784943]], [[-0.1586228758096695]], [[-0.016005808487534523]], [[-0.16601136326789856]], [[-0.11636579036712646]], [[-0.1551325023174286]], [[0.032750073820352554]], [[-0.13542985916137695]], [[-0.16997118294239044]], [[1.644790530204773]], [[-0.12242031842470169]], [[-0.15326641499996185]], [[-0.1612042784690857]], [[-0.16465389728546143]], [[0.17869360744953156]], [[-0.16883985698223114]], [[-0.14006447792053223]], [[-0.035348355770111084]], [[0.19451116025447845]], [[0.0]], [[-0.16803163290023804]], [[-0.150054931640625]], [[-0.09190046787261963]], [[-0.14268237352371216]], [[-0.1120525673031807]], [[-0.1636074185371399]], [[0.6020405888557434]], [[-0.08713756501674652]], [[0.10944517701864243]], [[-0.03381793946027756]], [[0.3107248842716217]], [[-0.062401771545410156]], [[-0.051724210381507874]], [[-0.14011669158935547]], [[1.6444228887557983]], [[0.1614150106906891]], [[-0.1525513082742691]], [[-0.1540547013282776]], [[0.030965497717261314]], [[-0.16701056063175201]], [[0.0]], [[0.23165984451770782]], [[-0.11949272453784943]], [[-0.1586228758096695]], [[-0.016005808487534523]], [[-0.16601136326789856]], [[-0.16984997689723969]], [[-0.15076139569282532]], [[-0.0031894061248749495]], [[-0.12792439758777618]], [[-0.16987571120262146]], [[1.640438199043274]], [[-0.12101706862449646]], [[-0.1596825271844864]], [[-0.1502678245306015]], [[-0.12120594084262848]], [[0.3434072732925415]], [[-0.16984811425209045]], [[-0.1440947949886322]], [[-0.05636743828654289]], [[0.18654431402683258]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[-0.15186746418476105]], [[0.2307833582162857]], [[-0.06910272687673569]], [[-0.16591642796993256]], [[-0.16216154396533966]], [[-0.09378682076931]], [[-0.16982561349868774]], [[0.26854661107063293]], [[-0.052404653280973434]], [[-0.11088455468416214]], [[-0.14731813967227936]], [[-0.0667070522904396]], [[-0.15460118651390076]], [[0.36339154839515686]], [[-0.008465689606964588]], [[-0.1340189278125763]], [[-0.030993370339274406]], [[0.637832522392273]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[-0.16564585268497467]], [[0.2072296291589737]], [[-0.1274595707654953]], [[0.09946554154157639]], [[-0.13170552253723145]], [[-0.024347564205527306]], [[-0.13703519105911255]], [[0.3177066743373871]], [[-0.10616175830364227]], [[0.17687180638313293]], [[-0.1535118967294693]], [[-0.03166263923048973]], [[0.06800268590450287]], [[-0.13709501922130585]], [[-0.16457879543304443]], [[-0.06536397337913513]], [[-0.16991975903511047]], [[-0.054951541125774384]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[-0.14964812994003296]], [[0.1881219446659088]], [[0.0013156472705304623]], [[-0.1601034551858902]], [[-0.1560381054878235]], [[-0.09725742042064667]], [[0.0052627804689109325]], [[-0.14141830801963806]], [[-0.09437388926744461]], [[-0.16145555675029755]], [[0.08368419855833054]], [[-0.1677147001028061]], [[0.21979357302188873]], [[-0.06623445451259613]], [[-0.15875837206840515]], [[0.08637125045061111]], [[-0.15474933385849]], [[-0.12113901972770691]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[0.006090202368795872]], [[-0.16667887568473816]], [[-0.05194895341992378]], [[-0.15298505127429962]], [[-0.14700283110141754]], [[0.02562898024916649]], [[-0.10806291550397873]], [[-0.012554220855236053]], [[0.35813891887664795]], [[-0.10956959426403046]], [[-0.13609899580478668]], [[-0.13698501884937286]], [[0.14506228268146515]], [[-0.16972783207893372]], [[-0.03308689221739769]], [[-0.162308007478714]], [[-0.1696706861257553]], [[-0.1610824316740036]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[-0.026867391541600227]], [[-0.03503894433379173]], [[-0.022242162376642227]], [[0.022181840613484383]], [[-0.02893143706023693]], [[-0.07974351197481155]], [[0.09742623567581177]], [[-0.15717077255249023]], [[-0.16820859909057617]], [[-0.1646205484867096]], [[-0.04353778809309006]], [[-0.16247880458831787]], [[0.352195143699646]], [[0.04402422159910202]], [[-0.14314253628253937]], [[-0.09186066687107086]], [[-0.15451842546463013]], [[0.042844414710998535]], [[0.0]], [[1.6401190757751465]], [[0.2363700568675995]], [[-0.15186746418476105]], [[0.1522352397441864]], [[-0.033250074833631516]], [[-0.15813855826854706]], [[-0.123044453561306]], [[0.043255098164081573]], [[-0.10591059178113937]], [[0.16834864020347595]], [[-0.15245072543621063]], [[-0.14927786588668823]], [[0.1393728405237198]], [[-0.112824447453022]], [[-0.05952254682779312]], [[-0.14532382786273956]], [[-0.07309432327747345]], [[-0.09771512448787689]], [[0.521703839302063]], [[-0.0341385193169117]], [[0.0]], [[-0.160543292760849]], [[-0.07644433528184891]], [[-0.10245219618082047]], [[-0.16666582226753235]], [[-0.10276094079017639]], [[-0.16676639020442963]], [[-0.1696215271949768]], [[-0.04938538372516632]], [[-0.10657218843698502]], [[0.28225985169410706]], [[1.6377133131027222]], [[-0.14427004754543304]], [[-0.16652512550354004]], [[-0.1621216982603073]], [[0.04654160141944885]], [[0.040777966380119324]], [[-0.040213145315647125]], [[-0.09439536929130554]], [[-0.16981777548789978]], [[-0.017546342685818672]], [[0.0]], [[0.23165984451770782]], [[-0.11949272453784943]], [[-0.1586228758096695]], [[-0.016005808487534523]], [[-0.16601136326789856]], [[-0.08644101023674011]], [[-0.11206026375293732]], [[0.05866216868162155]], [[-0.13448022305965424]], [[-0.16996240615844727]], [[1.6351813077926636]], [[-0.1321674883365631]], [[-0.15949110686779022]], [[-0.1435152143239975]], [[-0.14974354207515717]], [[0.3317282497882843]], [[-0.1689658761024475]], [[-0.14655837416648865]], [[-0.03692811354994774]], [[0.20629729330539703]], [[0.0]], [[-0.1548370122909546]], [[-0.1689263880252838]], [[-0.16673590242862701]], [[0.0880856141448021]], [[0.1921442598104477]], [[-0.16977673768997192]], [[-0.13596832752227783]], [[-0.16242079436779022]], [[0.3395123779773712]], [[0.07988757640123367]], [[1.6211142539978027]], [[-0.1696147322654724]], [[-0.14561130106449127]], [[-0.059591345489025116]], [[-0.13732172548770905]], [[0.12922433018684387]], [[-0.12698625028133392]], [[-0.13382339477539062]], [[-0.10401376336812973]], [[-0.1684783697128296]], [[0.0]], [[-0.10113580524921417]], [[-0.08823657780885696]], [[-0.1538444608449936]], [[-0.0918862447142601]], [[-0.0978991687297821]], [[-0.16852453351020813]], [[-0.1069362536072731]], [[-0.04408024623990059]], [[-0.10447864234447479]], [[0.5749984383583069]], [[-0.1476394385099411]], [[-0.15832705795764923]], [[-0.12643133103847504]], [[0.20357562601566315]], [[1.6034274101257324]], [[0.2854534983634949]], [[-0.06977695226669312]], [[-0.12711331248283386]], [[-0.08979232609272003]], [[0.2069557011127472]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.16584616899490356]], [[-0.16996900737285614]], [[0.1370236575603485]], [[0.36681488156318665]], [[-0.15824882686138153]], [[0.012241289019584656]], [[-0.12461581826210022]], [[-0.08289003372192383]], [[-0.16675914824008942]], [[-0.14271265268325806]], [[0.27046316862106323]], [[-0.16876135766506195]], [[1.6007689237594604]], [[0.8790927529335022]], [[-0.1661595106124878]], [[0.05565976724028587]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.05693347007036209]], [[-0.07120919972658157]], [[0.4374609589576721]], [[-0.1691451072692871]], [[-0.13924093544483185]], [[0.2926238179206848]], [[-0.15470387041568756]], [[-0.1530618667602539]], [[0.22144511342048645]], [[0.08774920552968979]], [[0.1787690818309784]], [[-0.14942312240600586]], [[-0.13430558145046234]], [[-0.16945770382881165]], [[0.21652920544147491]], [[1.5876214504241943]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.16330692172050476]], [[-0.15870022773742676]], [[-0.15641742944717407]], [[-0.07879382371902466]], [[-0.15022367238998413]], [[-0.09685541689395905]], [[-0.16957785189151764]], [[0.12821711599826813]], [[-0.1423387974500656]], [[-0.11310189217329025]], [[-0.16435864567756653]], [[-0.16987735033035278]], [[-0.1687994748353958]], [[-0.08075947314500809]], [[0.3427954912185669]], [[1.5847537517547607]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.16650696098804474]], [[-0.1699703186750412]], [[-0.15877611935138702]], [[-0.09412365406751633]], [[0.06685110926628113]], [[-0.16489721834659576]], [[-0.12244539707899094]], [[0.4077647924423218]], [[0.33758801221847534]], [[-0.02816535159945488]], [[1.5831012725830078]], [[-0.04337470233440399]], [[-0.14160123467445374]], [[0.18222838640213013]], [[-0.16234509646892548]], [[0.0]], [[0.21093788743019104]], [[0.33196374773979187]], [[0.15959233045578003]], [[-0.15984590351581573]], [[-0.09102829545736313]], [[-0.05571717023849487]], [[-0.1538434624671936]], [[-0.16267579793930054]], [[-0.08981136232614517]], [[-0.10610245168209076]], [[-0.16582807898521423]], [[0.029150307178497314]], [[-0.16496337950229645]], [[-0.05448506772518158]], [[-0.1695662885904312]], [[-0.16782203316688538]], [[-0.169961616396904]], [[-0.1586717963218689]], [[-0.06943733245134354]], [[1.5784080028533936]], [[0.0]], [[0.37896329164505005]], [[1.561352252960205]], [[0.2635817527770996]], [[-0.1126481294631958]], [[-0.07250118255615234]], [[-0.1459057629108429]], [[-0.12832702696323395]], [[-0.1699521541595459]], [[-0.14705127477645874]], [[0.01840919628739357]], [[-0.16950714588165283]], [[-0.15735115110874176]], [[-0.08440437912940979]], [[-0.13503128290176392]], [[-0.11309028416872025]], [[-0.11186888068914413]], [[-0.036074813455343246]], [[-0.06358132511377335]], [[-0.12167703360319138]], [[-0.12915271520614624]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.15602445602416992]], [[-0.14601661264896393]], [[-0.15506666898727417]], [[1.556410551071167]], [[0.06590213626623154]], [[-0.15521204471588135]], [[-0.10482227802276611]], [[-0.1506701111793518]], [[-0.06853871047496796]], [[-0.09527720510959625]], [[0.4080847501754761]], [[0.5328764915466309]], [[0.13636904954910278]], [[0.26305049657821655]], [[-0.11668730527162552]], [[-0.16779346764087677]], [[0.0]], [[-0.03431781008839607]], [[-0.16341239213943481]], [[-0.1495531052350998]], [[-0.15599006414413452]], [[-0.1615099161863327]], [[-0.10317923873662949]], [[-0.1193547248840332]], [[-0.1018352061510086]], [[-0.13666614890098572]], [[-0.04713029786944389]], [[-0.09792754799127579]], [[-0.1636548936367035]], [[-0.15633150935173035]], [[-0.08235147595405579]], [[-0.09923983365297318]], [[-0.14746639132499695]], [[-0.04576648026704788]], [[0.13292187452316284]], [[0.13639266788959503]], [[1.555603265762329]], [[0.0]], [[-0.08149205893278122]], [[0.03308156505227089]], [[0.006072402466088533]], [[-0.16814734041690826]], [[-0.16697174310684204]], [[-0.1598092019557953]], [[0.29292652010917664]], [[0.3214848041534424]], [[0.08431366831064224]], [[0.7100701928138733]], [[-0.018874764442443848]], [[0.10075145214796066]], [[0.46738359332084656]], [[0.2469601333141327]], [[-0.1218155175447464]], [[-0.008555605076253414]], [[0.0005256166332401335]], [[-0.15168829262256622]], [[1.544736623764038]], [[0.14291495084762573]], [[0.0]], [[0.3763119578361511]], [[0.20412014424800873]], [[-0.1345561444759369]], [[-0.08208293467760086]], [[-0.16809320449829102]], [[0.05278623476624489]], [[0.3194836676120758]], [[-0.14501173794269562]], [[-0.025548934936523438]], [[0.15315014123916626]], [[0.5106325745582581]], [[1.718478798866272]], [[-0.1677255630493164]], [[-0.16997119784355164]], [[-0.04901627078652382]], [[-0.14003723859786987]], [[-0.023360569030046463]], [[1.5245360136032104]], [[-0.1086616963148117]], [[-0.13644523918628693]], [[0.0]], [[-0.10728812217712402]], [[-0.15988284349441528]], [[-0.16639143228530884]], [[-0.027422793209552765]], [[-0.1418984830379486]], [[-0.1639641374349594]], [[-0.153864324092865]], [[-0.16168437898159027]], [[0.0321657694876194]], [[1.5158531665802002]], [[-0.16216988861560822]], [[-0.1164517030119896]], [[-0.1661105751991272]], [[-0.06432773172855377]], [[-0.00431557884439826]], [[-0.08185955882072449]], [[0.1475820392370224]], [[0.8069777488708496]], [[-0.12007537484169006]], [[-0.14038099348545074]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
              "    )\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "try this for pythia 70M model, last layer. Try on 3 neurons.\n",
        "\n",
        "If that fails too, then for one neuron, do hypothesis testing & verification on the top examples. If the top examples can be separated, like some seem to be duplicate token detectors and others are salutations, then we may can find a new neuron that separates those two classes\n",
        "\n",
        "Suppose you had max-activating examples like:\n",
        "\"a big dog\"\n",
        "\"a hairy dog\"\n",
        "\"I love dogs\"\n",
        "\n",
        "You may think it's just activating on the word dog(s). But maybe it's activating on the third word, or would also activate on \"cat\" or something. You can test this using the visualize top examples.\n",
        "You can also write your own examples to be run through the network and shown.\n",
        "So you have some hypothesis \"It's the word 'dog'\", then you test & verify it/try to falsify it.\n",
        "If a neuron in a specific activation range (e.g. the max activating range) seems to have two different hypothesis (e.g. half seem to be dogs, half seem to be pronouns), then you can directly search for another neuron that separates those examples (e.g. Neuron 43 also fires for dogs and Neuron 1786 also fires for pronouns)"
      ],
      "metadata": {
        "id": "tEjnBPpUU_YQ"
      }
    }
  ]
}